\chapter{Artificial Intelligence}

Cybersecurity attacks targeting AI have a \textbf{different nature} from the ones studied before, since vulnerabilities in AI cannot simply be identified and patched, 
they are more \textit{intrinsic}:
the algorithms themselves and their reliance on data are the problem.

Despite this fundamental difference, the two are \textit{linked} in important ways:
in fact, many AI attacks are aided by gaining \textbf{access to assets} such as datasets or model details.
\textbf{Confidentiality} attacks  enable adversaries to obtain the assets needed to engineer \textit{input} attacks,
while \textbf{integrity} attacks will enable adversaries to make the changes to a dataset or model needed to execute a \textit{poisoning} attack.\\
More generally, while AI attacks can certainly be crafted \underline{without} accompanying cyberattacks,
strong traditional cyber defenses will increase the difficulty of crafting certain attacks.

\nl

Since AI attacks are becoming popular only in the last few years,
it is still unclear whether they can be sufficiently practical and effective to represent a relevant threat.\\
However, even if some may say that AI attacks do not deserve equal consideration with
their traditional cybersecurity attack counterparts, \underline{such point of view is \textbf{incorrect}}.

\nl

Most AI systems are \textbf{opaque}, meaning that the systems accept inputs, and generate
outputs without ever revealing the internal logic, algorithms or parameters.
Also training data sets, which effectively contain all the knowledge of the
trained system, are usually kept \textbf{confidential}.\\
This makes it usually \underline{impossible} for an outside observer to apply reverse engineering techniques to determine exactly how a system works, or why it produces particular outputs.

\section{Attack categorization}
\subsection{Poisoning}
\begin{enumerate}
   \item \textbf{Dataset} poisoning is the most direct attack:
   \begin{enumerate}
      \item[] Introducing during data \textit{collection} or \textit{curation} incorrect, or incorrectly labelled, data into the set, may result in the entire learning to be disrupted.
      \item \textbf{Label Poisoning} (\textit{Backdoor} Poisoning)\\
      Adversaries inject mislabeled or malicious
      data during the training phase to influence the model's behavior during inference,
      eventually resulting in triggering an unexpected output by a designed input (called \textit{triggers}).
      \item \textbf{Training Data Poisoning}\\
      Adversaries modify a significant portion of the training data to influence the AI model's learning process.\\
      The misleading or malicious examples
      allow the attacker to \textit{bias} the model's decision-making towards a particular outcome.
   \end{enumerate}
   \item \textbf{Algorithm} poisoning occur when an attacker interferes with the learning algorithms.
   \item \textbf{Model} poisoning occurs when the entire deployed model is simply replaced
   by an alternative one.
   \note{
      This is similar to a traditional attack where the
      electronic files comprising the model could be altered or replaced.
   }
\end{enumerate}

\subsection{Input/Evasion attacks}
In an \textbf{input attack} (aka \textit{evasion attack}) an attacker modifies the AI system input to cause the system to malfunction,
acting on the \textit{communication channel} or directly on the \textit{input source}, which is the most interesting case.\\
Data perturbations can be very small, making them very hard, if
not impossible, to detect. 
\note{
   There is a famous example which highlighted how by changing just a few pixels of
   an input, the system might be forced to wrongly identify an image.
}

\subsection{Microsoft "Failures" categorization}
\labelitemize{\textit{Intentional} failures}{
   \begin{itemize}
      \item \textbf{Perturbation} attack\\
      Attacker modifies the query to get appropriate response
      \begin{itemize}
         \item \textbf{Image}:
         Noise is added to an X-ray image, which makes the predictions go from normal scan to abnormal.
         \item \textbf{Text translation}:
         Specific characters are manipulated to result in incorrect translation.
         The attack can suppress specific word or can even remove the word completely
         \item \textbf{Speech}:
         Researchers showed how given a speech waveform,
         another waveform can be exactly replicated transcribes into a totally different text
      \end{itemize}
      \item \textbf{Poisoning} attack\\
      Attacker contaminates the training phase of ML systems
      to get intended result
      \begin{itemize}
         \item \textbf{Targeted}:
         the attacker wants to misclassify specific examples
         \item \textbf{Indiscriminate}:
         the aim here is to cause DoS like effect,
         which makes the system unavailable.
         \note{
            In a medical dataset where the goal is to predict the dosage of anticoagulant drug Warfarin using demographic
            information, etc. Researchers introduced malicious samples at $8\%$ poisoning rate, which \underline{changed dosage by $75.06\%$} for
            half of patients.\nl
            
            In the Tay chatbot, future conversations were tainted because a fraction of the past conversations were used to
            train the system via feedback.
            }
      \end{itemize}
      \item \textbf{Model Inversion}\\
      Attacker recovers the secret features used in the model by through careful queries
      \item \textbf{Membership Inference}\\
      Attacker can infer if a given data record was part of the model’s training dataset or not
      \item \textbf{Model Stealing}\\
      Attacker is able to recover the model through carefully-crafted queries
      \item \textbf{Reprogramming ML system}\\
      Repurpose the ML system to perform an activity it was not programmed for
      \item \textbf{Adversarial Example in Physical Domain}\\
      Attacker brings adversarial examples into
      physical domain to subvert ML system\\
      e.g: 3d printing special eyewear to fool facial recognition system
      \item \textbf{Malicious ML provider recovering training data}\\
      Malicious ML provider can query the model used by
      customer and recover customer’s training data
      \item \textbf{Attacking the ML supply chain}\\
      Attacker compromises the ML models as it is being downloaded for use
      \item \textbf{Backdoor ML}\\
      Malicious ML provider backdoors algorithm to activate with a specific trigger
      \item \textbf{Exploit Software Dependencies}\\
      Attacker uses traditional software exploits like buffer overflow to confuse/control ML systems
   \end{itemize}
}
\labelitemize{\textit{Unintentional} failures}{
   \begin{itemize}
      \item \textbf{Reward Hacking}:\\
      Training mismatch with reality
      \item \textbf{Side Effects}
      \item \textbf{Distribution Shift}:\\
      Usage in environment different from test
      \item \textbf{Natural Adversarial}:\\
      Unexpected real data
      \item \textbf{Common Corruption}:\\
      Unable to manage perturbations
      \item \textbf{Incomplete Testing}
   \end{itemize}
}

\section{ETSI on Securing AI}
\textit{European telecommunication Standard Institute} \textbf{ETSI} has produced
interesting documents on AI security defining:
\begin{itemize}
   \item A threat ontology
   \item An attack taxonomy
   \item Attack mitigation
\end{itemize}
\note{
   Currently there are other efforts by Google and others but not so well focused and detailed as the ETSI standard
}

They also provide a classification of learning methods
\begin{itemize}
   \item \textit{supervised}: training data labeled
   \item \textit{semi-supervised}: training data partially labeled
   \item \textit{unsupervised}: training data unlabeled
   \item \textit{reinforcement}: a policy defining how to act is learned by agents through experience to maximize their reward, with experience being gathered by interacting in an environment through state transitions.
\end{itemize} 

The question of securing AI systems can be simply stated as ensuring the \textit{confidentiality}, \textit{integrity} and \textit{availability} of those systems throughout their lifecycle.
\begin{table}[htbp]
   \centering
   \begin{tabular}{|c|c|}
      \toprule
      \textbf{Lifecycle Phase} & \textbf{Issues}\\
      \hline
      Data acquisition & {\color{red}Integrity}\\
      \hline
      Data curation & {\color{red}Integrity}\\
      \hline
      Model design & Generic issues only \\
      \hline
      Software Build & Generic issues only\\
      \hline
      Train & {\color{blue}Confidentiality}, {\color{red}Integrity}, {\color{green}Availability}\\
      \hline
      Test & {\color{green}Availability}\\
      \hline
      Deployment & {\color{blue}Confidentiality}, {\color{red}Integrity}, {\color{green}Availability}\\
      \hline
      Updates & {\color{red}Integrity}, {\color{green}Availability}\\
      \hline
   \end{tabular}
   \caption{AI Lifecycle and corresponding issues}
   \label{tab:ailifecycle_issues}
\end{table}

\subsection{Data Acquisition and Curation}
Training data is collected by multiple sources and sensors, and may be in various formats.
The data may become \textbf{poisoned} (or \textit{degraded}, in the latter case) due to a malicious poisoning attack but also due to insufficient care in collecting consistent and fit-for-purpose data.

\begin{paracol}{2}
   \colfill
\labelitemize{Data Curation}{
   \begin{enumerate}
      \item Integrating data from multiple sources and formats,
      \item Identifying missing components of the data,
      \item Removing errors and noise (aka anomaly detection)
      \item Format conversion
      \item \textit{Labelling}
   \end{enumerate}
}
\colfill
\switchcolumn
\colfill
Data labelling should be accurate and complete to ensure that the labelling retains its \textbf{integrity} and is not compromised through poisoning attacks.
It is also important to address the challenge of ensuring the data set is
\textit{unbiased}.
\note{
   Data augmentation can impact on data integrity.
}
\colfill
\end{paracol}

\subsection{Training}
The training phase is one of the most critical steps, since it establishes the baseline behaviour of the application.
To biggest challenge in training is to provide a high quality training dataset and to ensure its integrity.
\subsubsection{Confidentiality}
The dataset confidentiality can be compromised by an attacker with some knowledge of the implementation (\textit{full} or \textit{partial knowledge attack}),
or even with no knowledge of the internal algorithm (\textit{zero knowledge attack}.)

Even with \textit{no knowledge} of the original dataset, an attacker might create an augmented training data set with malicious inputs designed to output labels containing information about
the original training data.

Instead, if knowing the dataset, unused bits in parameters may be exploited by an attacker to leak information.

\subsubsection{Integrity}
Three main points concerning integrity in training

\begin{enumerate}
   \item In \textbf{transfer learning} a pre-trained network is (incrementally) finely tuned each time with few new training samples.\\
   An attacker might inject malicious samples in these phases.
   
   \item
   As previosly discussed a \textbf{backdoor} vulnerability merges a \textit{special pattern} included during the training
   phase, and a \textit{trigger} to generate an output during the inference phase.
   
   This type of attack can include poisoning during the training phase as a
   component of its attack, but it is important to emphasize that
   \begin{enumerate}
      \item a backdoor attack requires action in \textit{both} the training and implementation
      phases,
      \item a poisoning attack requires action \textit{only} in the training phase
   \end{enumerate}

   \item The learning algorithm can be modified by an attacker with full or partial knowledge to ensure an incorrect inference for certain samples, whilst maintaining correct performance for the client test set.
\end{enumerate}

\subsubsection{Availability}
The availability of the machine learning model can be compromised by
\textbf{poisoning} attacks on the training data set, which result in the wrong
inference result.

Differently, for unsupervised learning, feature selection is an important step and strongly depends on the \textbf{weightings} associated to each feature.\\
An attacker with full or partial knowledge of such weightings could
modify them to perform an availability attack where the salient
features are made unavailable by reversing the weights,
implementing a \textbf{Denial of Features} attack.

\subsection{Testing}
Learnt models can be vulnerable to \textbf{adversarial} samples that result in the
functionality not meeting the specification and therefore making function or service
not available.\\
To this extent \textit{Adversarial testing} should be performed which aims to quickly find testing samples that can cause failure
and might also test unexpected or previously unseen inputs do not cause the
system to malfunction or become unavailable.

\subsection{Deployment}
Deployment of machine learning systems has the same challenges as generic
systems, including choices about architecture, hardware/software deployment
and use of features such as \textit{Trusted Execution Environments} (\texttt{TEE}s),
which are more secure but provide looser performance than generic processors.


The main vulnerability concerning \textit{Confidentiality} in the deployment of machine learning models
is their susceptibility to a backdoor attack that can compromise the
\textbf{confidentiality} of the training set;
the attack may vary depending on the knowledge of the implementation of the model, whether it is zero, partial or full.
Regarding confidentiality in ML systems there are two issues:
\begin{itemize}
   \item \textbf{Membership inference}: given a data sample and access to the model, infer whether this sample is included in the training dataset or not
   \item \textbf{Model inversion}: given a prediction result and access to the model, infer the input
\end{itemize}
Aside from these ML-specific vulnerabilities, there also the common ones when deploying on \textit{untrusted devices}.
\nl

\textbf{Integrity} of an ML algorithm might be impacted by \textit{input attacks} performed directly while the system is operating, resulting in the algorithm behaving incorrectly.\\
As seen before, also \textit{backdoor attacks}, carried out during training phase (\textit{malicious input injection}) and operating phase (\textit{triggers}).

\textbf{Availability} instead can be targeted by several attacks, including evasion ones e.g. \textit{malware obfuscation}.

\section{AI Design Challenges}
Aside from aspects strictly related to security,
it is relevant to briefly discuss also some notions regarding the design of an AI system, to whom the effectiveness of the system is strictly related.

\begin{itemize}
   \item \textbf{Confirmation bias}: data is selected or manipulated to produce outputs aligned to
   predetermined assumptions
   \item \textbf{Selection bias}: data is selected subjectively, resulting in a set that does not accurately
   reflect the population. 
   \note{
      When data is gathered using a survey, people willing to participate
      in the survey are not necessarily an accurate reflection of the entire population.
   }
   \item \textbf{Outliers}: data points which contain extreme values, and therefore can have a
   disproportionate impact
   \note{
      When analysing customer spending habits, a single customer
      who spends significantly more than all the others will impact heavily on the average.
   }
   \item \textbf{Underfitting} (model is too simple) and \textbf{overfitting} (model is overly
   complex) can both lead to an inaccurate view of the real data.
\end{itemize}
It is important to distinguish between systems that display \textit{unintended bias}, and those
whose design displays specific tendencies.\\
In a safety-critical application, such as
autonomous vehicles, it can be desirable for the system to exhibit behaviour which errs on
the side of safety rather than risk.

Bias does not necessarily represent a security issue, but can simply result in the system not meeting its functional requirement.
\note{
   Note that issues related to ethics, inclusivity, etc. in biases are not discussed here.
}