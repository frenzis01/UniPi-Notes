\chapter{Kademlia}
\textbf{Kademlia} is a protocol used by some of the largest public DHTs
\note{
   \ns
\begin{itemize}
   \item BitTorrent Mainline DHT
   \item Ethereum P2P network
   \item IPFS
\end{itemize}
}
It has three key charateristics which are not offered by other DHTs
\begin{enumerate}
   \item routing information spreads automatically as a side-effect of lookups
   \item flexibility to send multiple requests in parallel to speed up lookups by
   avoiding timeout delays (parallel routing)
   \item iterative routing
   \note{At each routing step of the query, the queried node sends a report to the starting querying node, even if it could not answer the query.}
\end{enumerate}

\section{Structure}
\begin{paracol}{2}
   \colfill
   Kademlia exploits the leaves of a \textbf{Trie}\footnote{k-ary search tree and prefix tree} to define the logical identifier space;

   Note that not all leaves correspond to nodes (peers)
   \colfill
   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/kademlia_trie.png}
      \caption{Trie}
      \label{fig:kademlia_trie}
   \end{figure}
\end{paracol}

\newpage
\subsection{Assigning keys to leaves}
The rule to partition the keys (content) among the nodes must respect the rules of \textit{consistent hashing}.

\begin{paracol}{2}
   \colfill
   \begin{definition}[Partitioning rule]
      A key is assigned to the node with the
      \textit{``\ul{lowest} common ancestor''}:\\
      Find the longest prefix between the
      key and the node identifier, and then assign the key to such node.
   \end{definition}
   \colfill
   \switchcolumn
   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.45\columnwidth]{images/kademlia_leaves01.png}
      \includegraphics[width=0.45\columnwidth]{images/kademlia_leaves02.png}
      \label{fig:kademlia_leaves}
   \end{figure}
\end{paracol}

\section{Distance - XOR Metric}
\begin{center}
   \textit{\ul{How to compute the \textbf{distance} between two nodes?}}
\end{center}

\section{Routing Table}
In order to look for data, Kademlia's key idea is to \ul{store a logarithmic number of node IDs and their corresponding IP} addresses and some contact taken from the identifier trie.

TODO

\section{Key Lookup}

TODO

\section{Protocol Messages}
TODO reformat this
\begin{enumerate}
   \item \texttt{FIND\_NODE $v\longrightarrow w$} (T) (v,w nodes, T target of the look up)\\
   the recipient of the message (w) returns k (IP address, UDP port, Node
   ID) triples for the k nodes it knows about closest to the target T.
   
   these triples can come from a single k-bucket, or they may come from multiple
   k-buckets if the closest k-bucket is not full.
   
   in any case, the recipient must return k items, unless there are fewer than k
   nodes in all its k-buckets combined, in which case it returns every node it
   knows about 



   
   \item \texttt{FIND\_VALUE} $v \longrightarrow w(T)(v,w \textit{ nodes}, T \textit{ value looked up})$\\
   in: T, 160-bit ID representing a value
   
   out:
   if a value corresponding to T is present in the queried node (w), the
   associated data is returned
   otherwise it is equivalent to \texttt{FIND\_NODE} and w returns a set of k triples
   
   If \texttt{FIND\_VALUE} returns a list of other peers, it is up to the requester to continue
   searching for the desired value from that list
   
   \item \texttt{PING} $v \longrightarrow w$
   
   probe node w to see if its online
   \item \texttt{STORE} $v \longrightarrow w$ $(Key, Value)$
   
   instructs node w to store a $\langle key, value \rangle$ pair
   
   the node has been retrieved through a
\end{enumerate}
The actual \textbf{Lookup algorithm} is based on \texttt{FIND\_NODE}.
many \texttt{FIND\_NODE} can be executed in parallel, according to $\alpha$ that is a system-wide concurrency parameter.
\note{With $\alpha = 1$, the lookup algorithm is similar to \textit{Chord}, one step progress each time}
\note{Lookup procedure is the same for \texttt{FIND\_VALUE} and \texttt{FIND\_NODE}}