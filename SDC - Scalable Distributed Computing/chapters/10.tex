\chapter{Replication}

\begin{definition}
   [Replication]
   Replication is the process of sharing information so as to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance, or accessibility.

\end{definition}

The aims of replication are reducing latency, increasing availability, and scaling read throughput.
However replication clearly poses some potential problems when handling changes to replicated data.
{The possible approaches to replication are:\ns
\begin{itemize}
   \item single-leader
   \item multi-leader
   \item leaderless replication
\end{itemize}}

\section{Leaders and followers}
Each node may store a copy of the database, in this case it is called a \textbf{replica}, so, ensuring consistency among replicas is crucial.



With leader based replication, one replica is designated as the \textit{leader} who receives write requests by clients, and is responsible for writing new data to its local storage.
Later on, followers update their local copy based on the leader's replication log.\\
When a new follower has to be set up, a standard file copy may result in inconsistencies due to ongoing writes, so a \textbf{snapshot} mechanism must be used to ensure data integrity.

\subsection{Synchronous vs Asynchronous Replication}
\begin{itemize}
   \item \textbf{Synchronous} - The \ul{leader waits for the followers} to acknowledge the write before acknowledging the write itself.
   \begin{itemize}
      \item Ensures that followers have an up-to-date copy of the data.
      \item The disadvantage is that the system may become unavailable if a follower is slow to respond.
   \end{itemize}
   \item \textbf{Asynchronous} - The \ul{leader does \textbf{not} wait for the followers} to acknowledge the write before acknowledging the write itself.
   \begin{itemize}
      \item Follower may fall behind the leader.
      \item The advantage is that the system continues processing even if a follower is slow to respond.
      \item The disadvantage is that the system may lose data if the leader fails before the followers have caught up, so there's the need for methods to ensure data integrity.
   \end{itemize}
   \item \textbf{Semi-synchronous} - The \ul{leader waits for a \textbf{quorum} of followers} (at least one follower, typically always the same and placed in another location) to acknowledge the write before acknowledging the write itself.
   \begin{itemize}
      \item Ensures that at least one follower has an up-to-date copy of the data.
      \item Typically there is \ul{only one node fully synchronous}, while others are asynchronous, and it is \ul{placed far away from the leader}.
   \end{itemize}
\end{itemize}

\section{Failure}
\begin{itemize}
   \item \textbf{Node} failures - Nodes can fail due to faults or planned maintenance. The goal is to minimize impact and keep the system running.
   \item \textbf{Follower} failure: Catch up recovery - Followers keep a log of data changes, but can catch up with the leader if they fall behind.
   \item \textbf{Leader} failure: Failover - Failover process involves promoting ---hence reconfiguring--- a follower to leader. Failover can be manual or automatic
   \item Challenges in \textbf{Failover} - Asynchronous replication may lead to data loss, as stated before.
\end{itemize}

\subsection{Implementing Replication Logs}
The replication log is the mechanism by which changes are propagated from the leader to the followers. Different approaches exist, each with specific trade-offs in terms of flexibility, consistency guarantees, and implementation complexity.

The choice of replication strategy depends on the database architecture, the need for cross-version compatibility, and the desired level of abstraction between the storage engine and the replication mechanism.

\begin{itemize}
   \item \textbf{Statement-based} replication - The leader logs and sends the actual SQL statements (e.g., \texttt{INSERT}, \texttt{UPDATE}, \texttt{DELETE}) to followers, which then execute them.
   
   \textit{Example}: Leader executes \texttt{UPDATE users SET balance = balance + 100 WHERE id = 42} and sends this exact statement to followers.
   
   \textit{Problems}:
   \begin{itemize}
      \item Non-deterministic functions like \texttt{NOW()}, \texttt{RAND()} produce different results on each replica
      \item Auto-incrementing columns may generate different values
      \item Statements with side effects (triggers, stored procedures) may behave differently
   \end{itemize}
   \note{This was used in VoltDB and MySQL before version 5.1.}
   
   \item \textbf{Write-ahead log (WAL)} shipping - The leader ships its low-level write-ahead log (containing byte-level changes to disk blocks) to followers, which replay the exact same disk modifications.
   
   \textit{Example}: PostgreSQL replicates the WAL that describes which bytes changed in which disk pages.
   
   \textit{Problems}:
   \begin{itemize}
      \item Tightly coupled to the storage engine internals
      \item Leader and followers must run the \textbf{same database version} and architecture
      \item Zero-downtime upgrades become difficult (cannot upgrade followers first)
   \end{itemize}
   
   \item \textbf{Logical (row-based) log} replication - Replicates a logical representation of changes at the row level, decoupled from the storage engine internals.
   
   \textit{Example}: For an inserted row, the log contains: \texttt{\{table: "users", operation: "INSERT", values: \{id: 42, name: "Alice"\}\}}. For updates, it includes old and new values.
   
   \textit{Advantages}:
   \begin{itemize}
      \item Storage-engine independent: allows different database versions or even different database systems
      \item Easier to parse by external applications (e.g., data warehouses, caches)
      \item Supports backward compatibility
   \end{itemize}
   \textit{Disadvantages}:
   \begin{itemize}
      \item Requires additional implementation effort and expertise
      \item Need to maintain the logical log format separately from storage internals
   \end{itemize}
   \note{Used by MySQL's binlog (in row-based mode), PostgreSQL's logical decoding.}
   
   \item \textbf{Trigger-based} replication - Uses database triggers to capture changes and write them to a separate replication log table, which is then read by an external process that applies changes to followers.
   
   \textit{Example}: A trigger on \texttt{INSERT/UPDATE/DELETE} writes change records to a \texttt{replication\_log} table; a replication daemon reads this table and applies changes to other databases.
   
   \textit{Use cases}:
   \begin{itemize}
      \item Flexible replication (e.g., replicate subset of data, transform data during replication)
      \item Replication to different database systems
      \item Custom conflict resolution logic
   \end{itemize}
   \textit{Disadvantages}:
   \begin{itemize}
      \item Higher overhead and performance impact
      \item More prone to bugs due to application-level code
      \item Trigger logic can become complex to maintain
   \end{itemize}
   \note{Used by Oracle GoldenGate, Databus for Oracle, Bucardo for PostgreSQL.}
\end{itemize}


\section{Eventual Consistency}
The eventual consistency model implies a temporary state where the followers lag behind the leader, but eventually they will catch up. 
Hence there is some ``\textbf{replication-lag}'' \dots

However, many modern applications require heavy reading workload, but light writing workload, so eventual consistency is a good trade-off.
Besides it is possible to increase capacity for read- only requests by adding more followers. Removes load from the leader. Allows read requests to be served by nearby replicas 

\subsection{Read-after-write consistency}
\begin{paracol}{2}
   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.95\columnwidth]{images/10/readafterwrite.png}
      \caption{Read-after-write consistency time schema}
      \label{fig:10/readafterwrite}
   \end{figure}


   \switchcolumn
   \colfill
   In asynchronous replication it is not trivial to ensure \textit{read-your-write} consistency, i.e. if a client writes a value to the leader, it should be able to read it.
   In other words, a user may think their data is lost, because they cannot see it, but it is actually there, just not yet replicated to the follower.
   
   \textit{Read-after-write} consistency guarantees that \ul{users see their own updates, but not necessarily the updates of others.}
   It may be implemented by reading user-modified data from the leader.
   \colfill
\end{paracol}

\subsection{Monotonic Reads}

\begin{paracol}{2}
   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.95\columnwidth]{images/10/monotonic.png}
      \caption{Monotonic reads time schema}
      \label{fig:10/monotonic}
   \end{figure}
   
   
   \switchcolumn
   \colfill
   \textit{Monotonic reads} consistency guarantees that if a user has read the value of a key, it will not read an older value of the same key in the future.
   It is stronger than eventual consistency, but weaker than strong consistency.
   
   In asynchronous replication may see data moving backward in time when reading from different replicas with varying lag.\\
   Monotonic reads can be achieved by ensuring that reads are always performed on the same replica.
   \colfill
\end{paracol}


\subsection{Consistent Prefix Reads w/different DBs}

\begin{paracol}{2}
   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.95\columnwidth]{images/10/consistentprefix.png}
      \caption{Consistent prefix reads with different DBs}
      \label{fig:10/consistentprefixs}
   \end{figure}

   \switchcolumn
   \colfill
   \textit{Consistent prefix reads} consistency guarantees that if a sequence of writes happens in a certain order, then a read will see those writes in the same order.
   
   Suppose that $A$ asks $B$ something, and $C$ is listening, but hears first the answers and later the question: weird, ain't it?\\
   This may happen due to replication lag. We want to prevent casuality violations.
   \colfill
\end{paracol}



\section{Multi-Leader Replication}
With a single-leader configuration, the leader is a bottleneck, and if it fails, the system elects a new leader (perhaps there may be a small downtime).
Especially in geographically distributed systems, having a single leader may introduce high latency for clients far away from the leader.

With multi-leader replication, there are multiple leaders, and each leader can accept writes. This is useful for geographically distributed systems, where each region has its own leader. The same applies for related datacenters, each with its own leader.
On a lower level, we could also have multiple leaders for multiple partitions of the data, one per partition.


\subsection{Collaborative Editing}
\ul{Real-time collaborative editing is a use case for multi-leader replication.}\\
Changes are instantly appied to the user's local replica and then asynchronously replicated to the server and other users.

Application must obtain a lock on the document before editing, as with single leader replication with transactions (?).\\
The unit of change may be small, as a single keystroke.\nl

When two users edit the same part of the document, they get local updates, but when the system asynchronously synchronizes the replicas, a conflict occurs and needs resolution.
\begin{itemize}
	\item User 1 changes title from A to B
	\item User 2 changes title from A to C
	\item Both changes are applied locally
	\item Conflict detected during asynchronous
replication
\end{itemize}
With single-leader replication, the second writer blocks or aborts if first write is incomplete, having the user to retry the input, ultimately avoiding the issue.\\
With multi-leader replication, the system must resolve the conflict, and the user must be notified of the resolution.

\subsection{Conflict Avoidance}
\labelitemize{\textit{Strategies}}{
   \begin{itemize}
      \item Routing writes towards the same leader prevents conflicts.
      \item Routing user requests towards the same datacenter allows to use only the leader of that datacenter; furthermore, geographical locality may be applied, to serve a user with the closest datacenter.
      \item[] Conflict resolution may happen \textit{on write}, \textit{on read}, or at \textit{row/document} level.
   \end{itemize}
}

% // Prof SKIPPING a few slides starting from "consistent and converging to a state"


\subsection{Topologies}
\begin{itemize}
   \item \textbf{Circular} - each node receives writes and forwards writes
   \item \textbf{Star topology} - root node forwards writes to all other nodes 
   \item \textbf{All-to-all} - every leader sends writes to every other leader
\end{itemize}

Note that this is not hardware (neither virtual) network, but a logical network of replication, made up by how we manage the information flow.

In whatever topology it is important to \textbf{monitor staleness}, which is monitoring obsolete replicas, i.e. how far behind a follower is from the leader.
Some metrics must be kept.\nl

There is also \textbf{leaderless replication}, where there is no designated leader, and any node can accept writes. This is useful for systems that require high availability and fault tolerance, as there is no single point of failure. 
Handling Node Outages is easier with leaderless replication, as any node can accept writes, and there is no need to elect a new leader. Write operations are considered successful once a quorum of nodes acknowledges the write.\\
Stale values may occur when a node comes back online. These are detected by reads with a quorum, which ensures that the most recent value is returned.

However, it introduces challenges in ensuring consistency among replicas, as concurrent writes may lead to conflicts that need to be resolved.

\subsection{Concurrent Writes}
Several clients can write to the same key simultaneously, and the order of writes may differ on different leaders. So\dots ``who wins?''

\begin{itemize}
   \item \textbf{Last write wins} - The last write is the one that is kept.
   \item \textbf{Most recent timestamp wins} - The write with the most recent timestamp is the one that is kept.
   \item \textbf{Merge values} - The values are merged together, based on a \textit{happens-before} relationship.
\end{itemize}

However, especially for geographically spread databases, it is not trivial to determine the most recent timestamp, because clocks may be unsynchronized, and also the network latency may vary. Remember also that the light speed is finite, so it takes time for a signal to travel from one point to another.\\
For this reason the \textit{happens-before} relationship is used, which is a partial order on events, which reflects the order in which events have occurred.

\section{Key Takeaways}
Replication is a fundamental technique in distributed systems that ensures data availability even when nodes fail, allowing systems to continue operating and serving requests. By maintaining multiple copies of data across different locations, replication also enables systems to function offline, which is particularly useful for applications that need to work without continuous network connectivity.

One of the primary benefits of replication is \textbf{latency reduction}: by placing replicas geographically closer to users, we can significantly enhance user experience through faster response times. Additionally, replication naturally \textbf{scales read operations} by distributing the workload across multiple replicas, making it essential for handling large volumes of read-heavy requests efficiently.

However, replication introduces significant challenges, particularly in managing concurrent writes and ensuring data consistency across replicas. The fundamental tension lies in balancing consistency, availability, and performance---a trade-off that manifests in different replication strategies (single-leader, multi-leader, leaderless) and consistency models (strong consistency, eventual consistency, causal consistency).

Understanding these consistency models is crucial for designing robust distributed systems. Each model offers different guarantees: \textit{eventual consistency} prioritizes availability and performance but allows temporary inconsistencies, while \textit{strong consistency} ensures all replicas are always synchronized but may sacrifice availability. Effective fault tolerance strategies, including proper failover mechanisms and conflict resolution policies, are necessary to maintain system integrity and reliability in the face of network partitions and node failures.