\chapter{Elements and Challenges in Distributed Systems}
\coolquote{A distributed system is one in which the failure of a computer you didn't know existed can render your own computer unusable}{Leslie Lamport}

Various definitions of Distributed Systems have been given in the literature, \ul{none of them \textbf{satisfactory}, and none of them in agreement with any of the others.}

Let’s try to accept a quite simple one:
\coolquote{
   A distributed system is a collection of autonomous computing elements that appears to its users as a single coherent system.
}{
   Andrew S. Tanenbaum
}

From this definition, we can derive two key points:
\begin{enumerate}
   \item \textbf{Autonomy} - Each node in the system is independent and can make decisions on its own
   \item \textbf{Transparency} - The system appears as a single entity to the user
\end{enumerate}

\section{Autonomous Computing Elements}
{Nodes can act independently from each other, so:\ns
\begin{itemize}
   \item nodes need to achieve common goals realized by \textbf{exchanging messages} with
   each other
   \item nodes \textbf{react to messages} leading to further communication through message passing
\end{itemize}}

Having a collection of nodes implies also that each node should know which other nodes are in the system and should contact, and how to reach them.
In particular it is important to have a \textbf{robust naming system} to identify nodes, and to allow scalability.
The key point of a naming system and a DNS is to \textbf{decouple} the name of a node from its physical address.


Since each node has its own notion of time, it is not always possible to assume that there is a ``global clock'':
there must be \textbf{synchronization} and \textbf{coordination} mechanisms to ensure that the system behaves correctly.

\section{Transparency}
The \ul{distributed system should appear as a single coherent system}, so end users should not even notice that they are dealing with the fact that processes, data, and control are dispersed across a computer network.


This so-called \textbf{distribution transparency} is an important design goal of distributed systems.
\note{A ---perhaps not-so-perfect--- example may be Unix-like operating systems in which resources are accessed through a unifying file-system interface Effectively hiding the differences between files, storage devices, and main memory, but also networks}


However, striving for a single coherent system introduces relevant issues: 
e.g., \textbf{partial failures} are inherent to any complex system, in distributed systems they are \ul{particularly difficult to hide}.

\subsection{Middleware}

To aid the need of easing the development of distributed applications, distributed systems are often organized to have a \textbf{separate layer} of software placed on top of the respective operating systems of the computers that are part of the system.\\
This layer is called \textbf{middleware}. [Bernstein, 1996]

In a sense, middleware is the same to a distributed system as what an operating system is to a computer:
a manager of resources offering its applications to efficiently share and deploy those resources across a network

We list below some example of possible middleware services.

\subsubsection{RPC - Communication}

Remote Procedure Call (RPC) allows an application to invoke a function that is implemented and executed on a remote computer as if it was locally available.

Developers need merely specify the function header and the RPC subsystem generates the necessary code that establishes remote invocations. 

Notable examples: Sun RPC, Java RMI, Google RPC (gRPC)

\subsubsection{Service Composition}
Enabling Service composition allows to develop new applications by taking existing programs and gluing them together, e.g. Web services.

An example: Web pages that combine and aggregate data from different sources, e.g. GMaps in which maps are enhanced with extra information from other services.

\subsubsection{Improving Reliability}

\begin{paracol}{2}
   
   
   Horus toolkit allows to build applications as a group of processes such that any message sent by one process is guaranteed to be received by all or no other process.
   
   As it turns out, such guarantees can greatly simplify developing distributed applications and are typically implemented as part of the middleware.

   \note{A more down-to-earth example of improving reliability is the use of RAID systems to ensure that data is not lost in case of disk failure.}

   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.2\columnwidth]{images/02/horus.png}
      \caption{Horus Toolkit}
      \label{fig:02/horus}
   \end{figure}   

\end{paracol}   

\subsubsection{Supporting Transactions on Services}
Many applications make use of multiple services that are distributed among several computers.

Middleware generally offers special support for executing such services in an all-or-nothing fashion, commonly referred to as an \textbf{atomic transaction}.

\note{Recall all the mess that can happen with threads, race conditions, and deadlocks? Middleware can help with that.}

The application developer need only specify the remote services involved.

By following a standardized protocol, the middleware makes sure that every service is invoked, or none at all.

\section{Building Distributed\dots is it a good idea?}
Building distributed systems is a challenging task, and it is not always the best choice.
There are \textbf{four important goals} that should be met to make building a Distributed System worth the effort:
\begin{enumerate}
   \item \textit{\textbf{Resource} accessibility}
   \item \textit{\textbf{Hide} Distribution}
   \item \textit{Be \textbf{open}}
   \item \textit{Be \textbf{scalable}}
\end{enumerate}

\subsection{Resource Accessibility}
In a distributed system the access, and share, of remote resources is of paramount importance.
\textbf{Resources} can be virtually anything (peripherals, storage facilities, data, \dots).

Connecting users and resources makes easier to collaborate and exchange information (hint: look at the success of the Internet).

File-sharing used for distributing large amounts of data, software updates, and data synchronization across multiple servers.

\subsection{Hide Distribution}
\textbf{Hiding distribution} is a fundamental goal in the design of distributed systems, and is related to an already mentioned key point, \textit{distribution transparency}, but does not limit to it.\\
It means to hide that processes and resources are physically distributed across multiple computers possibly separated by large distances.\\
More precisely, it means to enforce the following properties in \ref{tag:02/hide_distribution}:

\begin{table}[htbp]
   \centering
   \begin{tabular}{|l|l|}
      \texttt{ACCESS} & Hide differences in data representation and how an object is accessed\\
      \texttt{LOCATION} & Hide where an object is located\\
      \texttt{RELOCATION} & Hide that an object may be moved to another location while in use\\
      \texttt{MIGRATION} & Hide that an object may move to another location\\
      \texttt{REPLICATION} & Hide that an object is replicated\\
      \texttt{CONCURRENCY} & Hide that an object may be shared by several independent users\\
      \texttt{FAILURE} & Hide the failure and recovery of an object\\
   \end{tabular}
   \caption{Hide Distribution properties}
   \label{tab:02/hide_distribution}
\end{table}

\subsubsection{Access Transparency}
Different systems have different ways to represent and access data.
A well designed distributed system needs to hide differences in:
\begin{itemize}
   \item physical machine architectures
   \item data representation by different operating systems
\end{itemize}

A distributed system may have computer systems that run
different operating systems, each having their own file-
naming conventions.

\note{Differences in naming conventions, file operations, or in
low-level communication mechanisms with other
processes, etc}

\subsubsection{Location and Relocation Transparency}

Location transparency means that users can access
resources even are not aware where an object is physically
located in the system.

Naming plays an important role by assigning logical names to
resources, i.e. names not providing information about physical location.

Basically, naming is an indirection process!

\note{\textit{Shardcake} is a Scala library that provides location transparency for distributed systems, by exploiting ``Entity Sharding''.}

An example of a such a name is the uniform resource locator
(URL) \href{http://www.unipi.it}{http://www.unipi.it}, which gives no information about the actual location of University’s main Web server.

The entire site may have been moved from one data center to another, yet users should not notice, that is an example of relocation transparency, which is becoming increasingly important in the context of cloud computing.
\note{Actually this is not the case for UniPi hihi \smiley.}

\subsubsection{Migration Transparency}
Relocation transparency refers just to being moved across the distributed system, migration transparency is:
\begin{itemize}
   \item offered by a system when it supports the mobility of processes and resources initiated by users
   \item does not affect ongoing communication and operations
\end{itemize}

A common example is communication between \textit{mobile phones}:
regardless whether two people are actually moving, mobile phones will allow them to continue their conversation teleconferencing using devices that are equipped with mobile Internet.

\subsubsection{Replication Transparency}
Resources may be replicated to increase availability or to improve performance by \ul{\textit{placing a copy close} to the place where it is accessed}.

Hide the existence of copies of a resource or that processes are operating in some form of lockstep mode so that \ul{one can take over when another fails}.

To hide replication to users, it is necessary that all replicas have the same name.

Systems that \ul{support replication transparency should support location transparency as well.}

\subsubsection{Concurrency Transparency}
Two independent users may each have stored their files on the same file server or may be accessing the same tables in a shared database.

It is important that each user does not notice that the other is making use of the same
resource, this phenomenon is called concurrency transparency

\begin{itemize}
   \item concurrent access to a shared resource leaves that resource in a \textbf{consistent} state
   
   \item consistency achieved through locking mechanisms to give users \textbf{exclusive access} to a
   resource
\end{itemize}
   
A more refined mechanism is based on \textbf{transactions}, but may strongly impact on scalability.

\subsubsection{Failure Transparency}
Failure transparency is the ability of a system to mask the failures of components from users, so a user ---or an application--- does not notice that some piece of the system failed and that the system subsequently (and automatically) \textbf{recovers} from that failure.

Masking failures is one of the hardest issues in distributed systems and is even impossible when certain assumptions are made.

The main difficulty in masking and transparently recovering from failures is in the inability to
distinguish between a dead process and a slowly responding one.

\note{\textit{``Is the server is actually down or is the network too badly congested ?''}
It is often not easy to tell the difference.}

\subsection{Degree of distribution transparency}
Distribution transparency is generally considered preferable for any distributed system, however there is a \ul{\textbf{trade-off} between a \textit{high degree of transparency} and the \textit{performance} of a system}.

There are situations in which attempting to blindly hide all distribution aspects from users is \textit{not} a good idea.

\labelitemize{\textit{examples}}{
   \begin{itemize}
      \item Internet applications repeatedly try to contact a server before finally giving up, as       attempting to mask a transient server failure before trying another one may slow down the system as a whole.
      \item Guarantee that replicas, located on different continents, must be consistent all the time,
      may be costly
      \note{a single update operation may take seconds to complete, that \ul{cannot be hidden from users}}
   \end{itemize}
}

{In other situations it is \textit{not at all obvious} that hiding distribution is ``not'' a good idea.\ns
\begin{itemize}
   \item devices that people carry around and where the very notion of location and context awareness is becoming increasingly important
   e.g., finding the nearest restaurant
   \item when working real-time on shared documents concurrency transparency could hinder the cooperation
\end{itemize}}

There \ul{are also other arguments against distribution transparency}.
Recognizing that full distribution transparency is \textit{impossible}, we should ask ourselves whether it is wise to pretend to achieve it.

In some cases, it may be better to make distribution \textbf{explicit} so that:
the user and application developer are never tricked into believing
that there is such a thing as transparency, resulting in users much better understanding the
behavior of a distributed system, and thus prepared to deal with its behavior.

\section{Openness}

An open distributed system is essentially a system that offers components that can easily be used-by, or integrated, into other systems. 
An open distributed system itself will often consist of components that originate from elsewhere.

Being open enables two key features:
\begin{itemize}
   \item Interoperability, composability, and extensibility
   \item Separation of policies from mechanisms
\end{itemize}

Open means that components adhere to standard rules describing the syntax and semantics of those components usually by relying on an Interface Definition Language (IDL).
An interface definition allows an arbitrary process that needs a certain interface, to interact with another process that provides that interface.
This allows two independent parties to build completely different implementations of those interfaces.

Proper specifications are \textbf{complete} and \textbf{neutral}.
\textit{Complete} means that everything that is necessary to make an implementation has indeed been specified.
However\dots many interface definitions are not at all complete so that it is necessary for a developer to add implementation-specific details.
This is just as important is the fact that specifications \textbf{do not prescribe what an implementation should look like}, they should be \textit{neutral}.



As pointed out in Blair and Stefani, completeness and neutrality are important for \textbf{interoperability} and \textbf{portability}.
\begin{itemize}
   \item \textbf{Interoperability} - characterizes the extent by which two implementations of
   systems from different manufacturers can work together
   \begin{itemize}
      \item by relying on each other’s services
      \item as specified by a common standard
   \end{itemize}
   \item \textbf{Portability} - characterizes to what extent an application developed for a
   given distributed system can executed
   \begin{itemize}
      \item without modification
      \item on a different distributed system implementing the same interfaces
   \end{itemize}
\end{itemize}


Another important goal for an open distributed system is that it should be \ul{easy to \textbf{configure} the system out of different components} (possibly from different developers).
Also, it should be easy to \textbf{add} new components or replace existing ones without affecting those components that stay in place.\\
In other words, an open distributed system should also be \textbf{extensible}.

\note{For example, in an extensible system, it should be relatively easy to add parts that run on a different operating system, or even to replace an entire file system}

\subsection{Policy and Mechanism Separation}

To achieve flexibility in open distributed systems, it is crucial that the system be organized as a collection of relatively small and easily replaceable or adaptable components.

A fundamental principle for achieving this flexibility is the \textbf{separation of policy and mechanism}:
\begin{itemize}
   \item \textbf{Mechanism} defines \textit{how} something is done - the low-level implementation that provides capabilities and operations
   \item \textbf{Policy} defines \textit{what} and \textit{when} to do - the high-level decision-making rules and strategies
\end{itemize}

By separating these concerns, the system becomes more flexible: policies can be changed or adapted without modifying the underlying mechanisms, and different policies can use the same mechanisms.

\textbf{Examples in distributed systems}:
\begin{itemize}
   \item \textit{Load balancing}: The mechanism provides request routing and node communication, while the policy decides which server receives each request (e.g., round-robin, least-loaded, geographic proximity)
   \item \textit{Caching}: The mechanism implements the cache data structure and access operations, while the policy decides which items to evict (e.g., LRU, LFU, FIFO)
   \item \textit{Resource allocation}: The mechanism manages resource availability and assignment, while the policy determines priorities and quotas for different users or applications
   \item \textit{Security}: The mechanism implements authentication and access control infrastructure, while the policy defines who can access what resources and under which conditions
\end{itemize}

This implies providing definitions not only for the highest-level interfaces (those seen by users and applications), but also for interfaces to internal parts of the system, describing how those parts interact. This approach is an alternative to the classical monolithic approach in which components are implemented as one huge program, making it hard to replace or adapt a component without affecting the entire system.

\section{Scalability}
We were used to having relatively powerful desktop computers for office applications and storage.

We are now witnessing that such applications and services are being placed ``in the cloud'',leading in turn to an increase of much smaller networked devices such as tablet computers.

With this in mind, scalability has become one of the most important design goals for developers of distributed systems.

\subsection{Dimensions of Scalability}
Scalability is a complex issue and can be seen from different perspectives, such as:
\begin{itemize}
\item \textbf{Size} - A system can be scalable with respect to its size i.e., we can add more users and resources to the system without any noticeable loss of performance.
\item \textbf{Geographical} - A geographically scalable system is one in which the users and resources may be distant, but communication delays are hardly noticed.
\item \textbf{Administrative} - An administratively scalable system is one that can still be easily managed even if it spans many independent administrative organisations
\end{itemize}

\subsection{Size scalability}
Many \textbf{users} need to be supported $\rightarrow$ limitations of centralized services.

Many services are \textbf{centralized} $\rightarrow$ implemented by a single server running on a specific machine or in a group of collaborating servers co-located on a cluster in the same location.

The problem with this scheme is obvious: the server, or group of servers, can become a \textbf{bottleneck} due to three root causes:
\begin{itemize}
   \item The computational capacity, limited by the CPUs [CPU bound]
   \item The storage capacity, including the I/O transfer rate [I/O bound]
   \item The network between the user and the centralized service
   [Network bound]
\end{itemize}

\subsection{Geographical scalability}

TLDR: Solutions developed for local-area networks cannot always be easily ported to a wide-area system.

This kind of scalability relates on the difficulties in scaling existing distributed systems that are \textbf{designed for local-area networks}, many of them even based on synchronous communication e.g., a party requesting service blocks until a reply is sent back from the server implementing the service.

Communication patterns are often consisting of many client-server interactions as may be the case with database transactions;
this approach generally works fine in LANs where communication between two machines is often just a few hundred microseconds, but does not scale to WANs where communication may take hundreds of milliseconds.
Besides in WANs, the probability of packet loss is much higher than in LANs, and the bandwidth is much lower.

\subsection{Administrative scalability}
This addresses how to scale a distributed system across multiple, independent administrative domains.
\note{A major problem that needs to be solved is that of conflicting policies with
respect to resource usage (and payment), management, and security}

\section{How to scale?}
Scalability problems in distributed systems appear as performance problems caused by limited capacity of servers and network. 
Improving their capacity (e.g., by increasing memory, upgrading CPUs, or replacing network modules) is referred to as \textbf{scaling up}, while \textbf{scaling out} refers to deploying more servers.

There are three main strategies to \textit{scale out}, they are discussed in the following sections.

\subsection{Hiding Communication Latencies}
Applies in the case of geographical scalability, and aims at avoiding waiting for responses to remote-service requests.
Instead waiting for a reply, do other useful work at the requester side, and when the reply arrives invoke a special \textit{handler};
in other words, implement \textbf{asynchronous communication}.\\
This is very much used in batch-processing systems and parallel applications, contexts where indenpendent tasks can be  scheduled for execution while another task is waiting for communication to complete.

In some scenarios asynchronous communication does not fit; a solution is to move part of the computation from server to client. This motivates \textit{``hierarchical approaches''}. This is the foundation of \textbf{edge-computing}.


\subsection{Distributing Work}

Another scaling technique is partitioning and distribution: taking a component, splitting it into smaller parts, and subsequently spreading those parts across the system.

A very simple example is the World Wide Web: to most users the web appears to be an enormous document-based information system, but in reality it is a distributed system in which the documents are stored on many different servers.

\subsection{Replication - Caching}
Scalability problems often appear in the form of \textbf{performance degradation}, thus it is generally a good idea to actually replicate components across a distributed system. Replication increases \textbf{availability} and also helps to \textbf{balance the load} between components, \ul{leading to better performance}.
\note{Also, in geographically widely dispersed systems, having a copy nearby can hide much of the \textit{communication latency} problems mentioned before.}

\subsubsection{Caching}
Caching results in making a copy of a resource, generally in the proximity of the client accessing that resource. In contrast to replication, \ul{caching is a decision made by the \textit{client} of a resource and \textbf{not} by the \textit{owner} of a resource.}

The most serious drawback to caching and replication in general is handling the \textbf{inconsistency} that may arise when a copy of a resource is updated.
Incostincency occurs always, and to what extent it can be tolerated depends highly on the usage of a resource.
\note{Seeing a cached web page, old of a few minutes, is acceptable. Old Stock-exchanges are not.}

\framedt{Non-scalability}{
   \textbf{Strong-consistency} is difficult to enforce. If two updates happen \textit{concurrently}, it is required that updates are processed in the same \textbf{order} everywhere, introducing an additional global ordering problem.
   Besides, combining strong consistency with high availability is, in general, impossible.
   
   Global synchronization mechanisms are typically not scalable. So\dots

   \ul{\textit{Scaling by replication may introduce inherently non-scalable solutions}}.
   
   This is a fundamental paradox: replication is used to \textit{improve} scalability, but maintaining strong consistency across replicas requires global coordination, which is \textit{inherently non-scalable}. The more replicas you add, the more coordination overhead is needed:
   \begin{itemize}
      \item Every update must be propagated to all replicas and confirmed before completion
      \item With N replicas, coordination involves N nodes, increasing communication overhead
      \item Latency is determined by the slowest replica (geographical distance matters)
      \item The coordination mechanism itself becomes a bottleneck
   \end{itemize}
   
   \textbf{Example}: A database replicated across 3 continents to improve availability. With strong consistency, each write must wait for confirmation from all data centers (~200-300ms inter-continental latency × 3), making writes very slow. Adding a 4th replica makes it worse.
   
   This leads to the \textbf{CAP theorem}: you cannot simultaneously have strong \textit{Consistency}, high \textit{Availability}, and \textit{Partition tolerance}. Most modern distributed systems choose eventual consistency instead of strong consistency to maintain scalability and availability.
}



\subsection{Pitfalls}
Developing a scalable and distributed system is a formidable task.
Resources \textbf{dispersion} must be taken into account at design time, otherwise the system will be complex and flawed.

Whenever someone approaches designing a distributed system for the first time, it is easy to make mistakes. In fact, Peter Deutsch, in his famous fallacies of distributed computing, lists the following fallacies that are often made by developers of distributed systems:
\begin{itemize}
   \item Network is reliable, secure and homogeneous
   \item The topology does not change
   \item Latency is zero, bandwidth is infinite
   \item Transport cost is zero
   \item There is one administrator
\end{itemize}

This happens because this issues, when developing non-distributed applications, most likely do not show up.

\note{Some latency-sensitive applications are:
\begin{itemize}
   \item Online gaming
   \item Video conferencing
   \item Remote surgery
   \item \dots % don't remeber the 4th, but it is not important
\end{itemize}}


\section{Types of distributed systems}

\subsection{HP(D)C - Clusters}
Collection of similare workstations closely connected by means of a high-speed network, used to solve large-scale problems.
Nodes typically run the same operating system and are somehow \textbf{homogeneous}.

Clusters are used in the most important supercomputers in the world.

\ul{Clusters are always \textit{preferrable} to a single super-powerful machine}, but not only because they are cheaper and reliable, but because they easily allow for \textbf{horizontal scalability}.

\framedt{
   Beowulf clusters
}
{Linux-based Beowulf clusters are cheap and easy to build, and are used in many scientific applications.}
% //TODO Mosix

\subsection{HP(D)C - Grid}
Still HPC, but here the nodes belong to different administrative domains, and may be geographically distributed.
Grid computing consists of distributed systems that are often constructed as a \textbf{federation} of computer systems.\\
Clearly, the nodes in a grid are \textbf{heterogeneous} and may run different operating systems.

Heterogeneity actually may be advanteageous, since different workloads may be better suited to different types of machines.

\note{\textbf{Globus} is an architecture initially proposed by Foster and Kesselman, and is one of the most widely used grid middleware systems.}


\subsection{Cloud}

Cloud computing is a model outsourcing the entire infrastructure.
The key point is the providing the facilities to dynamically construct an infrastructure and compose what is needed from available resources.\\
This is not really about being HPC, but about being able to scale up and down as needed, and in general providing lots of resources.

The father of \textit{cloud} was the the concept of \textbf{utility computing}\footnote{\textit{``utilities''} in english are water, gas, electricity, etc\dots}., by which a customer could upload tasks to a data center and be charged on
a per-resource basis.

\labelitemize{Types}{
\begin{itemize}
   \item \textbf{IaaS} - Infrastructure as a Service
   \item \textbf{PaaS} - Platform as a Service
   \item \textbf{SaaS} - Software as a Service
   \item \textbf{FaaS} - Function as a Service
   \item \textit{Many-other-stuff as a service}, such as Backend aaS, Database aaS, etc\dots
\end{itemize}}

{Cloud computing is very popular, but there a few issues concerning it:\ns
\begin{itemize}
   \item Lock-in - Once you start using a cloud provider, it is hard to switch to another one
   \note{Prof. Dazzi says that in general, it is cheaper to push data in the cloud, but more costful to pull it.}
   \item Security and privacy issues - You are giving your data to someone else
   \item Dependence on the network - If the network resources ---or simply the network--- go down, you are in trouble
\end{itemize}}

\section{Pervasive systems}

The distributed systems discussed so far are largely characterized by their stability:
nodes are fixed and have a more or less permanent and high-quality connection to a network.
To a certain extent, this stability is realized through the various techniques for achieving distribution transparency

However, matters have changed since the introduction of mobile and embedded computing devices, leading to what are generally referred to as \textbf{pervasive systems}.\\
The separation between users and system components is much more \textit{blurred}.
Typically there is no single dedicated interface, such as a screen/keyboard combination, and in the system there may be many \textbf{sensors} picking up various aspects of a user's behavior.

Many devices in pervasive systems are characterized by being \textbf{small}, battery-powered, mobile, and a wireless connection.
\note{These are not necessarily \textit{restrictive} aspects, consider Smartphones, for instance.}

We may distinguish three types of pervasive systems, which may overlap:
\begin{enumerate}
   \item Ubiquitous computing systems
   \item Mobile systems
   \item Sensor networks
\end{enumerate}