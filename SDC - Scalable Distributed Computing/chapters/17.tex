\chapter{Data Processing}

Data may be processed in either \textbf{batch} or \textbf{real-time} (\textit{stream}) fashion, or a combination of both. Batch processing is used when data can be collected and processed in a single operation, while real-time processing is used when data must be processed as soon as it is generated.

\section{Batch Processing}
This approaches relaxes the need for producing results as soon as data arrives, storing it in a buffer to later process it in chunks.

Batch processing is typically cheaper and easier to implement than real-time processing, but it may not be suitable for all use cases, but is very effective for data transformations, generate reports, and other tasks that do not require immediate results.

We have high latency, but also high throughput, and the system is easier to reason about, however, \ul{it lacks the ability for real-time decision making}.

The key paradigm here is clearly MapReduce, and the key frameworks are Hadoop and Spark.

Batch processing is characterized by a trade-off between throughput and latency;
it typically has high throughput but can have high latency depending on the size and complexity of the workload.

\section{Stream Processing}

Here the focus is on processing data as soon as it is generated, providing low-latency results, and thus enabling near-real-time processing.\\
Stream processing is designed to handle unbounded datasets, such as those generated ---typically in a continuous flow--- by IoT devices, allowing to compute real-time analytics, monitoring, and decision-making.

\subsection{Challenges}

Stream processing requires efficient \textbf{state management} in a system with high data velocity.\\
\textbf{Fault tolerance} is a key challenge in stream processing due to the potential for data loss if a node fails.\\
\textbf{Consistency} for data spread across multiple nodes is not trivial to achieve.\\
Furthermore, stream processing system feature the \textbf{windowing} concept, which is powerful, but bears with it the need to define proper \textbf{window boundaries}.


Data consistency refers to the accuracy and correctness of data over time.
In stream processing, \ul{data \textbf{consistency} is crucial to ensure that real-time analytics are accurate and reliable}. 
There are three main types of data consistency models used in stream processing systems:
\begin{itemize}
   \item \textbf{Exactly-once} processing ensures that each data item is processed exactly once, preventing duplicates or omissions. This is the most desirable level of consistency, but it can be challenging to achieve in distributed systems.
   \item \textbf{At-least-once} processing guarantees that each data item is processed at least once, but it may be processed multiple times, leading to potential duplicates.
   \item \textbf{At-most-once} processing ensures that each data item is processed at most once, but it may be lost if there are failures in the system.
\end{itemize}


\subsection{Time semantics}

In stream processing systems, understanding different time semantics is crucial for correctly handling events, especially when dealing with out-of-order data or latency issues. The three main time concepts are defined from the \textbf{perspective of the stream processing system} (e.g., Flink, Kafka Streams, Spark Streaming):

\begin{itemize}
   \item \textbf{Event time}: The timestamp when the event \textit{actually occurred} in the real world, typically embedded in the event data itself. For example, when a sensor detects a temperature reading, the event time is the moment the sensor took that measurement. This is the most accurate representation of when something happened.
   
   \item \textbf{Ingestion time}: The timestamp when the event \textit{enters the stream processing system} (e.g., when it arrives at the Kafka broker or is read by the streaming application). There may be delays between event time and ingestion time due to network latency, batching, or other factors.
   
   \item \textbf{Processing time}: The timestamp when the stream processing system \textit{actually processes} the event (e.g., when a Flink operator applies a transformation or aggregation to it). This is the system's wall-clock time at the moment of processing. Processing time can differ significantly from event time due to system load, backpressure, or processing delays.
\end{itemize}

\textbf{Example scenario}: A temperature sensor records 25Â°C at \texttt{10:00:00} (event time). Due to network delays, this data reaches Kafka at \texttt{10:00:05} (ingestion time). The Flink application processes this event at \texttt{10:00:10} (processing time) because it was busy handling previous events.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=3cm,
    box/.style={rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=0.8cm, align=center, font=\small},
    event/.style={box, fill=blue!20},
    system/.style={box, fill=green!20},
    arrow/.style={->, >=stealth, thick}
]

% Nodes
\node[event] (sensor) {Sensor\\Event Occurs};
\node[system, right=of sensor] (ingestion) {System\\Receives Event};
\node[system, right=of ingestion] (processing) {System\\Processes Event};

% Timeline
\draw[thick, gray] ([yshift=-1.5cm]sensor.south) -- ([yshift=-1.5cm]processing.south);
\foreach \x in {0,1,2,3} {
    \draw[gray] ([yshift=-1.5cm, xshift=\x*2.8cm]sensor.south) -- ++(0,-0.15);
}

% Time labels
\node[below, font=\scriptsize] at ([yshift=-1.65cm]sensor.south) {10:00:00};
\node[below, font=\scriptsize] at ([yshift=-1.65cm]ingestion.south) {10:00:05};
\node[below, font=\scriptsize] at ([yshift=-1.65cm]processing.south) {10:00:10};

% Arrows
\draw[arrow, blue!70] (sensor) -- node[above, font=\tiny] {network delay} (ingestion);
\draw[arrow, green!70!black] (ingestion) -- node[above, font=\tiny] {processing delay} (processing);

% Time type labels
\node[below=0.3cm of sensor, font=\scriptsize\bfseries, blue!70!black] {Event Time};
\node[below=0.3cm of ingestion, font=\scriptsize\bfseries, green!70!black] {Ingestion Time};
\node[below=0.3cm of processing, font=\scriptsize\bfseries, orange!70!black] {Processing Time};

\end{tikzpicture}
\caption{Time semantics in stream processing: the lifecycle of an event from occurrence to processing}
\label{fig:17/time-semantics}
\end{figure}

The choice of time semantics affects windowing, aggregations, and correctness guarantees. \ul{Event time is preferred for accurate analytics}, but requires handling out-of-order events and late arrivals. Processing time is simpler but can produce incorrect results if events are delayed.

\textbf{Watermarks} are used to track the progress of event time, and to determine when to emit results. They act as markers indicating that no more events with timestamps earlier than the watermark should be expected (or that such events will be considered late). In general, watermarks aim to manage late-arriving data by providing a mechanism to balance between result accuracy and latency.

In \textbf{Stream} processing there are \ul{\textbf{Transactions}, which are atomic operations between streams}.
They are implemented with two-phase commit protocols, where a coordinator manages the transaction and ensures that all participants either commit or abort the transaction together, or idempotent writes, i.e. writing the same data multiple times has the same effect as writing it once.\\
To address Fault Tolerance in stream processing, systems often implement checkpointing and state backends, or log replay, to periodically save the state of the stream processing application. This allows the system to recover from failures by restoring the state from the last checkpoint, minimizing data loss and ensuring continuity in processing.\\
Clearly it may happen also that \ul{events arrive out of order}, and we need to handle this, either by buffering to wait for more data, or by allowing lateness in \textit{windowing}.

\begin{itemize}
	\item \textbf{Time-based windowing} is used to group events into windows based on time, and to process them in parallel. Windows can be fixed or sliding, and can be based on event time or processing time.
	\item \textbf{Count-based windowing} instead is used to group events into windows based on the number of events, but the same principles apply.
	\item \textbf{Session-based windowing} groups events based on inactivity periods.
\end{itemize}

\section{Lambda architectures}

\begin{figure}[htbp]
   \centering
   \includegraphics{images/17/lambdaarchitecture.png}
   \caption{Lambda architecture schema}
   \label{fig:17/lambdaarchitecture}
   Data sources simultaneously feed both the batch layer and the speed layer. The batch layer processes the data in large chunks, while the speed layer processes the data in real-time.
   The serving layer then combines the results from both layers to provide a unified view of the data to the end-users.
   It gets data directly from the batch layer, while it uses queries to the speed layer to get real-time updates.

\end{figure}

A \textbf{lambda architecture} is a data processing architecture designed to handle massive quantities of data by taking advantage of both \textit{batch} and \textit{stream} processing methods. 
{It consists of three layers:\ns
\begin{itemize}
   \item The \textbf{batch layer} is responsible for processing large volumes of data in a fault-tolerant and scalable manner. It is used to generate views of the data that can be queried by the serving layer.
   \item The \textbf{speed layer} is responsible for processing real-time data in a low-latency manner. It is used to generate real-time views of the data that can be queried by the serving layer.
   \item The \textbf{serving layer} is responsible for serving queries on the views generated by the batch and speed layers. It is used to provide real-time access to the data to the end-users through APIs and various tools.
\end{itemize}
}

The architecture provides a unified view of both batch (hence, historical) and real-time data processing, possibly handling large volumes of data and scaling horizontally.
However it may be complex to implement and maintain, and may generate redundant data processing and storage costs.

Lambda architecture requires careful coordination between batch and speed layers.
The architecture may even need to be redesigned or updated as data processing needs evolve.

\section{Kappa architectures}
Lambda difficulties in implementation led to the development of \textsc{Kappa}, which simplifies the architecture by removing the batch layer, focusing on streams, ultimately offering better scaling and fault tolerance than lambda.\\
\textsc{Kappa} relies on recomputing state from streams rather than storing it.
It exploits checkpointing and state backends to manage state.

\begin{figure}[htbp]
   \centering
   \includegraphics{images/17/kappaarchitecture.png}
   \caption{Kappa Architecture}
   \label{fig:17/kappaarchitecture}
\end{figure}

A \textsc{Kappa} architecture is a data processing architecture designed to handle massive quantities of data by taking advantage of stream processing methods. It consists of a single layer that processes both batch and real-time data in a fault-tolerant and scalable manner.

The main components, as shown in Figure~\ref{fig:17/kappaarchitecture}, are:
\begin{itemize}
   \item \textbf{Kafka Cluster (Input Topic)}: Acts as the immutable event log that stores all incoming data. This distributed messaging system provides durability, scalability, and fault tolerance for the data stream. All events are retained for a configurable period, enabling replay and reprocessing.
   
   \item \textbf{Stream Processing System}: The core computation layer that processes events from Kafka in real-time. Multiple job versions (e.g., \texttt{job\_version\_n}, \texttt{job\_version\_n+1}) can run simultaneously, allowing for seamless updates and deployments without downtime. This enables A/B testing and gradual migration to new versions.
   
   \item \textbf{Serving DB (Output Tables)}: Stores the materialized views and results produced by the stream processing jobs. Different job versions write to separate output tables, allowing clients to query the appropriate version. This database must support high write throughput and low-latency reads.
   
   \item \textbf{App (Client)}: End-user applications that query the serving database to access processed results. The app can send queries to specific output table versions based on requirements.
\end{itemize}

\textsc{Kappa} is very well suited for processing data from IoT devices, or analytics in general, where continuous stream processing is the primary requirement.

It is still more complex than traditional stream processing techniques, but it is more efficient and scalable.

\section{Other Architectural flavors}
\subsection{CQRS - Command Query Responsibility Segregation}


\begin{paracol}{2}
   
   \colfill
   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.95\columnwidth]{images/17/cqrs.png}
      \caption{CQRS architecture}
      \label{fig:17/cqrs}
   \end{figure}
   \colfill

   \switchcolumn
   \colfill

   \coolquote{
      What happens if i have many writes and few reads?
   }{}
   
   {\textbf{Command Query Responsibility Segregation} (\texttt{CQRS}) is a design pattern that separates the read and write operations of a system into two separate components, along with a UI to interact with them:\ns
   \begin{itemize}
      \item The \textbf{command model} (write) is responsible for handling the write operations of the system, such as creating, updating, and deleting data.
      Hence it is responsible for altering the system \textbf{state}.
      \item The \textbf{query model} (read) is responsible for handling the read operations of the system, such as retrieving data.
   \end{itemize}}

   This allows for better scalability, performance, and maintainability of the system.
   
   This looks good on the paper, but it is not always easy to implement, and may lead to inconsistencies between the read and write models, alongside with latency and communication overhead.
   \colfill
\end{paracol}

\subsection{Shared-Nothing}
A shared-nothing architecture is a distributed computing architecture in which \ul{each node is \textbf{self-sufficient} and has its own private memory and storage}.
The \ul{nodes communicate with each other by \textbf{passing messages}}.
This seems to nicely fit the Actor Model approach, as each actor can be seen as a node in a shared-nothing architecture.
In this paradigm, data is split across multiple nodes, with each node responsible for a subset of the data, on which it has complete control.

This architecture allows for horizontal scaling by
adding more nodes to the cluster, enabling also fault-tolerace, as a failure in one node does not affect the rest of the cluster.\\
In general its design favors high availability and low-latency.

This is cool and has many advantages, but it also poses challenges in terms of complexity, communication overhead, and data partitioning strategies.
Despite offering a high degree of flexibility, it may be complex to standardize and maintain.

Shared-nothing architectures are commonly used for big data processing and web applications.
\note{Hadoop and DynamoDB exploit shared-nothing architectures.}

\section{Data Processing Frameworks}

% Wrap-up table

\begin{table}[htbp]
\centering
\scriptsize
\setlength{\arrayrulewidth}{0.8pt}
\renewcommand{\arraystretch}{1.3}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|>{\raggedright\arraybackslash}p{1.8cm}|>{\raggedright\arraybackslash}p{2.8cm}|>{\raggedright\arraybackslash}p{2.8cm}|>{\raggedright\arraybackslash}p{2.8cm}|>{\raggedright\arraybackslash}p{2.8cm}|}
\hline
\textbf{Feature} & \textbf{Lambda Architecture} & \textbf{Kappa Architecture} & \textbf{CQRS} & \textbf{Shared-Nothing Architecture} \\
\hline
\textbf{Description} & Combines batch and stream processing for unified data views. & Focuses solely on stream processing, eliminating batch layers. & Separates read (queries) and write (commands) operations. & Decentralized nodes with no shared resources. \\
\hline
\textbf{Primary Use Case} & Mixed workloads requiring both batch and real-time data processing. & Real-time data streams, such as IoT or continuous analytics. & Systems with high read/write loads or complex domain logic. & Large-scale distributed systems requiring high scalability. \\
\hline
\textbf{Key Components} & Batch Layer, Speed Layer, Serving Layer. & Stream Processor, State Management, Event Logs. & Command Model, Query Model, Event Store. & Independent nodes, Partitioned Data, Local State. \\
\hline
\textbf{Data Consistency} & Consistency eventually across batch and stream layers. & Eventual consistency in state and data views. & Separate consistency models for commands and queries. & Relies on consistency protocols (e.g., eventual or strong). \\
\hline
\textbf{Scalability} & High scalability but requires coordination across layers. & Naturally scalable due to single data pipeline. & High scalability with independent query and write paths. & Horizontal scalability with no shared bottlenecks. \\
\hline
\textbf{Latency} & Low for speed layer; higher for batch processing. & Low latency, designed for real-time processing. & Low latency for reads, but can have write overhead. & Minimal latency in read/write, depends on partitioning. \\
\hline
\textbf{Fault Tolerance} & High, with redundancy in both batch and stream layers. & High, with focus on replaying events for state recovery. & Fault tolerance depends on event sourcing mechanisms. & High, as nodes operate independently. \\
\hline
\textbf{Complexity} & High, due to the need to manage two processing layers. & Moderate, simpler than Lambda but requires stream expertise. & High, due to synchronization between command and query models. & High, complexity in partitioning and load balancing. \\
\hline
\textbf{Advantages} & Unified view of historical and real-time data. & Simplified architecture with real-time focus. & Optimized read/write paths; flexibility in scaling. & High scalability and resilience; no single point of failure. \\
\hline
\textbf{Disadvantages} & Complexity in maintenance; redundant storage. & Limited to real-time use cases; relies heavily on logs. & Higher implementation cost; requires expertise. & Difficult to ensure consistency and partitioning strategy. \\
\hline
\textbf{Examples} & Log analysis, fraud detection, hybrid analytics. & IoT sensor data processing, real-time analytics. & E-commerce systems, banking transaction systems. & Global-scale distributed databases (Cassandra, DynamoDB). \\
\hline
\textbf{Best For} & Scenarios needing both batch and real-time processing. & Systems requiring low-latency, real-time stream data. & High-volume, complex, or read-heavy applications. & Large-scale distributed systems with decentralized data. \\
\hline
\end{tabular}%
}
\caption{Comparison of Data Processing Architectures}
\label{tab:dataprocessingarchitectures}
\end{table}



\subsection{Data Lakehouse}

This is a modern data architecture that combines the best features of data lakes and data warehouses. It allows for the storage of raw data in a data lake, and the processing of that data in a data warehouse.

A data lake is a centralized repository that allows for the storage of \textit{structured and unstructured} data at any scale. Its main use cases include data archiving, big data analytics, and machine learning.\\
A data warehouse, instead, is a centralized repository that allows for the storage of \textit{structured and processed data from multiple sources}. Its main use cases include business intelligence, reporting, and data analysis.
Differently from the data lake, the main focus of a data warehouse is on data quality, consistency, and performance, while a data lake is more about storing large volumes of raw data for future processing.


This architecture provides unified approach for structured, unstructured, and semi-structured data, and allows for the use of both batch and real-time processing methods.

An example is \textsc{Snowflake}, which is a cloud-based data warehouse that supports both batch and real-time processing methods.

\subsubsection{Data Mesh}
Data Mesh is a modern architectural approach designed to address the challenges of managing large-scale
and complex data systems, especially in distributed organizations. It emphasizes decentralization, domain-driven design, and treating data as a product.

\textbf{Key difference from Data Lakehouse}: While \textbf{Data Lakehouse} is a \textit{technical architecture} focused on \textit{where and how data is stored} (combining lake and warehouse capabilities in a unified platform), \textbf{Data Mesh} is an \textit{organizational and architectural paradigm} focused on \textit{who owns and manages data} (decentralized ownership by domain teams).

{Main characteristics:\ns
\begin{itemize}
	\item \textbf{Decentralization}: Data responsibility is distributed across domain teams, avoiding a centralized data team as a bottleneck. Each domain owns its data as a product.
	\item \textbf{Domain-oriented ownership}: Data is organized and owned by business domains (e.g., Sales, Marketing, Logistics), not by a central data platform team.
	\item \textbf{Data as a Product}: Each domain treats its data as a product with clear ownership, quality standards, and APIs for consumption.
	\item \textbf{Self-serve data infrastructure}: Provides platform capabilities that enable domain teams to manage their own data pipelines and products autonomously.
	\item \textbf{Federated computational governance}: Establishes global policies and standards while allowing domains autonomy in implementation.
	\item \textbf{Interoperability}: Data products across domains are designed to be interoperable through standard interfaces and contracts.
	\item \textbf{Scalability}: Scales naturally with organizational and data growth, as each domain independently manages its data.
	\item \textbf{Data Discovery}: Metadata and catalogs help discover data assets across the distributed domains.
	\item \textbf{Automation}: Strong reliance on automation for deploying and managing data pipelines.
	\item \textit{Example}: Netflix uses Data Mesh principles to provide teams with the autonomy to own their datasets while maintaining consistency and governance.
\end{itemize}}

\textbf{Data Lakehouse vs Data Mesh}: A Data Lakehouse can be used \textit{within} a Data Mesh architecture as the technical storage layer for individual domain data products. They are complementary: Lakehouse solves the \textit{``what storage technology''} problem, while Mesh solves the \textit{``how to organize teams and ownership''} problem.

\subsubsection{Choosing the right architecture}

\begin{itemize}
	\item \textbf{Lambda Architecture}: Best for systems needing both real-time and historical data processing.
	\begin{itemize}
		\item \textit{Example}: Log analysis systems combining real-time alerts and long-term trend reports, or big data analytics platforms.
	\end{itemize}
	
	\item \textbf{Kappa Architecture}: Ideal for real-time data applications with high throughput.
	\begin{itemize}
		\item \textit{Example}: IoT sensor data processing, where insights must be generated continuously.
	\end{itemize}
	
	\item \textbf{CQRS}: Suitable for applications with distinct read and write patterns, or complex domain logic.
	\begin{itemize}
		\item \textit{Example}: E-commerce platforms with high read traffic (product browsing) and write operations (orders).
	\end{itemize}
	
	\item \textbf{Shared-Nothing Architecture}: Perfect for distributed systems needing extreme scalability.
	\begin{itemize}
		\item \textit{Example}: Global-scale applications like social networks or e-commerce backends.
	\end{itemize}
\end{itemize}

\subsection{Federated Learning}

With federated learning, the model is trained on the \textbf{edge} devices, and the updates are sent to a central server. This allows for better privacy and security, as the data never leaves the edge devices.

This approach is well suited for IoT devices, as it allows for the training of models on the edge devices, and the updates are sent to a central server for aggregation.

\subsection{Serverless Data Processing}
Abstracts infrastrcture management, and allows for the deployment of data processing pipelines without the need to manage servers.
This paradigm is increasingly applied to data
processing workflows.

The key difference with traditional architectures is that \ul{serverless computing automatically manages the allocation of compute resources}, scaling them up or down based on demand.
There is no need to provision or manage servers, as the cloud provider takes care of that.
 