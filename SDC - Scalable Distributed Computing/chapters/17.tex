\chapter{Data Processing}

Data may be processed in either \textbf{batch} or \textbf{real-time} (\textit{stream}) fashion, or a combination of both. Batch processing is used when data can be collected and processed in a single operation, while real-time processing is used when data must be processed as soon as it is generated.

\section{Batch Processing}
This approaches relaxes the need for producing results as soon as data arrives, storing it in a buffer to later process it in chunks.

Batch processing is typically cheaper and easier to implement than real-time processing, but it may not be suitable for all use cases, but is very effective for data transformations, generate reports, and other tasks that do not require immediate results.

We have high latency, but also high throughput, and the system is easier to reason about, however, it lacks the ability for real-time decision making.

The key paradigm here is clearly MapReduce.

\section{Stream Processing}

Here the focus is on processing data as soon as it is generated.

\subsection{Time semantics}
\begin{itemize}
   \item \textit{Event} time - the time when the event occurred.
   \item \textit{Processing} time - the time when the event is processed. 
   \item \textit{Ingestion} time - the time when the event was received. 
\end{itemize}

Watermarks are used to track the progress of event time, and to determine when to emit results. In general allow for managing late-arrive data.

In Stream processing there are \ul{\textbf{Transactions}, which are atomic operations between streams}.
Clearly it may happen also that events arrive out of order, and we need to handle this, either by buffering to wait for more data, or by allowing lateness in \textit{windowing}.


\textbf{Time-based windowing} is used to group events into windows based on time, and to process them in parallel. Windows can be fixed or sliding, and can be based on event time or processing time.

\textbf{Count-based windowing} instead is used to group events into windows based on the number of events, but the same principles apply. \\
\textbf{Session-based windowing} groups events based on inactivity periods.

\subsection{Lambda architectures}

A lambda architecture is a data processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream processing methods. 
{It consists of three layers:\ns
%  the batch layer, the speed layer, and the serving layer.
\begin{itemize}
   \item The \textbf{batch layer} is responsible for processing large volumes of data in a fault-tolerant and scalable manner. It is used to generate views of the data that can be queried by the serving layer.
   \item The \textbf{speed layer} is responsible for processing real-time data in a low-latency manner. It is used to generate real-time views of the data that can be queried by the serving layer.
   \item The \textbf{serving layer} is responsible for serving queries on the views generated by the batch and speed layers. It is used to provide real-time access to the data to the end-users through APIs and various tools.
\end{itemize}
}

The architecture provides a unified view
of both batch (hence, historical) and real-time data
processing, possibly handling large volumes of data and scaling horizontally.
However it may be complex to implement and maintain, and may generate redundant data processing and storage costs.


\subsection{Kappa architectures}
Lambda difficulties in implementation led to the development of Kappa, which simplifies the architecture by removing the batch layer, focusing on streams, ultimately offering better scaling and fault tolerance than lambda.\\
Kappa relies on recomputing state from streams rather than storing it.
It exploits checkpointing and state backends to manage state.

Kappa is very well suited for processing data from IoT devices, or analytics in general.

A kappa architecture is a data processing architecture designed to handle massive quantities of data by taking advantage of stream processing methods. It consists of a single layer that processes both batch and real-time data in a fault-tolerant and scalable manner.

It is still more complex than traditional stream processing techniques, but it is more efficient and scalable.


\subsection{CQRS}
What happens if i have many writes and few reads?

{\textbf{Command Query Responsibility Segregation} (\texttt{CQRS}) is a design pattern that separates the read and write operations of a system into two separate components, along with a UI to interact with them:\ns
\begin{itemize}
   \item The \textbf{command model} (write) is responsible for handling the write operations of the system, such as creating, updating, and deleting data.
   \item The \textbf{query model} (read) is responsible for handling the read operations of the system, such as retrieving data.
\end{itemize}}
This allows for better scalability, performance, and maintainability of the system.


This looks good on the paper, but it is not always easy to implement, and may lead to inconsistencies between the read and write models, alongside with latency and communication overhead.

\section{Data Processing Frameworks}

\subsection{Sharing or not}

In shared-nothing architectures each is self-sufficient, has its own private memory and storage, and communicates with others using message passing.\\
These are typically used in big data processing and web applications, as they easily allow horizontal scaling.

Examples of shared-nothing architectures are Hadoop, DynamoDB, Couchbase, and Facebook TAO.

\subsection{Data Lakehouse}

This is a modern data architecture that combines the best features of data lakes and data warehouses. It allows for the storage of raw data in a data lake, and the processing of that data in a data warehouse.

This architecture provides unified approach for structured, unstructured, and semi-structured data, and allows for the use of both batch and real-time processing methods.

An example is Snowflake, which is a cloud-based data warehouse that supports both batch and real-time processing methods.

\subsection{Federated Learning}

With federated learning, the model is trained on the edge devices, and the updates are sent to a central server. This allows for better privacy and security, as the data never leaves the edge devices.

This approach is well suited for IoT devices, as it allows for the training of models on the edge devices, and the updates are sent to a central server for aggregation.

\subsection{Serverless Data Processing}
Abstracts infrastrcture management, and allows for the deployment of data processing pipelines without the need to manage servers.
This paradigm is increasingly applied to data
processing workflows.