\chapter{Partitioning}

\textbf{Partitioning} becomes necessary when dealing with \textbf{large data sets} and high \textbf{query throughput}. It is a technique that divides a large data set into smaller, more manageable parts. This is done to improve the performance of the system. Partitioning can be done in various ways, such as horizontal partitioning, vertical partitioning, and functional partitioning. In this chapter, we will discuss the different types of partitioning and their benefits.

``Partition'' is the most common term, but it may vary depending on the technology:
\begin{itemize}
   \item \textbf{Shard} in MongoDB, Elasticsearch, and Cassandra.
   \item \textbf{Region} in HBase.
   \item \textbf{VBucket} in Couchbase.
   \item \textbf{Tablet} in Bigtable.
   \item \textbf{VNode} in Riak and Cassandra.	
\end{itemize} 

\section{Partitioning concepts}
First, partitions must be \textbf{defined}, in the sense that each piece of data must be assigned to a partition. This can be done in various ways, such as hashing, range partitioning, or list partitioning.

Then, each partition should have its own \textbf{characteristics}, i.e. should support a known set of operations, since it acts as a small per se database.

Clearly, different partitions may reside on different nodes, enabling scalability.


\subsection{Combining partitioning with replication}

\begin{paracol}{2}
   \colfill
   Partitioning can be combined with replication to improve the performance and reliability of the system. Replication is the process of creating multiple copies of the data and storing them on different nodes. This ensures that the data is available even if one of the nodes fails. \ul{By combining partitioning with replication, you can achieve high availability and fault tolerance.}
   \colfill
   
   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.95\columnwidth]{images/11/leader_follower.png}
      \caption{Leader-follower model}
      \label{fig:11/leader_follower}
   \end{figure}
\end{paracol}

This can be achieved in various ways:
\begin{enumerate}
   \item Node Storage of Multiple Partitions
   \begin{itemize}
      \item Nodes can store more than one partition, and \ul{each partition can be stored on multiple nodes}.
   \end{itemize}
   \item \textbf{Leader} and \textbf{Follower} assignment
   \begin{itemize}
      \item Each node stores a partition, and one of the nodes is designated as the leader. The leader is responsible for handling all write operations, while the followers, assigned to different nodes, replicate the data from the leader.
   \end{itemize}
   \item Replication and Partitioning
   \begin{itemize}
      \item The two techniques are enforced independently.
   \end{itemize}
\end{enumerate}

The Goal of partitioning is to spread data and query load \textbf{evenly} among the nodes.
Unfair partitioning can lead to \textbf{hot spots}, where some nodes are overloaded while others are underutilized.\\
Randomizing the partitioning function can help to avoid hot spots, but it can also make it difficult to locate data, requiring to query all nodes to get a value.

\subsection{Key-Range Partitioning}
A first improvement might be to provide a range-key assignment allowing to locate data, as it happens in libraries, where books are ordered by author or title.\\
In this way you know the boundaries of where to search. This is very easy to implement and to understand,
however, it is \textit{not} optimal:
\ul{data may \textbf{not} be evenly distributed among the possible keys}.

Consider partitioning a student database by student ID. If student IDs are assigned sequentially, then all students enrolled in the same year will have similar IDs. This can lead to uneven distribution of data across partitions, as students from the same year may be assigned to the same partition, while other partitions may be underutilized.

\section{Avoiding Hot spots}
So with key-range partitioning, the problem is that some keys may be more popular than others due to access patterns, leading to hot spots.\\
In, for instance, a sensor database, all today's writes would end up in the same ``today's partition'', while the rest of the partitions would be idle.
\nl

A solution might be to change-key structure, for instance by adding a prefix to the key, such as the sensor ID, to distribute the data more evenly.

\subsection{Hash partitioning}
A better solution is to use \textbf{hash partitioning}, where a hash function is used to map keys to partitions.\\
However we must be careful because \textbf{inconsistent} hashing can lead to hot spots, as the hash function may not distribute keys evenly.
In general, however, a good hash function will make skewed data uniformly distributed across partitions.

Note also that hash partitioning is very bad for range queries, as even similar data is spread randomly across the partitions.\\
In Cassandra, a \textbf{compound primary key} is used, where the first part is hashed to determine the partition, while the second part is an index used to order data within the partition, allowing for efficient range queries within a partition.\\
Range queries on the first column (the hashed one) are not possible, as data is spread randomly across partitions, so all partitions must be queried in such case.

\subsubsection{Secondary Indexes}
% Copilot generated
Secondary indexes are a way to avoid hot spots in hash partitioning.\\
The idea is to create a separate index that maps the secondary key to the primary key, and then use the primary key to locate the data.\\
This way, the secondary key is hashed and distributed evenly across the partitions, while the primary key is used to locate the data.


\begin{itemize}
   \item \textbf{Document}-based partitioning (local indexes)
   \begin{itemize}
      \item Each listing has a unique document ID
      \item Database is partitioned based on the document ID
      \item Secondary indexes are on fields like color and make
      \item Each partition maintains its own secondary indexes
      \item \ul{\textit{Reading} requires querying all partitions}
   \end{itemize}
   \item \textbf{Term}-based partitioning (global indexes)\\
   The ``term'' is essentially the value of the indexed field.
   \begin{itemize}
      \item A Global index covers data in all partitions.
      \item Partitioning is based on the term ID, which is not the primary key index.
      \item Example: Colors a-r in partition 1, s-z in partition 2.
      \item A single entry in secondary index may include records from all primary key partitions.
      \item Term determines the partition of the index
      \item The Term concept comes from full-text indexes
   \end{itemize}
\end{itemize}

Global index allows clients to request specific partitions, and avoids scatter/gather over all partitions, so read operations are more efficient.
However, write operations become more complex, as the write to a document may require updating multiple partitions, both the document partition and the index partition.
Besides, each term in a document could be on a different partition, leading to multiple writes for a single document update.

\section{Rebalancing}
Rebalancing is the process of moving data between partitions to ensure that the data is evenly distributed among the nodes. This is necessary when the data distribution changes, for example, when new nodes are added to the system or when the data distribution becomes uneven due to hot spots. Rebalancing can be done in various ways, such as automatic rebalancing, manual rebalancing, and dynamic rebalancing.

\framedt{Challenges in rebalancing}{
   \begin{itemize}
      \item Fair load distribution
      \item Continuous operation during rebalancing
      \item Minimizing data movement
   \end{itemize}
}

\begin{itemize}
   \item \textbf{Automatic} rebalancing - System automatically rebalances the data when necessary without any human intervention.
   May be unpredictable and expensive, but requires less maintanance.
   \item \textbf{Manual} rebalancing - Administrator manually rebalances the data when necessary. May be better since and admin may have a more \textit{comprehensive} view of the distributed system, while a machine may be limited by network partitioning, discovery, etc.
   \item \textbf{Hybrid} approach - Combines automatic and manual rebalancing. The system automatically rebalances the data when necessary, but the administrator can override the system's decisions. 
\end{itemize}


\subsection{Techniques}

A common approach to address rebalancing is the ``\textbf{Hash mod N}'' strategy, where each key is hashed and then the modulo operation with the number of nodes (N) determines the partition. However, this approach can lead to significant data movement when nodes are added or removed, as all keys need to be rehashed.

\framedt{Optimizing rebalancing - Kademlia and P2P recalls}{
Assigning keys to nodes computing $h(key) mod \#{nodes} \rightarrow node$ is intuitive but leads to a tremendous amount of data movement when nodes are added or removed, since all modulo values must be recomputed.\\
A better solution is to use a \textbf{distributed hash table} (DHT) such as \textbf{Kademlia} or \textbf{Chord}, which allows to find the node responsible for a key in $O(\log n)$ steps.

A simple intuition exploited by both Kademlia and Chord is to fix the granularity of the keyspace prior to partitioning.

In other words, having a fixed number of partitions may help to avoid data movement when nodes are added or removed.
}


Another idea is to have many \textbf{fixed partitions}, more than nodes, and assigning several (like 1000) partitions to each node. When a node is added it steals partitions from other nodes, and when a node is removed its partitions are reassigned to other nodes. This way, only a small portion of the data needs to be moved during rebalancing.
Partitions are never split or merged, only reassigned. Clearly, we can exploit more powerful nodes to hold more partitions.\\
The issue with this is that incorrect boundaries can lead to hot spots, so careful monitoring is required.

\textbf{Dynamic Partitioning} is another approach, where partitions can be split or merged based on the data distribution. This allows for more flexibility in rebalancing, as partitions can be adjusted to better fit the data distribution. However, this approach can be more complex to implement and manage.\\
Partitions are split when they exceed a configured size, and merged when some other gets deleted.\\
Each partition assigned to one node.

\textbf{Proportional Partitioning} may be enforced in different fashions:
\begin{itemize}
   \item Dynamic Partitioning - Number of partitions varies is proportional to dataset size.
   Splitting and merging processes maintain partition size.
   \item Fixed Number of Partitions - the number of partitions is fixed and proportional to dataset size, it is independent of the number of nodes.
   \item Proportional Assignment to Nodes - Number of partitions proportional to number of nodes
\end{itemize}
This allows for better utilization of resources, as more powerful nodes can handle more partitions. However, this approach requires careful monitoring and management to ensure that the data is evenly distributed among the nodes.

\framedt{Automatic vs Manual Rebalancing}{
   With automatic rebalancing the system automatically rebalances the data when necessary without any human intervention.
   May be unpredictable and expensive, but requires less maintanance.
   \nl

   With manual rebalancing the administrator manually rebalances the data when necessary. May be better since and admin may have a more \textit{comprehensive} view of the distributed system, while a machine may be limited by network partitioning, discovery, etc.
   This is considerably slower but may prevent operational surprises.
}

\subsection{Routing and Querying}
Routing is the process of determining which partition to send a query to. This is done by either forwarding requests to a routing tier, or by having partition-aware clients which know how the data is distributed. Alternatively, client and also query any node, if there is no routing tier nor they are partition-aware. Routing can be done in various ways, such as static routing, dynamic routing, and consistent hashing.\\
ZooKeeper is often used as a routing tier, as it provides a distributed configuration service that can be used to store the partitioning information.

Queries may be executed in parallel across multiple partitions, or they may be executed sequentially. Parallel execution is more efficient, but it requires more resources. Sequential execution is less efficient, but it is easier to implement.


\begin{figure}[htbp]
   \centering
   \includegraphics{images/11/routing.png}
   \caption{Routing}
   \label{fig:11/routing}
\end{figure}



\section{Takeaways}
Data partitioning enhances scalability by distributing data across multiple nodes, and when combined with replication, it can improve the performance and reliability of the system. However, partitioning can introduce challenges such as hot spots and rebalancing. To address these challenges, you can use techniques such as hash partitioning, secondary indexes, and dynamic partitioning. By carefully designing the partitioning strategy, you can achieve high availability, fault tolerance, and scalability in your system.