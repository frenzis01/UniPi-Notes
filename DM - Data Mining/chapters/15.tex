\chapter{Gradient-based optimization}

\section{Initial notions - Computational graphs}

\begin{definition}
   [Computational graph]
   A computational graph is a directed acyclic graph where:
   \begin{itemize}
      \item nodes represent operations or variables
      \item edges represent the flow of data between them
   \end{itemize}

   In a computational graph there are
   \begin{itemize}
      \item Parameters $\Theta$ given by the weights on edges
      \item Structure given by the graph's architecture and function.
   \end{itemize}
\end{definition}

\begin{paracol}{2}
   
   \colfill
   We can generalize several gradient-based optimization to computational graphs,
   made up of objects which compute a generic function $f(x)$. Functional form: $h(\omega^T x)$. Note: $\omega^T x$ is
   simply a linear combination of $x$.
   \colfill
   
   \switchcolumn
   \begin{figure}[htbp]
      \centering
      \includegraphics{images/15/computationalGraph.png}
      \caption{A computational graph: nodes indicate objects involved in the computation, and edges indicate their flow in the graph.
      \textbf{Structure} given b y the operations $(h, \cdot)$ while the parameters are the weights $\omega_{1\dots4}$.
      }
      \label{fig:15/computationalGraph}
   \end{figure}

\end{paracol}

\subsection{Forward and backward passes}

\textbf{Forward pass}: given input $x$, compute $f(x)$ by following the graph from input to output, thus \textbf{bottom-up}.
Recall that nodes are either functions or variables, so the forward pass consists in applying functions to variables as we traverse the graph, with the edges indicating the flow of data along with a \textbf{weight} (parameter) associated to each edge.

\textbf{Backward pass}: given a ---differentiable--- loss function $l(f(x), y)$, compute the gradient of the loss wrt the parameters of the graph.


\newpage
\section{Scaling up - Layering graphs}

\begin{paracol}{2}
   
   The backward pass is made possible by the
   differentiability of the loss... but this can be applied
   also to the functions within $g$: if a function $h_i$
   within $g$ is differentiable, then I can \textbf{recursively compute gradients} on it.
   
   
   \begin{definition}
      [Chain Rule of Calculus]
      If $f(x) = h(g(x)) = h \circ g $, then
      \[
         \frac{\vartheta f}{\vartheta x} = \frac{\vartheta h}{\vartheta g} \cdot \frac{\vartheta g}{\vartheta x}
         \]
      \end{definition}

      \switchcolumn

      \begin{figure}[htbp]
         \centering
         \includegraphics{images/15/backwardpass.png}
         \caption{A backward pass in $g$: change parameters indicated by the gradient indicates the direction towards which move parameters to minimize the loss $l$}
         \label{fig:12/backwardpass}
      \end{figure}
   \end{paracol}

Applying the chain rule recursively, and going down the computational graph, we can
compute optimization directions for all parameters! The algorithm chaining back the
loss gradient is called \textbf{backpropagation}.

\begin{figure}[htbp]
   \centering
   \includegraphics{images/15/chainrule.png}
   \caption{A multi-layered computational graph, and its forward (left) and backward (right) pass. The chain rule enables propagation of
updates back and over the entire network.}
   \label{fig:15/chainrule}
\end{figure}

\subsection{Neural Networks}
Neural networks implement computational graphs.
\begin{itemize}
	\item \textbf{Layer}: set of all adjacent nodes
	\item \textbf{Block}: collection of consecutive layers
	\item \textbf{Activation function}: functions found in nodes
	\item \textbf{Hidden layer}: a layer, except the first or last one
	\item \textbf{Output layer/nodes}: layer/nodes yielding the computed output $f_\theta(x)$
\end{itemize}

Fitting a neural network consists in, at a very high level, repeatedly computing forward and backward passes, each pair improving on the current loss.

\begin{figure}[htbp]
   \centering
   \includegraphics{images/15/NN.png}
   \caption{Consecutive foreward and backward passes on a network, each backward pass leverages gradients of the loss to find local
directions of loss minimization, which are then used to fit the model.}
   \label{fig:15/NN}
\end{figure}

\subsection{Learning - Stochastic learning}

Backpropagation takes care of defining optimal directions for the parameters, but how
do we estimate and leverage these directions?

\begin{paracol}{2}
   
   Estimating directions: Through stochastic gradient, by computing gradient on a batch of data, rather than the whole dataset.
   
   Provides a good approximation and a much faster computation.

   Batches can be aggregated, the directions they yield combined: a training-specific approach to combat overfit.
   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/15/gradientDirections.png}
      \caption{The gradient $\nabla L^X$ estimated on some data $X$, and gradients $\nabla L^{X_1}$ and $\nabla L^{X_2}$ estimated on two batches  $X_1$ and $X_2$ (subsets of X). The batch gradients can be combined to approximate the full gradient.}
      \label{fig:15/gradientDirections}
   \end{figure}

\end{paracol}


\subsubsection{Momentum}

Backpropagation takes care of defining optimal directions for the parameters, but how
do we estimate and leverage these directions?

Estimating weight update: Update directions are weighted by a possibly decaying learning rate $\eta$, and possibly weighted by past updates (Adam, RMSProp, Nesterov momentum).

\begin{figure}[htbp]
   \centering
   \includegraphics{images/15/momentum.png}
   \caption{Momentum}
   \label{fig:15/momentum}
\end{figure}


\subsubsection{Regularization}
Regularization refers to techniques aimed at reducing overfitting during training.
Regularization of neural networks can take many forms.
\begin{itemize}
   \item \textbf{Weight Regularization} - The loss includes a term $|| \Omega ||_2$ to penalize large weights in the netork, as higher weights tend to increase the network capacity.\\
   Weight regularization constrains the capacity of the network by discouraging large weights.
   \item \textbf{Dropout} - Random deletion of network connections, aims to create networks less reliant on a small number of neurons.\\
   In other terms, dropout forces the network to be more robust, as it cannot rely on specific paths in the network.
   \item \textbf{Early Stopping} - Stop training when the validation loss starts increasing, even if the training loss is still decreasing.
   Early stopping prevents overfitting by halting training before the model starts to memorize the training data.\\
   More specifically, there are two rolling metrics:
   \begin{enumerate}
      \item The gap between train and validation error
      \item Train error itself
   \end{enumerate}
   Early stopping halts training when the gap starts increasing (indicating overfitting) or when the train error does not decrease significantly anymore (indicating convergence, i.e., the model has learned as much as it can from the training data, so no further training is beneficial).
\end{itemize}

\section{Structure}

Activation functions impact the flow of data throughout the network, and their outputs
are called \textbf{activations}. Different activations define different \textbf{representations} of the data,
which, unlike in PCA, are dependent on learned parameters.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Formulation} & \textbf{Heads} & \textbf{Task} \\
\hline
Identity & $\omega^T x$ & 1 & Regression\\
\hline
Logistic & $\displaystyle\frac{1}{1 + e^{-\omega^T x}}$ & 1 & Classification*\\
\hline
ReLU & $\max\{0, \omega^T x\}$ & 1 & Regression*\\
\hline
Softmax & $\displaystyle\frac{exp(\omega_i x_i)}{exp(\omega^T x)}$ & $\geq 1$ & Classification*\\
\hline
$\ldots$ & $\ldots$ & $\ldots$ \\
\hline
\end{tabular}
\caption{Common activation functions in neural networks}
\note{*\textit{Indirectly}: classification is often rendered as a continuous value (to turn into discrete), while regression may be bounded, e.g., to be positive by ReLU.}
Remember, $\omega^T x$ is the sum of the incoming edges in the node.
\label{tab:activation-functions}
\end{table}

\section{Architectures}
Given their high capacity, and innate ability to encode data, networks are often not fit
from scratch. Rather, a network is fit on a task, then adapted to other tasks.
\begin{itemize}
   \item \textbf{Fine tuning}: use a pre-trained network as initialization for a new specialized task, then continue training on the new task.
   \item \textbf{Adapters}: insert small layers (adapters) between pre-trained layers, freeze the pre-trained layers, and only train the adapters on the new task.
   Adapters are fit to ``steer'' the parameters of the larger pre-trained network towards the new task, while keeping the original knowledge intact.
\end{itemize}


\newpage
\subsection{ResNet}
Architectures have proven to be extremely important, and in several cases, the
application dictates the network architecture. On tabular dataset, \textbf{residual networks}
(ResNets) are a particularly strong baseline. They have a functional form
\[f(x) = x + h(x)\]

\begin{figure}[htbp]
   \centering
   \includegraphics{images/15/resnet.png}
   \caption{A residual block: a node is forwarded to the next layer, and to the one after it as well. Residual (also called skip) connections allow a more effective backpropagation. Weights on the skip connection are set to to preserve data.}
   \label{fig:15/resnet}
\end{figure}

\subsection{Feature Transformer}
\begin{paracol}{2}

   \colfill
   
   Blocks constructs circuits of similarity, computing degrees of ``attention'' between features, and representations. Similarity then weighs on skip connections.
   Functional form:
   \[f(x^i) = f(x^i) + \sum_{i\neq j}\alpha_{i,j} x^j\]
   where $\alpha_{i,j}$ is the attention weight between features $i$ and $j$.
   \colfill
   
   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/15/transformer.png}
      \caption{An attention block: similarities between representations are computed through a (scaled) multiplication. Addition through a residual connection allows representations to explicitly influence each other}
      \label{fig:15/transformer}
   \end{figure}

\end{paracol}