\chapter{Anomaly Detection}

We may refer to anomalies in data also as \textit{outliers.}

\section{Outliers}
\begin{itemize}
	\item Inherently \textbf{fuzzy} - An instance has a degree of outlierness, which we can threshold to decide whether an instance is an outlier or not.
	\item \textbf{Data-dependent} - Outlier are exceptions to the data. But outliers themselves define the data...?
	\item \textbf{Not noise} - Noise is random, outliers are exceptional.
	\item \textbf{Mono/multi-dimensional} - An outlier can be so on just one dimension, or on multiple.
\end{itemize}

Outliers are something that is either \textbf{unusual} or \textbf{extreme}, or both.\\
Outliers are, by nature, defined in terms of other instances. Whatever approach we use to detect them, we should take into account that they influence it as well.

\section{Outlier Detection Algorithms}
Algorithms used to detect outliers usually involve two key steps:
\begin{enumerate}
   \item \textbf{Grading} - Define a grading function $\tilde{o}$ that assigns to each instance $x$ a degree of outlierness/anomaly $\tilde{o}(x)$.
   \item \textbf{Thresholding} - Decide on a threshold $\hat{o}$ such that instances with $\tilde{o}(x) > \hat{o}$ are considered outliers.
\end{enumerate}

We can categorize outlier detection algorithms depending on the \textit{axis} on which they operate:
\begin{itemize}
   \item \textbf{Locality} - is the outlier detection performed in a local neighborhood (local) or considering the entire dataset (global)?
   \item \textbf{Sensitivity} - is the outlier detection sensitive to the presence of other outliers (sensitive) or not (robust)?
   \item \textbf{Interpretability} - can we interpret/explain why an instance is considered an outlier (interpretable) or not (black-box)?
\end{itemize}

\begin{table}[htbp]
   \centering
   \begin{tabular}{|c||c|}
      \hline
      \textbf{Locality} & \textit{Global}/\textit{Local} \\
      \hline
      \textbf{Sensitive} & \textit{Robust}/\textit{Sensitive} \\
      \hline
      \textbf{Interpretable} & \textit{Black-box}/\textit{Interpretable} \\
      \hline
   \end{tabular}
\end{table}

\begin{figure}[htbp]
   \centering
   \includegraphics{images/06/outlierfunction}
   \caption{Distribution, manifold, neighborhood}
   \label{fig:}
\end{figure}

\newpage
\subsection{Distributions}
\begin{paracol}{2}
   In case of a normal distribution $\mathcal{N}(\mu, \sigma)$, we can define outliers as instances that are more than $k$ standard deviations away from the mean $\mu$ (see Figure~\ref{fig:06/normaldistribution}).\\
   This approach is \textbf{global}, \textbf{robust} and \textbf{interpretable}.

   The degree of anomaly $\tilde{o}(x)$ can be defined as:
   \[
      \text{z-score} = \tilde{o}(x) = \frac{|x - \mu|}{\sigma}
   \]

   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/06/normaldistribution.png}
      \caption{For a normal distribution $\mathcal{N}(\mu, \sigma)$, we can define outliers as instances that are more than $k$ standard deviations away from the mean $\mu$.}
      \label{fig:06/normaldistribution}
   \end{figure}
\end{paracol}

\subsubsection{Grubbs test}
$z-scores$ generate sample-dependent outlier degrees $\tilde{o}(x_1), \tilde{o}(x_2), \ldots, \tilde{o}(x_n)$, but does not tackle the \textbf{+1 problem}.

The +1 problem is the problem of deciding whether the most extreme instance in a dataset is an outlier or not.\\
Grubb's test iterates over detected outliers, \ul{removing \textit{one layer} of outliers at a time, until no more outliers are found}.
The key point is that after removing outliers, the distribution parameters $\mu, \sigma$ are recomputed, thus changing the z-scores of the remaining instances.
This is key to detect multiple outliers in a dataset.
\begin{algorithm}
   \caption{Grubb's test for outliers}
   \begin{algorithmic}[1]
      \State Find current outlier set $\hat{X}$
      \State If $\hat{X} = \emptyset$, stop
      \State $X = X \setminus \hat{X}$ - remove outliers from dataset
      \State Go to step 1
   \end{algorithmic}
\end{algorithm}

\begin{table}[htbp]
   \begin{tabular}{|c||c|}
      \hline
      \textbf{Locality} & \textbf{Global} \\
      \hline
      \textbf{Sensitive} & Outliers influence the distribution but may be removed by Grubb's test \\
      \hline
      \textbf{Interpretable} & \textbf{Black-box}, no clear explanation for outlierness, simply, there are not many similar instances.\\
      \hline
   \end{tabular}
\end{table}


Data may vary \textit{locally}: subsets of the data each follow a different distribution.\\
Assumption: there exists a partition of the data, each block distributed according to a Normal distribution.\\
Thus we could use multiple models to model the data, and define outliers as instances that are outliers with respect to their local distribution.\\

One of $k$ models $M_{\theta_{0}}, M_{\theta_{1}}, \ldots, M_{\theta_{k-1}}$ is sampled, each with a sampling probability $m_i$. Different distributions sample different regions of the density.

\begin{figure}[htbp]
   \centering
   \includegraphics{images/06/mixture.png}
   \caption{A mixture of Normals $M_{\theta_0}, M_{\theta_1}, M_{\theta_2}$; each sampled with probability $m_0, m_1, m_2$, respectively.}
   \label{fig:06/mixture}
\end{figure}

\begin{table}
   \begin{tabular}{|c||c|}
      \hline
      \textbf{Locality} & \textbf{Local} \\
      \hline
      \textbf{Sensitive} & Outliers influence the distribution, may be unstable, but may be removed by Grubb's test (?) \\
      \hline
      \textbf{Interpretable} & \textbf{Black-box}, no clear explanation for outlierness, simply, there are not many similar instances.\\
      \hline
   \end{tabular}
\end{table}


\subsection{Thresholding}
\framedt{Grubb's test}{
   Choosing a threshold $\hat{o}$ is arbitrary, but there are algorithms such as Grubb's test which define their own thresholding mechanism.
   In Grubb's test, the threshold is defined as:
   \[
      \hat{o} = n \frac{\sum(x_i - \bar{X})^4}{(\sum(x_i - \bar{X})^2)^2}
   \]
}

\textit{z-scores} assume a Normal distribution, but often this is not the case. Yet, we can still
identify tails of a distribution, and in turn, anomalies, by exploiting \textit{Markov} and \textit{Chebyshev} \textbf{inequalities}, that hold for any distribution.

\begin{itemize}
   \item \textit{Markov} inequality - for a variable (distribution )$X$ with positive values and threshold $\beta$, it holds
   \[
      P(X \geq \beta) \leq \frac{\mathbb{E}(X)}{\beta}
   \]
   Thus, given an estimate of the variable's expected value, we can retrieve the inverse of an image of its cumulative distribution
   \item \textit{Chebyshev} inequality - for a variable (distribution) $X$ with mean $\mu$ ($=\mathbb{E}(X)$) and standard deviation $\sigma$, and threshold $\beta$, it holds
   \[
   P(|X - \mathbb{E}(X)| > \beta) \leq \frac{\sigma^2}{\beta^2}
   \]
   That is, the probability of deviation from the mean is inversely proportional to the deviation
\end{itemize}

\subsection{Manifold}

Distributional approaches define the density, but do not describe the data itself. $\tilde{o}$ is defined in terms of the manifold: does the given instance lie in the manifold? Just like
the distributional approach, we must assume the manifold family.
To preserve the interpretability of our results, we initially stick to linear manifolds.

We can define the anomaly degree $\tilde{o}(x)$ as the distance of $x$ from the manifold.\\
We can use PCA to find the linear manifold that best fits the data.\\
PCA finds the directions of maximum variance in the data, and uses them as a new basis for the data.

A matrix $A$ spans a linear space, thus every vector $b$ in its spanned space is defined as a
linear combination of $A: b = Ax$. For non fullrank matrices $A$, such a solution $x$ may not exist. Thus, we need to project on the data manifold, to look for the closest point $\hat{b}$ to $b$ that lies in the space spanned by $A$.

\framedt{Least Squares}{
   Least Squares is a method to find the best fitting solution to an overdetermined system of equations $Ax = b$ (more equations than unknowns).\\
   The best fitting solution is the one that minimizes the residual sum of squares (RSS):
   \[
      RSS = ||Ax - b||^2_2
   \]

   Least squares assumes a linear manifold, and squared norm as distance metric.


   The instability of least squares is due to the data collinearity. A possible solution: de-correlate the data! PCA does exactly that.
}

\note{Some mathy examples are displayed in the lecture slides\dots}

\begin{table}[htbp]
   \centering
   \begin{tabular}{|c||c|}
      \hline
      \textbf{Locality} & \textbf{Global} \\
      \hline
      \textbf{Sensitive} & Strongly influenced by outliers \\
      \hline
      \textbf{Interpretable} & Partial: which instances have lower degrees? What even is a ``low'' degree?\\
      \hline
   \end{tabular}
   \caption{Least squares}
   \label{tab:least_squares}
\end{table}

Manifold-based algorithms are as flexible as the defined manifold. Like with mixture models, neighbor-based approaches reintroduce locality: outliers are defined in function of their neighbors:
\begin{itemize}
	\item \textbf{Connectivity} - An outlier is defined in terms of the connectivity to its neighbors
	\item \textbf{Concentration} - An outlier is defined in terms of its neighbor concentration
\end{itemize}

\[
\begin{bmatrix}
5   & 11  & \cdots & 0 \\
\cdots & \cdots & \cdots & \cdots \\
54  & 27  & \cdots & 3 \\
\cdots & \cdots & \cdots & \cdots
\end{bmatrix}
\]

In the above matrix we display connectivity as \textbf{postings}, rather than distances.\\
$A_{i,j}$ is the ---row ID?--- $j^{th}$ neighbor of instance $i$, making each row ordered by increasing distance from $i$.


The underlying assumption is that an in instance is an inlier if it is well connected to its neighbors, and lies in a dense area of the space.\\
Each instance has a posting list of neighbors, from
the closest to the farthest: the lower the aggregated
position in other lists, the higher the connectivity
degree.
\begin{itemize}
	\item Posting position defines connectivity: it is not density
	\item Connectivity is asymmetric: I may be your closest instance, you may not be mine
\end{itemize}

\subsubsection{Grading neighbors connectivity}
Posting matrices are often used as a base on which
to measure different indices of connectivity, e.g.,
\begin{itemize}
   \item \textbf{hub} - instance $x_i$ is at least the $t^{th}$ neighbor of at least $k$ instances.
   
   Definition used by ODIN: given a posting matrix $A$,$x_i$ is a \textit{hub} if it appears at least $k$ times in the first $t$ columns of $A$. Hence,$x_i$ is an outlier if the opposite is true:
   \[
      \tilde{o}(x_i) = \begin{cases}
         1 \quad\textit{if }\quad |{i|i \in A_{\neq i, \leq t}} | < k\\
         0 \quad\textit{otherwise}
      \end{cases}
   \]
   That is, if $x_i$ appears less than $k$ times in the first $t$ columns of $A$, it is an outlier.
   \note{You must be at most the $t^{th}$ neighbor of at least $k$ instances to be considered a hub, hence, not an outlier.}
   \item \textbf{popularity} - instance $x_i$ is on average the $t^{th}$ neighbor of at least $k$ instances
   
   Given a posting matrix $A$, $x_i$ is an outlier if, on average, is not less than the $t-th$ neighbor of other instances:
   \[
      \hat{o}(x_i) = \frac{\sum_{l=0, l \neq i}^{n-1} \sum_{j=0}^{n-1} \mathbb{1}\{a_{l,j} = x_i\}l}{n-1} > t.
   \]
   where $\mathbb{1}\{a_{l,j} = x_i\}$ is an indicator function that equals 1 if $a_{l,j} = x_i$ and 0 otherwise, representing the position in the posting list.
   
   \item \textbf{ostracism} - instance $x_i$ is at worst $t^{th}$ neighbor of other $k$ instances
\end{itemize}

\newpage
\begin{paracol}{2}
   
   \colfill
   Connectivity and concentration can be
   approximated through similar
   structures: we go from \textit{postings} matrix to
   distance matrix!
   To ease notation, we use a row-sorted
   distance matrix $A_{\gamma}$, so that row $i$ holds
   increasing distances from instance $x_i$.
   \colfill

   \switchcolumn
   \colfill

   \[
   A = \begin{bmatrix}
   0 & 2.28 & 0.16 & 0.21 \\
   2.21 & 0 & 1.21 & 3.91 \\
   0.16 & 1.21 & 0 & 0.76 \\
   0.21 & 3.91 & 0.76 & 0
   \end{bmatrix}
   \]
   
   \[
   A_{\gamma} = \begin{bmatrix}
   0.16 & 0.21 & 2.28 \\
   1.21 & 2.28 & 3.91 \\
   0.16 & 0.76 & 1.21 \\
   0.21 & 0.76 & 3.91
   \end{bmatrix}
   \]
   
   A distance matrix $A$ (top), and its row-sorted version $A_{\gamma}$ (bottom). First column of 0s trimmed from $A_{\gamma}$.\\
   $A_{\gamma}$ is also called the \textbf{reach matrix}, which will be discussed below.
   \colfill
\end{paracol}


\subsection{Reach}
An instance $x$ has reach $\gamma^k(x)$ if the $k-th$ nearest neighbor is at distance $\gamma^k$, and average reach $\bar{\gamma}^k(x)$ if the average of $\{\gamma^1, \ldots, \gamma^k\}$ is $\bar{\gamma}^k(x)$.

Our row-sorted distance matrix $A_{\gamma}$ is the \textit{reach} matrix of the data! Indeed, $A_{\gamma}$ defines both reach and average reach.
\nl

The reach of instance $x_i$ is encoded in the row-sorted distance matrix $A_{\gamma}$ as follows:

\[
A_{\gamma} = \begin{bmatrix}
\gamma^1(x_1) & \gamma^2(x_1) & \gamma^3(x_1) \\
\gamma^1(x_2) & \gamma^2(x_2) & \gamma^3(x_2) \\
\gamma^1(x_3) & \gamma^2(x_3) & \gamma^3(x_3)
\end{bmatrix}
\]

where $\gamma^j(x_i)$ denotes the $j$-th nearest neighbor of instance $x_i$.

For example, consider the following transformation:

\[
A_{\gamma} \begin{bmatrix}
1 & \frac{1}{2} & \frac{1}{3} \\
0 & \frac{1}{2} & \frac{1}{3} \\
0 & 0 & \frac{1}{3}
\end{bmatrix} = \begin{bmatrix}
\bar{\gamma}^1(x_1) & \bar{\gamma}^2(x_1) & \bar{\gamma}^3(x_1) \\
\bar{\gamma}^1(x_2) & \bar{\gamma}^2(x_2) & \bar{\gamma}^3(x_2) \\
\bar{\gamma}^1(x_3) & \bar{\gamma}^2(x_3) & \bar{\gamma}^3(x_3)
\end{bmatrix}
\]

$A_{\gamma}$ explicitly encodes reach ($A_{\gamma}$ itself) and average reach.
\ul{Note that the left part of the equation above is a matrix multiplication!} 
The matrix with the fractions is a \textbf{transformation matrix} that transforms reach into average reach, by assigning weights to each neighbor distance.

\framedt{Average Reach as concentration}{
   \ul{The key point is that the average reach defines an empirical approximate \textbf{concentration}}, since it measures what's the average distance of the $k$ nearest neighbors.
}

\subsubsection{Reach ratio factor}
Assumption: Inliers have lower reach than their neighbors. We formalize this in a reach
(ratio factor:
\[
   \tilde{o}^k_{i,j} = \frac{\bar{\gamma}^k(x_i)}{\bar{\gamma}^k(x_j)}
\]
which 1 is for pairs $x_i,x_j$ with equal k-neighbors concentration, and $>1$  for instances
with different concentrations, $x_i$ laying in a sparser area of the space.
It can be also $<1$ if $x_i$ is in a denser area than $x_j$.
\newpage

\begin{paracol}{2}

   \colfill
	\textbf{Local outlier factor} generalizes outlier (\textit{reach ratio}) factor by averaging the outlier factor over the
	neighbors of an instance:
	\[
		\tilde{o}(x_i) = \sum_{x_j \in neigh(x_i)} \tilde{o}^k_{i,j}
   \]
   \coolquote{
      But\dots how many neighbors should we consider?
   }{}
   It matches the number of columns in the reach matrix $A_{\gamma}$, which is a parameter to be set by the user, which should be $k$ (if I'm not mistaken).

   \colfill

	\switchcolumn

	\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.9\columnwidth]{images/06/reachneighbors.png}
		\caption{Neighbors at different $k$ values.}
		Local outlier factor respects the postings matrix, as it creates \textit{clusters} of neighbors.
		\label{fig:06/reachneighbors}
	\end{figure}
\end{paracol}


\begin{paracol}{2}
   \colfill
   \textbf{Connectivity outlier factor} (COF) is based on the idea that outliers are poorly connected to their neighbors. It is defined as:
   \[
		\tilde{o}(x_i) = \sum_{x_j \in \textit{connect\_neigh}(x_i)} \tilde{o}^k_{i,j}
      \]
      The connected neighbors of an instance $x_i$ is recursively defined as the 1-nearest neighbor to the last element in the chain.
      \colfill
   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.95\columnwidth]{images/06/cof.png}
      \caption{Neighbors with different $k$ values.}
      Connectivity Outlier Factor does not respects the posting matrix. Rather, it creates \textit{chains} of neighbors.
      \label{fig:06/cof}
   \end{figure}
\end{paracol}

\begin{paracol}{2}
   \colfill
   \textbf{k-NN} outlier factor (\textsc{kOF}) replaces the average reach at $k$ (denoted with $\bar{\gamma}^k)$ with the maximum reach at $k$ (denoted with $\hat{\gamma}^k$):
   \[
   \tilde{o}(x_i) = \gamma^k(x_i)
   \]
   \colfill
   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.95\columnwidth]{images/06/kof.png}
      \caption{Neighbors at different $k$ values}
      \label{fig:06/kof}
   \end{figure}
\end{paracol}

\subsection{Concentration}
 Reach degrees approximate space concentration with (inverse) reach. Rather than picking a $k$, we can swap in a more natural definition of concentration: instances found per unit of space.
 Even better, instances found within an hypersphere $B(\cdot,\varepsilon)$ of radius $\varepsilon$ centered in the instance.

 Clearly, the assumption is, again, that outliers are found in low-concentration areas of the space.

\begin{paracol}{2}
   \colfill
   We compute concentration on a two-radii approach:
   \begin{itemize}
      \item \textbf{concentration radius} $\varepsilon$: determines the hyperspheres $B(x_i, \varepsilon)$ estimating concentration $c^{\varepsilon}(x_i)$ of $x_i$ within a radius $\varepsilon$
      \item \textbf{neighborhood radius} $\delta$: proportional to $\varepsilon$, determines the neighborhood $B_i$ of $x_i$ as the instances laying within $B(x_i, \delta)$
   \end{itemize}
   \colfill

   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/06/tworadii.png}
      \caption{The two radii $\varepsilon, \delta$: the former is used to estimate \textit{concentration}, the latter to choose which neighbors to compare concentration against. \textbf{Note:} $\delta$ may also be larger than $\varepsilon$!}
      \label{fig:06/tworadii}
   \end{figure}
\end{paracol}

Like reach-based concentration, degrees are defined on a basis of comparisons between
some degree of an instance, and its neighbors:
\[
\tilde{o}(x_i) = \bar{c}^\varepsilon(B_i) - c^\varepsilon(x_i),
\quad \text{with} \quad
\bar{c}^\varepsilon(B_i) = \frac{\sum_{x_j \in B_i} c^\varepsilon(x_j)}{|B_i|}
\]
{that is, two-radii concentration compares the concentration of an instance, with the
concentration of its neighbors. For\ns
\begin{itemize}
   \item $\tilde{o}(x_i) \gg 1$ neighbors have a much higher concentration 
   \item $\tilde{o}(x_i) \rightarrow 0^+$ neighbors have a much higher concentration 
\end{itemize}
}

\begin{table}[htbp]
   \centering
   \begin{tabular}{|c|c|}
      \hline
      \textbf{Locality} & \textbf{Local} \\
      \hline
      \textbf{Sensitive} & Choice of neighborhood, connectivity parameter \\
      \hline
      \textbf{Interpretable} & Partial: can inspect what instances lead to different reaches\\
      \hline
   \end{tabular}
   \caption{Grading connectivity factors}
   \label{tab:06/connectivity}
\end{table}

\subsection{Neighborhoods}

\begin{figure}[htbp]
   \centering
   \includegraphics{images/06/hyperplanes.png}
   \caption{Faster and easier to compute hyperplane-based neighborhoods.}
   \label{fig:06/hyperplanes}
\end{figure}

\begin{paracol}{2}
   
   \colfill
   An \textbf{isolation tree} $t$ is a random tree which randomly
   partitions the space into a set of blocks (attributes, typically two at a time to get a binary tree).
   \begin{itemize}
      \item Splits are sampled randomly
      \item Tree grows up to a predefined height, or until all leaves contain one instance
   \end{itemize}
   Outlier degree 
   \[\tilde{o}^t(x_i) = \frac{path(x_i,t)}{c} \]
   where $c$ defines the average path length in the tree, which in case of a binary tree, for instance, is $\log_2(n)$, with $n$ number of instances.
   $path(x_i,t)$ is the length of the path from the root to the leaf containing $x_i$.\\
   That is, the instance is an outlier it the distance from the root is higher than average.
   \colfill
   
   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/06/isoltree.png}
      \caption{Isolation tree}
      \label{fig:06/isoltree}
   \end{figure}
\end{paracol}

Consider \autoref{fig:06/isoltree}: \ul{every line (split) is a node in the isolation tree}, which in fact splits the previous node in two.
That is each split partitions the space in two half-spaces, and the tree grows downwards until each leaf contains exactly one instance ---probably but not necessarily an outlier, depends on the length of the path---, or until a predefined height is reached.
The path of the colored instance in \autoref{fig:06/isoltree} is 5, as it is isolated by 5 splits (actually 2 well placed splits would be enough, but they are chosen randomly, so\dots); this means that in the tree, the path from the root to the leaf containing the instance is 5 edges long.

Suppose there are 1000 instances in a very small portion of the plane, representing the main distribution, and one instance very far away from them. The far away instance will likely be isolated in just a few splits, as the random splits will likely isolate it quickly.\\
Isolating one of the 1000 instances will likely take \textit{a lot} of splits, as the splits are random, and the instance is surrounded by many other instances, making it hard to isolate it quickly.
That's the reason why we set a maximum height for the tree. \ul{Dense data won't end up in a super deep leaf, avoiding making normal instances look like outliers}.

// TODO l'outlier degree è inverso!!! Più è basso, più si è outlier!!!
Aggiungere fix explanation di chat

% \note{Probably, given the span of the instances, we could also estimate the average depth of the tree as some whatever-random-distribution}



An isolation forest $T$ is comprised of several isolation trees, further sampling the hyperplane space.\\
The key point is that we can combine multiple trees to get a more comprehensive outlier degree:

Outlier degree 
$$\tilde{o}(x_i) = 2 - \frac{\sum_{t \in T} path(x_i,t)}{|T|c}$$
where $|T|$ is the number of trees in the forest.

\begin{table}[htbp]
   \centering
   \begin{tabular}{|c||c|}
      \hline
      \textbf{Locality} & \textbf{Global} and \textbf{local} \\
      \hline
      \textbf{Sensitive} & Dataset noise can be interpreted as outlier \\
      \hline
      \textbf{Interpretable} & One of the most interpretable. Splits are induced by the tree, if the tree is univariate. \\
      \hline
      
   \end{tabular}
   \caption{Grading Isolation Forests}
   \label{tab:06/isolation_forests}
\end{table}