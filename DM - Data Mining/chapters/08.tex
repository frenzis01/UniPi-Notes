\chapter{Association Analysis}

\textbf{Association Rule Mining} refers to, given a set of transactions, finding rules that will predict the occurrence of an item based on the occurrences of other items in the transaction

\section{Basic Concepts}

\subsection{Frequent Itemset}
An \textbf{itemset} is a collection of one or more items. An itemset with $k$ items is called a $k$-itemset.

\note{TODO does this apply only to transactional data?}

\begin{definition}
   [Frequent Itemset]
   An itemset is \textbf{frequent} if its \textbf{support} (the fraction of transactions that contain the itemset) is greater than or equal to a user-specified minimum support threshold \textit{minsup}.
\end{definition}
   
\begin{table}[htbp]
   \centering
   \begin{tabular}{|c|c|}
      \hline
      Transaction ID & Items Purchased \\
      \hline
      1              & Bread, Milk         \\
      2              & Bread, Diaper, Beer, Eggs            \\
      3              & Milk, Coke, Beer, Diaper            \\
      4              & Bread, Milk, Beer, Diaper         \\
      5              & Bread, Milk, Diaper, Coke            \\
      \hline
   \end{tabular}
   \caption{Example of transactions}
   \label{tab:itemset}
\end{table}

\begin{itemize}
   \item Itemset - ${Milk,Bread,Diaper}$
   \item Suppport count ($\sigma$) - Frequency of occurrence of an itemset - $\sigma({Milk,Bread,Diaper}) = 2$
   \item Support $\in [0,1]$ - Fraction of transactions that contain an itemset - $s({Milk,Bread,Diaper})=\frac{2}{5}$
   \item An itemset whose support is greater than or equal to a \textit{minsup} threshold is called a frequent itemset.
   \item Association Rule - An implication expression of the form $X \Rightarrow Y$, where $X$ and $Y$ are itemsets - ${Milk,Diaper} \Rightarrow {Beer}$
   \item Rule Evaluation Metrics - 
   \begin{itemize}
      \item Support ($s$ $\in [0,1]$)- The proportion of transactions that contain the itemset .
      \[
      s = \frac{\sigma(X \cup Y)}{|T|}
      \]
      \item Confidence ($c$ $\in [0,1]$)- The proportion of the transactions that contain $X$ which also contain $Y$. 
      \[c = \frac{\sigma(X \cup Y)}{\sigma(X)}\]
   \end{itemize}
\end{itemize}

Given a set of transactions T, the goal of association rule mining is to find all rules having
\begin{itemize}
	\item support $\geq$ \textit{minsup} threshold
	\item confidence $\geq$ \textit{minconf} threshold
\end{itemize}
\note{
Brute-force approach:
\begin{itemize}
	\item List all possible association rules
	\item Compute the support and confidence for each rule
	\item Prune rules that fail the minsup and minconf
thresholds
\end{itemize}

\ul{Computationally prohibitive!}}

Two-step approach:
\begin{itemize}
	\item Frequent Itemset Generation - Generate all itemsets whose support $\geq$ \textit{minsup}
   \note{This is still computationally expensive}
	\item Rule Generation - Generate high confidence rules from each frequent itemset, where each rule is a binary partitioning of a frequent itemset
\end{itemize}


The problem is finding the frequent itemsets efficiently.
\begin{definition}
   [Apriori principle]
   A subset of a frequent itemset is also a frequent itemset.
   Conversely, \ul{if an itemset is not frequent, none of its supersets can be frequent}.
   We can exploit this property to prune the search space.

   The support of an itemset never exceeds the support of its subsets. This is also known as the anti-monotone property of support.
\end{definition}

\section{Apriori Algorithm}
The purpose of the Apriori algorithm is to find all frequent itemsets in a transaction database, which means \ul{identifying itemsets that satisfy the support constraint} (threshold), i.e. finding all itemsets whose support is greater than or equal to a user-specified minimum support threshold.

The Apriori principle holds due to the following property of support of the support measure:
\begin{definition}
   [Anti-monotone Property]
   Anti-monotone property of support is formulated as:
   \[
      \forall X,Y : (X \subseteq Y) \Rightarrow s(X) \geq s(Y)
   \]
   This states that the \ul{support of an itemset never exceeds the support of its
   subsets}.
\end{definition}

\begin{algorithm}[H]
\caption{Apriori Algorithm}
\begin{algorithmic}[1]
\State $k \gets 1$
\State $F_1 \gets \{\text{frequent 1-itemsets}\}$ \Comment{Scan database and count support of each item}
\Repeat
\State $L_{k+1} \gets$ \textbf{Candidate Generation} from $F_k$ \Comment{Generate candidate $(k+1)$-itemsets}
\State $L_{k+1} \gets$ \textbf{Candidate Pruning} of $L_{k+1}$ \Comment{Prune candidates with infrequent $k$-subsets}
\State Scan transaction database to count support of each candidate in $L_{k+1}$
\State $F_{k+1} \gets$ candidates in $L_{k+1}$ with support $\geq$ minsup
\State $k \gets k + 1$
\Until{$F_k = \emptyset$}
\State \Return $\bigcup_k F_k$ \Comment{Return all frequent itemsets}
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}
%    [Apriori Algorithm]
%    \begin{enumerate}
%       \item Initialize $k=1$. 
%       \item Find all $F_1 = \{\textit{frequent 1-itemsets}\}$ by scanning the transaction database and counting the support of each item.
%       \item Repeat until no new frequent itemsets are found (i.e. $F_k = \emptyset$):
%       \begin{enumerate}
%          \item \textbf{Candidate Generation} - Generate candidate $L_{k+1} = (k+1)$-itemsets from the set $F_k$ of frequent $k$-itemsets found in the previous iteration.
%          \item \textbf{Candidate Pruning} - Prune candidate itemsets in $L_{k+1}$ that have any subset (of size $k$) that are infrequent (not in $F_k$).
%          \item \textbf{Support Counting} - Scan the transaction database to count the support of each candidate in $L_{k+1}$.
%          \item \textbf{Candidate Elimination} - Eliminate candidates in $L_{k+1}$ that do not satisfy the minimum support threshold to form the set of frequent $(k+1)$-itemsets $F_{k+1}$, leaving only those candidates that are frequent.
%          \item  - Increment $k$ by 1.
%       \end{enumerate}
%    \end{enumerate}
% \end{algorithm}

\subsection{Candidate Generation}

We can generate candidates using the $F_{k-1} \times F_{k-1}$ prefix method:
\ul{merge two frequent $(k-1)$-itemsets} if their first $k-2$ items are the same. For example, $\{A, B, C\}$ and $\{A, B, D\}$ can be merged to form the candidate $\{A, B, C, D\}$.
Instead, $\{A, B, C\}$ and $\{A, C, D\}$ cannot be merged because their first $k-2$ items are not the same.

There is also an alternative $F_{k-1} \times F_1$ method: \ul{
   Merge two frequent $(k-1)$-itemsets if the last $(k-2)$ items of
the first one is identical to the first $(k-2)$ items of the
second.}
For instance, $Merge(ABC,BCD) = ABCD$, but $Merge(ABC,ABE)$ is not possible.

\subsection{Candidate Pruning}
Suppose we have have this set of \textit{frequent} 3-itemset and the generated $L_4$ candidates with alternative $F_{k-1} \times F_1$ method:
\begin{align*}
   F_3 = {ABC, ABD, ABE,ACD,BCD,BDE,CDE} \\
   L_4 = {ABCD, ACDE, ABDE, BCDE}
\end{align*}
{To prune the candidates in $L_4$ we must check if all their possible 3-item subsets are in $F_3$:\ns
\begin{itemize}
   \item $ABCD$ - all subsets are frequent - keep
   \item $ACDE$ - $ADE$ and $ACE$ are not frequent - prune
   \item $ABDE$ - $ADE$ is not frequent - prune
   \item $BCDE$ - $BCE$ is not frequent - prune
\end{itemize}}

At this point, the only candidate left is $ABCD$ which \textit{might} be frequent, while the others could not possibly be, thus they were pruned. We need to scan the database to count its support.

This can be done with the aid of a hash tree structure to store the candidates and facilitate efficient support counting, to avoid checking each candidate against every transaction (computationally expensive).

\subsection{Rule Generation}
Once we have found all frequent itemsets, we can generate high-confidence association rules from each frequent itemset. \\
Given a frequent itemset $L$, find all non-empty subsets $f \subset L$ such that $f \rightarrow (L - f)$ has confidence $\geq$ \textit{minconf}.
If $|L| = k$, there are $2^k - 2$ possible rules that can be generated from $L$ (excluding the empty set $L\rightarrow\emptyset$ and the set itself $\emptyset \rightarrow L$).

In general, confidence does not have an antimonotone
property, so $c(ABC \rightarrow D)$ can be larger or smaller than $c(AB \rightarrow D)$.
However, confidence of rules generated from the same itemset has an anti-monotone property. 
E.g., Suppose ${A,B,C,D}$ is a frequent 4-itemset:
\[c(ABC \rightarrow D) \geq c(AB \rightarrow CD) \geq c(A \rightarrow BCD)\]
Confidence is anti-monotone w.r.t. number of items on the right-hand-side (RHS) of the rule. This happens because, recalling the confidence formula 
\[c(X \Rightarrow Y) = \frac{s(X \cup Y)}{s(X)}\]
the numerator $s(X \cup Y)$ remains the same while the denominator $s(X)$ decreases as we move items from the RHS to the left-hand-side (LHS) of the rule, because $\sigma(A) \geq \sigma(AB) \geq \sigma(ABC)$ according to the Apriori principle, yielding an increase in confidence.

This allows to prune rules during generation during the process, as in the figure below.

\begin{figure}[htbp]
   \centering
   \includegraphics{images/08/confidenceLattice.png}
   \caption{Confidence Lattice}
   \label{fig:08/confidenceLattice}
\end{figure}

\begin{definition}
   [Closed Itemset]
   An itemset X is \textbf{closed} if none of its immediate supersets
   has the same support as the itemset X.\\
   X is not closed if at least one of its immediate supersets
   has support count as X.
\end{definition}

\begin{paracol}{2}
   \colfill
\begin{table}[H]
   \centering
   \begin{tabular}{|c|c|}
      \hline
      TID & Items \\
      \hline
      1   & \{A, B\}         \\
      2   & \{B, C, D\}            \\
      3   & \{A, B, C, D\}            \\
      4   & \{A, B, D\}         \\
      5   & \{A, B, C, D\}            \\
      \hline
   \end{tabular}
   \caption{Example Transaction Database}
   \label{tab:08/exampleTransactionDB}
\end{table}
   \colfill

\switchcolumn

\begin{table}[H]
   \centering
   \begin{tabular}{|c|c|}
      \hline
      Itemset & Support \\
      \hline
      \{A\} & 4 \\
      \{B\} & 5 \\
      \{C\} & 3 \\
      \{D\} & 4 \\
      \{A,B\} & 4 \\
      \{A,C\} & 2 \\
      \{A,D\} & 3 \\
      \{B,C\} & 3 \\
      \{B,D\} & 4 \\
      \{C,D\} & 3 \\
      \{A,B,C\} & 2 \\
      \{A,B,D\} & 3 \\
      \{A,C,D\} & 2 \\
      \{B,C,D\} & 2 \\
      \{A,B,C,D\} & 2 \\
      \hline
   \end{tabular}
   \caption{Frequent Itemsets and Their Support}
   \label{tab:08/frequent_itemsets}
\end{table}
\end{paracol}

\subsection{Closed Itemsets}

From the example above, the \textbf{closed itemsets} are:
\begin{itemize}
   \item \{B\} with support 5 - closed because its immediate superset \{A,B\} has support 4 $\neq$ 5
   \item \{A,B\} with support 4 - closed because its immediate supersets \{A,B,C\} and \{A,B,D\} have support 2 and 3 respectively, both $\neq$ 4
   \item \{B,D\} with support 4 - closed because its immediate superset \{A,B,D\} has support 3 $\neq$ 4 and \{B,C,D\} has support 2 $\neq$ 4
   \item \{A,B,D\} with support 3 - closed because its immediate superset \{A,B,C,D\} has support 2 $\neq$ 3
   \item \{C,D\} with support 3 - closed because its immediate supersets have different support
   \item \{A,B,C,D\} with support 2 - closed because it has no supersets
\end{itemize}

Examples of \textbf{non-closed itemsets}:
\begin{itemize}
   \item \{A\} with support 4 is \textbf{not closed} because its immediate superset \{A,B\} has the same support 4
   \item \{A,C\} with support 2 is \textbf{not closed} because its immediate superset \{A,B,C\} has the same support 2
   \item \{A,B,C\} with support 2 is \textbf{not closed} because its immediate superset \{A,B,C,D\} has the same support 2
\end{itemize}

Closed itemsets are important because they provide a compact representation of all frequent itemsets while preserving complete support information.

\subsection{Maximal Itemsets}

\begin{definition}
   [Maximal Itemset]
   An itemset X is \textbf{maximal} if none of its immediate\footnote{immediate means? \dots TODO} supersets
   is frequent.\\
   X is not maximal if at least one of its immediate supersets
   is frequent.
\end{definition}

Assuming a minimum support threshold of 2, the \textbf{maximal frequent itemsets} from the example are:
\begin{itemize}
   \item \{B\} with support 5 - maximal if we consider only singleton itemsets, but \textbf{not maximal} overall because its supersets \{A,B\}, \{B,C\}, \{B,D\} are frequent
   \item \{A,B,D\} with support 3 - maximal because its only superset \{A,B,C,D\} has support 2, which is still frequent, so \textbf{not maximal}
   \item \{B,D\} with support 4 - \textbf{not maximal} because its superset \{A,B,D\} is frequent
   \item \{C,D\} with support 3 - \textbf{not maximal} because its supersets \{A,C,D\}, \{B,C,D\}, and \{A,B,C,D\} are frequent (support $\geq$ 2)
   \item \{A,B,C,D\} with support 2 - \textbf{maximal} because it has no supersets and it is frequent
\end{itemize}

Therefore, with minsup = 2, the only \textbf{maximal frequent itemset} is:
\begin{itemize}
   \item \{A,B,C,D\} with support 2
\end{itemize}

\note{
   Every maximal frequent itemset is also closed, but not every closed itemset is maximal. Maximal itemsets provide an even more compact representation than closed itemsets, but they only preserve information about which itemsets are frequent, not their exact support counts.
}

\begin{figure}[htbp]
   \centering
   \includegraphics{images/08/sets.png}
   \caption{Relationships between frequent, closed, closed frequent and maximal frequent itemsets}
   \label{fig:08/sets}
\end{figure}

\section{Confidence}
The \textbf{confidence} of an association rule $X \Rightarrow Y$ is a measure of the reliability of the rule. It is defined as the conditional probability that a transaction contains the itemset $Y$ given that it contains the itemset $X$. Mathematically, confidence is expressed as:
\[c(X \Rightarrow Y) = \frac{s(X \cup Y)}{s(X)}\]
where:
\begin{itemize}
   \item $s(X \cup Y)$ is the support of the itemset that contains both $X$ and $Y$.
   \item $s(X)$ is the support of the itemset $X$.
\end{itemize}

The confidence value ranges from 0 to 1, where a higher confidence indicates a stronger association between the itemsets $X$ and $Y$. For example, a confidence of 0.8 means that 80\% of the transactions that contain $X$ also contain $Y$.

\subsection{Contingency tables}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
 & \textit{X} & $\overline{\textit{X}}$ &  \\ \hline
\textit{Y} & $f_{11}$ & $f_{10}$ & $f_{1+} = f_{11} + f_{10}$ \\ \hline
$\overline{\textit{Y}}$ & $f_{01}$ & $f_{00}$ & $f_{0+} = f_{01} + f_{00}$ \\ \hline
\hline
 & $f_{+1} = f_{11} + f_{01}$ & $f_{+0} = f_{10} + f_{00}$ & $N$ \\ \hline
\end{tabular}
\caption{Contingency table for itemsets X and Y}
\end{table}


Given a dataset with $N$ transactions, we can represent the occurrences of itemsets $X$ and $Y$ using a contingency table, where the entries are the support counts of the combinations of presence and absence of $X$ and $Y$:

\begin{paracol}{2}
   
   \begin{table}[h]
      \centering
      \begin{tabular}{|c|c|c|c|}
         \hline
         \textbf{Customers} & \textbf{Tea} & \textbf{Coffee} & $\cdots$ \\ \hline
         C1 & 0 & 1 & $\cdots$ \\ \hline
         C2 & 1 & 0 & $\cdots$ \\ \hline
         C3 & 1 & 1 & $\cdots$ \\ \hline
         C4 & 1 & 0 & $\cdots$ \\ \hline
         $\cdots$ &  &  &  \\ \hline
      \end{tabular}
      \caption{Transaction data for Tea and Coffee}
   \end{table}
   
   \switchcolumn

   \begin{table}[h]
   \centering
   \begin{tabular}{|c|c|c|c|}
   \hline
    & \textit{Coffee} & $\overline{\textit{Coffee}}$ &  \\ \hline
   \textit{Tea} & 150 & 50 & 200 \\ \hline
   $\overline{\textit{Tea}}$ & 650 & 150 & 800 \\ \hline
   \hline
    & 800 & 200 & 1000 \\ \hline
   \end{tabular}

      \begin{tabular}{|c|c|c|c|}
   \hline
    & \textit{Honey} & $\overline{\textit{Honey}}$ &  \\ \hline
   \textit{Tea} & 100 & 100 & 200 \\ \hline
   $\overline{\textit{Tea}}$ & 20 & 780 & 800 \\ \hline
   \hline
    & 120 & 880 & 1000 \\ \hline
   \end{tabular}
   \caption{Contingency table for Tea and Coffee, and another one for Tea and Honey}
   \end{table}



\end{paracol}
$Confidence \approx P(Coffee|Tea) = \frac{150}{200} = 0.75$, suggesting that people who drink tea are more likely to drink coffee than not drink coffee, given that the confidence is greater than $0.5$.\\
But, in fact, $P(Coffee) = \frac{800}{1000} = 0.8$, meaning that people who drink tea are actually less likely to drink coffee than the general population.

$Confidence \approx P(Honey|Tea) = \frac{100}{200} = 0.5$, suggesting that people who drink tea are equally likely to drink honey or not drink honey.\\
But, actually, $P(Honey) = \frac{120}{1000} = 0.12$, meaning that people who drink tea are much more likely to drink honey than the general population.

\subsection{Drawbacks}

Confidence has some drawbacks:
\begin{itemize}
   \item It does not consider the overall frequency of the consequent itemset $Y$ in the dataset.
   \item It can be misleading in cases where the consequent itemset $Y$ is very common in the dataset, leading to high confidence values even when there is no real association between $X$ and $Y$.
\end{itemize}
\note{To address these issues, other measures such as \textbf{lift} and \textbf{conviction} are often used in conjunction with confidence to provide a more comprehensive evaluation of association rules.}

\subsubsection{What rules do we want}
\begin{itemize}
	\item Confidence$(X \Rightarrow Y)$ should be sufficiently high
	\begin{itemize}
		\item To ensure that people who buy $X$ will more likely buy $Y$ than not buy $Y$
	\end{itemize}
	\item Confidence$(X \Rightarrow Y) > support(Y)$
	\begin{itemize}
		\item Otherwise, rule will be misleading because having item $X$ actually reduces the chance of having item $Y$ in the same transaction
	\end{itemize}
	\item Is there any measure that capture this constraint?
	\begin{itemize}
		\item Answer: \textit{Yes}. There are many of them.
	\end{itemize}
\end{itemize}

\section{Other criteria}
$ \textit{confidence}(X \Rightarrow Y) = support(Y)$ is equivalent to:
\begin{align*}
   P(Y|X) = P(Y)\\
   P(X,Y) = P(X)P(Y) \qquad \text{(independence)}
\end{align*}

\begin{align*}
   P(X,Y) < P(X)P(Y) \qquad \text{(negative correlation)}\\
   P(X,Y) > P(X)P(Y) \qquad \text{(positive correlation)}
\end{align*}


\subsection{Lift}
The \textbf{lift} of an association rule $X \Rightarrow Y$ is a measure of how much more likely the occurrence of itemset $Y$ is when itemset $X$ is present, compared to when $Y$ occurs independently of $X$. It is defined as:
\[\text{lift}(X \Rightarrow Y) = \frac{c(X \Rightarrow Y)}{s(Y)} = \frac{s(X \cup Y)}{s(X) \times s(Y)}\]
where:
\begin{itemize}
   \item $c(X \Rightarrow Y)$ is the confidence of the rule.
   \item $s(Y)$ is the support of the itemset $Y$.
\end{itemize}

In the slides it is defined as:
\[\text{lift}(X \Rightarrow Y) = \frac{P(X,Y)}{P(X) \times P(Y)}\]

A lift value greater than 1 indicates a positive association between $X$ and $Y$, meaning that the presence of $X$ increases the likelihood of $Y$. A lift value less than 1 indicates a negative association, while a lift value equal to 1 suggests that $X$ and $Y$ are independent.
\note{Lift is particularly useful for identifying interesting rules that may not be apparent from confidence alone, as it accounts for the overall frequency of the consequent itemset $Y$ in the dataset.}

Lift would fix the coffee problem, because, 0.75 / 0.8 = 0.9375 < 1, indicating a negative correlation between tea and coffee.
Also the honey problem would be fixed, because 0.5 / 0.12 = 4.1667 > 1, indicating a positive correlation between tea and honey.

\subsection{Interest}
The \textbf{interest} of an itemset $X$ is a measure of how much the actual support of $X$ deviates from what would be expected if the items in $X$ were independent. It is defined as:

\[\text{interest}(X) = \frac{P(X,Y)}{P(X),P(Y)}\]

A positive interest value indicates that the items in $X$ co-occur more frequently than would be expected under independence, suggesting a positive association. A negative interest value indicates that the items co-occur less frequently than expected, suggesting a negative association. An interest value of zero suggests that the items are independent.

\note{
   Alternative copilot definition
   \[\text{interest}(X) = s(X) - \prod_{i \in X} s(\{i\})\]
where:
\begin{itemize}
   \item $s(X)$ is the support of the itemset $X$.
   \item $\prod_{i \in X} s(\{i\})$ is the product of the supports of the individual items in $X$.
   \item[$\circ$] 
   Written like this it should always be negative\dots that's weird
\end{itemize}
}

\subsection{Other measures}
In the slide two other measures are defined:
\begin{align*}
   PS = P(X,Y) - P(X)P(Y) \\
   \sigma \textit{-coefficient} = \frac{P(X,Y) - P(X)P(Y)}{\sqrt{P(X)P(Y)(1-P(X))(1-P(Y))}}
\end{align*}
These measures also aim to capture the degree of association between itemsets, taking into account their individual supports and the expected co-occurrence under independence.

\section{Non-binary Attributes}

\begin{figure}[htbp]
   \centering
   \includegraphics{images/08/nonbinary.png}
   \caption{Handling non-binary attributes by changing the actual columns}
   \label{fig:08/nonbinary}
\end{figure}

\subsection{Categorical attributes}
Categorical attributes are variables that can take on a limited, fixed number of possible values, representing distinct categories or groups. These attributes are often used in data mining and machine learning tasks to classify or group data points based on their characteristics.

We have to apply association analysis to non-asymmetric binary attributes, so we have to formulate rules like:
\[
   {Gender=Male, Age \in [21,30)} \Rightarrow {\textit{No of hours online} \geq 10}
\]

Some attributes can have many possible values, with many of these values having very low support. To deal with this, we can use \textbf{attribute
generalization}, which involves replacing specific attribute values with more general categories based on a predefined \textbf{hierarchy} or \textbf{taxonomy}.
For example, instead of using specific ages, we can group them into age ranges like [0-10), [10-20), [20-30), etc. This helps to reduce the number of distinct values and increases the support for each category, making it easier to identify meaningful patterns in the data.

In some other cases we also may have a hierarchy of values for an attribute. For example, for the attribute ``Location'', we may have a hierarchy like City $\rightarrow$ State $\rightarrow$ Country. In such cases, we can use the hierarchy to generalize the attribute values and find association rules at different levels of granularity.

\subsection{Continuous attributes}
Continuous attributes are variables that can take on an infinite number of values within a given range. These attributes are often used in data mining and machine learning tasks to represent measurements or quantities that can vary continuously.

To handle such values we have various methods:
\begin{itemize}
   \item Discretization-based
   \item Statistics-based
   \item Non-discretization (such as \textsc{minApriori})
\end{itemize}

\note{
   // TODO skipped support counting with hash tree
}


\section{Rule Extraction}
\subsection{Association Rule Mining}
Association Rule Mining refers to (everything in the previous sections \smiley), given a set of transactions, finding rules that will predict the occurrence of an item based on the occurrences of other items in the transaction. Formally, an association rule is an implication of the form $X \Rightarrow Y$, where $X$ and $Y$ are disjoint itemsets. The goal is to discover interesting relationships between items in large datasets.

Rules obtained in this way are unordered, so no priority is given to any of them. When applying the rules to a new instance, we may have multiple rules that apply, and we need a way to resolve conflicts.

They induce a form of local optimization (?) since each rule is evaluated independently of the others.

\subsection{Rule lists}
 A rule list is a sorted set of rules: starting from rule $1$, if rule $i$ does not support the given instance, move to rule $i+1$.
Here, we do have priority: the first rule that supports the instance is the one predicting.

 \lstset{language=python}
 \begin{lstlisting}
(1) if age in (23, 26) and priors in (2, 3) then recidivous
(2) if age in (18, 20) then recidivous
(3) if sex is male and age in (21, 22) then recidivous
(4) if priors > 3 then recidivous
(5) else not_recidivous
 \end{lstlisting}

 Support, confidence, and other measures still hold, but we need to reconsider support: in
a rule list, the first rule to support an instance is the one predicting.
To adjust, we treat support as an indicator variable, evaluating to $1$ for the \textit{first} rule of a given list to apply, and otherwise: $0$.

\[
supp_A(r,x) = \begin{cases}
   1 \quad \textit{if } r \in A \textit{ is the first rule to satisfy instance } x\\
   0 \quad \textit{otherwise}
\end{cases}
\]


\begin{table}[htbp]
   \centering
   \begin{tabular}{|c|c|c|c|c|c|c|c|}
      \hline
      age & priors & sex & $r_1$ & $r_2$ & $r_3$ & $r_4$ & $r_5$\\
      \hline
      24 & 4 & m & 0 & 0 & 0 & 1 & 0\\
      20 & 1 & f & 0 & 1 & 0 & 0 & 0\\
      20 & 5 & m & 0 & 1 & 0 & 0 & 0\\
      \hline
   \end{tabular}
   \caption{Support indicator variable for a rule list}
   \label{tab:08/support_indicator}
\end{table}

\subsection{Branch and bound algorithms}

Family of optimization algorithms that defines a space of solutions to search, according
to a given objective function. Exploring the whole space is unfeasible, hence branch and
bound algorithms define:
\begin{itemize}
	\item A set of feasible solutions to explore from a starting set: the branches of the solution tree to explore
	\item A bound for the objective: it allows to prune branches, thus reducing the search space
\end{itemize}

A search tree has an
exponentially large number
of states! For a tree of order $k$ and depth $d$, $\mathcal{O}(k^d)$ states!
So we do not need to go very deep in the tree, just enough to find a good solution.

Exploration populates a queue of states to consider: the larger the queue, the larger the computational cost. A search algorithm simply
\begin{itemize}
	\item Inserts states in the queue
	\item Pops states from the queue
\end{itemize}

\subsection{CORELS - Certifiably Optimal Rule Lists}
CORELS is a branch-and-bound algorithm to learn optimal rule lists for categorical data. It uses a variety of techniques to prune the search space and efficiently find the optimal rule list.

We model our rule list using a tuple $R = \langle P_R, Y_R, y_r\rangle$, where:
\begin{itemize}
   \item $P_R$ is the ordered list of antecedents (if-clauses)
   \item $Y_R$ is the list of consequent predictions (then-clauses)
   \item $y_r^0$ is the default prediction (else-clause)
   \item[\circ] $k^R$ is the number of rules in the list (length of $P_R$ and $Y_R$)
\end{itemize}

Rules are ordered, and a Rule list $R$ may be prefix of another Rule list $R'$, having $R \preceq R^+$.
In the search tree, child nodes extend parent nodes by adding a new rule at the end of the list. 

\begin{align*}
(1)\;& \textbf{if } \text{age} \in (23,26) \textbf{ and } \text{priors} \in (2,3)
      \textbf{ then recidivous} \\
(2)\;& \textbf{if } \text{age} \in (18,20)
      \textbf{ then recidivous} \\
(3)\;& \textbf{else not\_recidivous}
\end{align*}

\vspace{1em}

\begin{itemize}
  \item $k^R = 2$
  \item $P_R : (p^1,\ldots,p^{k^R}) 
    = (\text{age} \in (23,26) \text{ and priors} \in (2,3),\ 
       \text{age} \in (18,20))$
  \item $Y_R : (y_R^1,\ldots,y_R^{k^R})
    = (\text{recidivous},\ \text{not\_recidivous})$
  \item $y_R^0 = \text{not\_recidivous}$
\end{itemize}

\subsubsection{CORELS empirical Error}
\[
l(R,X,Y)=l_R(R,X,Y)+l_0(R,X,Y),
\]
dove
\[
l_R(R,X,Y)=\frac{1}{n}\sum_{(x,y)\in X\times Y}\sum_{j=1}^{k^R}
\operatorname{supp}_{P_R}(p_R^j,x)\;\mathbf{1}[y\neq y_R^j],
\]
% \[
% l_0(R,X,Y)=\frac{1}{n}\sum_{(x,y)\in X\times Y}\Big(1-
% \sum_{j=1}^{k^R}\operatorname{supp}_{P_R}(p_R^j,x)\Big)\;
% \mathbf{1}[y\neq y_R^{k^R}].
% \]
\[
l_0(R,X,Y)=\frac{1}{n}\sum_{(x,y)\in X\times Y}\Big(1-
\operatorname{supp}(P_R,x)\Big)\;
\mathbf{1}[y\neq y_R^{0}]
\]


To read the formula, the fract and the sum over $X \times Y$ is just the average over all instances, which will yield a ratio.
Recall that $\operatorname{supp}_{P_R}(p_R^j,x)$ is $1$ if rule $j$ is the first rule to satisfy instance $x$, and $0$ otherwise, and that \ul{\textit{exactly one} rule will satisfy} $x$.
The rightmost indicator function is $1$ if the prediction of rule $j$ ($y_R^j$) does not match the actual label $y$ of instance $x$, and $0$ otherwise.\\
In other words, $l_R(R,X,Y)$ counts the fraction of misclassified instances by the rules in the list.
The second term, $l_0(R,X,Y)$, handles the default rule (else-clause). It counts the fraction of misclassified instances that are not covered by any of the rules in the list.
\nl

We may compute bounds on \textbf{length} and \textbf{accuracy}. The explanation related to length is rather confused, I couldn't get it all, the idea is that we have an an upper bound on the maximum number of rules ($k^R$) we want in the list, and if we reach that number, we can prune that branch of the search tree.\\
For the accuracy, the idea is that the number of instances correctly classified by a rule $p$ to be added to the rule list should be higher than some threshold, in order to allow the addition of that rule to the list. If not, we can prune that branch of the search tree.

\subsubsection{Trimming the search space}
Trimming is related to \textbf{Loss} $L_R$ and \textbf{Lower bound} $b(R)$.
Here we see appearing a \textbf{regularization} term $\lambda k^R$, where $\lambda$ is a hyperparameter that controls the trade-off between \textit{accuracy} and \textit{complexity} of the rule list, and $k^R$ is the number of rules in the list.\\
In general less rules mean more generalization and interpretability, so we want to penalize longer rule lists, trading with the potential increase in accuracy they might yield.

\begin{align*}
   L(R,X,Y) &= l(R,X,Y) + \lambda k^R \\
   b(R,X,Y) &= l_R(R,X,Y) + \lambda k^R \leq L(R,X,Y)
\end{align*}


\note{In formulas this  is a mess, and I couldn't get it.}

The overall idea, if I understood correctly is that extensions $R^+$ of a rule list $R$ will always have a loss $L_R(R^+)$ greater than or equal to the loss of the parent rule list $L_R(R)$, because adding more rules can only increase the number of misclassifications (or leave it unchanged, because the rules in $R$ have priority, and if they were picked before, they will still be picked even if there are more rules ``at the bottom''). Thus, if we have already found a rule list $R_{best}$ with a certain loss lower than $R$'s, we can prune any extensions of $R$ that cannot improve upon this loss. 




\section{FP Tree Growth}

The FP-tree contains a compressed
representation of the transaction database.

A trie (prefix-tree) data structure is used

Each transaction is a path in the tree (paths may overlap).
Once the FP-tree is constructed the recursive, divide-and-conquer FP-Growth algorithm is used to enumerate all frequent itemsets.

Since transactions are sets of items, we
need to transform them into ordered
sequences so that we can have
prefixes
% â€¢ Otherwise, there is no common prefix
% between sets {A,B} and {B,C,A}
We need to impose an order to the items.Initially, we assume a lexicographic order.

\begin{figure}[htbp]
   \centering
   \includegraphics{images/08/trie1.png}
   \caption{Each transaction is a path in the trie. There is a header table to point to the first occurrence of each item in the tree.}
   \label{fig:08/trie1}
\end{figure}

The size of the tree depends on the compressibility
of the data
\begin{itemize}
	\item \textit{Extreme case}: All transactions are the same, the FP-tree is a single branch
	\item \textit{Extreme case}: All transactions are different the size of the tree is the same as that of the database (bigger actually since we need additional pointers)
\end{itemize}

The size of the tree also depends on the ordering of the items.
We could order the items according to their frequency from larger to smaller, but we need to do an extra pass over the dataset to count frequencies.

\begin{algorithm}
\caption{FP-Growth Algorithm}
\begin{algorithmic}[1]
\For{each suffix $X$}
    \State \textbf{Phase 1:} Construct the prefix tree for $X$ and compute the support using the header table and the pointers
    \If{$X$ is frequent}
        \State \textbf{Phase 2:} Construct the \textit{conditional} FP-tree for $X$:
        \State \quad 1. Recompute support
        \State \quad 2. Prune infrequent items
        \State \quad 3. Prune leaves and recurse
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}