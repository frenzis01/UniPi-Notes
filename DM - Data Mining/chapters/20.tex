\chapter{Questions \& Answers}

These will be answered in italian, as the exam will be in italian. The questions are collected from the students of the course, and they are not necessarily the ones that will be asked in the exam, but they are a good indication of the topics that may be covered.

\section{Data Understanding and Representation}

\subsection{t-SNE}
% - t-SNE. (x4)
% - tsne

\href{https://www.youtube.com/watch?v=NEaUSP4YerM}{Check out this video for a visual explanation of t-SNE.}


t-SNE è una tecnica di riduzione della dimensionalità non lineare, particolarmente utile per la visualizzazione di dati ad alta dimensione in uno spazio a due o tre dimensioni.
t-SNE funziona costruendo una distribuzione di probabilità sui dati ad alta dimensione, in cui la probabilità che due punti siano vicini è proporzionale alla loro distanza. Poi, t-SNE cerca di trovare una rappresentazione a bassa dimensione che preservi queste probabilità, minimizzando la divergenza di Kullback-Leibler tra le due distribuzioni.

t-SNE funziona in due fasi principali:
\begin{enumerate}
   \item Fase di \textbf{Similarità}: nello spazio originale $\mathcal{X}$, quanto sono simili $x_i$ e $x_j$?\\
   La similarità è misurata convertendo la distanza Euclidea in una distribuzione di probabilità $P_{ij}$, che è simmetrica e normalizzata. Tipicamente si usa una distribuzione gaussiana centrata su $x_i$, con una varianza adattiva $\sigma_i$ (\textit{``perplexity''}) che è scelta in modo da mantenere un numero costante di vicini per ogni punto.\\
   Notare che la probabilità di vicinanza viene calcolata per ogni punto $x_i$, e la similarità fra $x_i$ e $x_j$, può avere valori diversi, perché dipende dalla varianza adattiva $\sigma_i$ e $\sigma_j$, che dipende dalla densità locale dei dati.\\
   Per ottenere similarità simmetriche, quindi $P_{ij} = P_{ji}$, si impone che somma di tutte le probabilità ---di vicinanza con $x_j$--- sia 1, ovvero che $\sum_{i \neq j} P_{ij} = 1$ e che la probabilità di vicinanza sia la media, ovvero $P_{ij} = \frac{p_{ij} + p{ji}}{2}$.\\
   Questo permette di costruire una matrice di similarità simmetrica, che è più facile da gestire nella fase di embedding.

   In altre parole, $P_{ij}$ rappresenta la probabilità che $x_i$ e $x_j$ siano vicini nello spazio originale, e dipende dalla distanza tra i due punti e dalla varianza adattiva.

   \item Fase di \textbf{Embedding} (mappatura): 
   nello spazio mappato (a due/tre dimensioni) $\hat{\mathcal{X}}$, quanto sono simili $\hat{x}_i$ e $\hat{x}_j$?\\
   Nello spazio a bassa dimensione $\mathcal{Y}$, t-SNE cerca di trovare una rappresentazione $\hat{x}_i$ per ogni punto $x_i$ in modo che le probabilità $Q_{ij}$ (ovvero la probabilità che $\hat{x}_i$ e $\hat{x}_j$ siano vicini nello spazio a bassa dimensione) siano il più simili possibile alle probabilità $P_{ij}$ nello spazio originale.\\
   Questo viene fatto minimizzando la divergenza di Kullback-Leibler tra le due distribuzioni.
   \[C = \sum_i KL(P_i||Q_i) = \sum_i\sum_j p_{j|i} \log \frac{p_{j|i}}{q_{j|i}}\]

   La formula si legge: per ogni punto $i$, calcoliamo la divergenza di Kullback-Leibler tra la distribuzione $P_i$ (le probabilità di vicinanza nello spazio originale) e la distribuzione $Q_i$ (le probabilità di vicinanza nello spazio a bassa dimensione), e poi sommiamo queste divergenze per tutti i punti.\\
   La distribuzione di probabilità $Q_{ij}$ nello spazio a bassa dimensione è definita usando una distribuzione di Student con un grado di libertà (equivalente a una distribuzione di Cauchy), che ha code più pesanti rispetto a una distribuzione gaussiana. Questo aiuta a preservare la struttura locale dei dati, evitando che i punti vicini nello spazio originale vengano proiettati troppo lontano nello spazio a bassa dimensione.

   A grandi linee, la matrice di similarità nello spazio a bassa dimensione, essendo i dati in esso inizialmente posizionati casualmente, avrà un aspetto casuale, e t-SNE cercherà di ottimizzare la posizione dei punti per minimizzare la differenza fra la matrice di similarità nello spazio originale e quella nello spazio a bassa dimensione, in modo da preservare la struttura dei dati il più possibile.
\end{enumerate}

t-SNE è particolarmente efficace nel preservare la struttura locale dei dati, il che significa che i punti che sono vicini nello spazio originale tendono ad essere vicini anche nello spazio a bassa dimensione. Tuttavia, t-SNE può essere meno efficace nel preservare la struttura globale dei dati, e può essere sensibile alla scelta dei parametri (come la perplexity) e alla scala dei dati.

Inoltre, t-SNE è non-deterministico, il che significa che può produrre risultati diversi a seconda dell'inizializzazione e dei parametri scelti. 
Il motivo è che t-SNE, a grandi linee, plotta i dati su un asse assegnando a ciascun punto una posizione casuale, e poi cerca di clusterizzare (avvicinare) i punti che sono vicini nello spazio originale, e di allontanare quelli che sono lontani. Se l'inizializzazione è diversa, il processo di clusterizzazione può portare a risultati diversi.
Più nello specifico, t-SNE cerca di ottimizzare la posizione dei punti per minimizzare la divergenza di Kullback-Leibler. Quindi, a seconda dell'inizializzazione casuale, t-SNE può convergere a soluzioni diverse, anche se i risultati tendono ad essere simili in termini di struttura dei dati.


\subsection{Handling unbalanced classes}
% - Gestione di classi sbilanciate (unbalanced classes). (x1)
% - how to handle unbalanced classes

TODO 

No clue, many answers are possible\dots


\subsection{UMAP and manifold learning}

% - UMAP (data representation / manifold learning). (x1)

UMAP è una tecnica di riduzione della dimensionalità e di apprendimento del manifold che si basa su concetti di topologia e geometria. UMAP cerca di preservare sia la struttura locale che quella globale dei dati, ed è spesso più veloce di t-SNE, soprattutto su dataset di grandi dimensioni.

Le distanze nello spazio originale inducono una struttura topologica sui dati, che UMAP cerca di preservare nello spazio a bassa dimensione. UMAP costruisce un grafo di connettività basato sulle distanze tra i punti nello spazio originale, e poi cerca di trovare una rappresentazione a bassa dimensione che preservi questa struttura di vicinanza.

A partire dal grafo di connettività, UMAP costruisce una matrice di adiacenza i cui valori rappresentano la forza della connessione tra i punti, corrispondenti alle distanze nello spazio originale e ai pesi degli archi nel grafo.

Per un set di archi E, UMAP minimizza una funzione di costo dove si considera sia la probabilità di esistenza di un arco nello spazio originale che la probabilità di esistenza di un arco nello spazio a bassa dimensione. In altri termini, se è probabile che esista un arco tra due punti nello spazio originale, allora è desiderabile che esista anche un arco tra i corrispondenti punti nello spazio a bassa dimensione, e viceversa. La funzione di costo è data da:
\[
-\sum_{e \in E} \left( \underbrace{\Pr(e; X) \log(\Pr(e; Z))}_{\text{existing edges}} + \underbrace{(1 - \Pr(e; Z)) \log(1 - \Pr(e; X))}_{\text{non-existing edges}} \right),
\]
Dove $\Pr(e; X)$ è la probabilità che esista un arco tra i punti nello spazio originale, e $\Pr(e; Z)$ è la probabilità che esista un arco tra i corrispondenti punti nello spazio a bassa dimensione. La funzione di costo cerca di massimizzare la probabilità di preservare gli archi esistenti e di minimizzare la probabilità di introdurre archi non esistenti.
% In this way, when two points in the original space are close ($\Pr(e; X) \approx 1$), the first term dominates and the cost is minimized by making them close in the transformed space as well ($\Pr(e; Z) \approx 1 \Rightarrow \log(\Pr(e; Z)) \approx 0$), because if instead they are far apart in the transformed space $Z$ ($\Pr(e; Z) \approx 0$), then $\log(\Pr(e; Z)) \approx -\infty$ and the cost becomes very large.\\

% Conversely, when two points in the original space are far apart ($\Pr(e; X) \approx 0$), the second term dominates and the cost is minimized by making them far apart in the transformed space as well ($\Pr(e; Z) \approx 0 \Rightarrow \log(1 - \Pr(e; Z)) \approx 0$), because if instead they are close in the transformed space ($\Pr(e; Z) \approx 1$), then $\log(1 - \Pr(e; Z)) \approx -\infty$ and the cost becomes very large.

In questo modo, quando due punti nello spazio originale sono vicini ($\Pr(e; X) \approx 1$), il primo termine domina e il costo è minimizzato rendendoli vicini anche nello spazio trasformato ($\Pr(e; Z) \approx 1 \Rightarrow \log(\Pr(e; Z)) \approx 0$), perché se invece sono lontani nello spazio trasformato $Z$ ($\Pr(e; Z) \approx 0$), allora $\log(\Pr(e; Z)) \approx -\infty$ e il costo diventa molto grande.

UMAP è non-deterministico, in quanto l'inizializzazione dei punti nello spazio a bassa dimensione è casuale (Copilot guess), e può produrre risultati diversi a seconda dell'inizializzazione e dei parametri scelti. Tuttavia, UMAP tende a produrre risultati più stabili rispetto a t-SNE, soprattutto su dataset di grandi dimensioni.

\subsection{PCA}
% - What is the main objective of a Principal Component Analysis-based representation?
% - data representation (PCA, TSNE, UMAP)

La Principal Component Analysis (PCA) è una tecnica di riduzione della dimensionalità che trasforma i dati in un nuovo sistema di coordinate, dove le nuove variabili (componenti principali) sono combinazioni lineari delle variabili originali e sono ordinate in modo tale che la prima componente cattura la massima varianza possibile, la seconda componente cattura la massima varianza residua, e così via. La matrice di covarianza dei dati originali gioca un ruolo centrale nella PCA, poiché le componenti principali sono gli autovettori della matrice di covarianza.

Gli step principali, a grandi linee, sono:
\begin{enumerate}
   \item A partire dai dati originali si ottengono i dati centrati (valori relativi) sottraendo la media da ogni colonna. Dunque, dalla matrice originale $X$, si ottiene la matrice centrata $\bar{X}$.
   \item Si calcolano gli autovettori $V$ e gli autovalori della matrice di covarianza dei dati centrati $\bar{X}$. La matrice di covarianza $\bar{\Sigma}$ è data da $\frac{1}{n-1} \bar{X}^T \bar{X}$, dove $n$ è il numero di campioni.
   \item Per eseguire la proiezione dei dati nello spazio delle componenti principali ci serve $V^T$ la trasposizione di $V$.
   Gli autovettori $V$ sono ordinati in base agli autovalori, in modo che le prime colonne di $V$ corrispondano alle componenti principali che catturano la maggior parte della varianza dei dati.
   \item Otteniamo i dati proiettati $\hat{X} = V^T\bar{X}$
\end{enumerate}

\subsubsection{What does the matrix $\bar{\Sigma}$ (covariance matrix) represent?}
% - PCA, what represents the matrix X^TX (covariance matrix)? What changes in the covariance matrix of the data after the application of PCA?
% - PCA: obiettivo della rappresentazione; cosa rappresenta $X^T X$ (covariance matrix); cosa cambia nella covarianza dopo PCA. (x4)


La matrice di covarianza $\bar{\Sigma}$ rappresenta la covarianza tra le variabili nei dati centrati. In altre parole, $\bar{\Sigma}_{ij}$ rappresenta la covarianza tra la variabile $i$ e la variabile $j$. La matrice di covarianza è simmetrica e positiva semidefinita, e le sue diagonali rappresentano la varianza di ciascuna variabile.

È importante per la PCA perché le componenti principali sono gli autovettori della matrice di covarianza, e gli autovalori associati a questi autovettori rappresentano la quantità di varianza catturata da ciascuna componente principale.
Ricordando che gli autovettori di una matrice $A$ sono, in generale, vettori $v$ tali che $Av = \lambda v$, dove $\lambda$ è l'autovalore associato all'autovettore $v$, possiamo interpretare gli autovettori della matrice di covarianza come le direzioni nello spazio dei dati lungo le quali la varianza è massima, e gli autovalori come la quantità di varianza catturata lungo quelle direzioni.  


\subsubsection{What changes in the covariance matrix of the data after the application of PCA?}

Dopo l'applicazione della PCA, i dati proiettati $\hat{X}$ hanno una matrice di covarianza che è diagonale, con gli autovalori sulla diagonale. Questo significa che le componenti principali sono ortogonali tra loro e non sono correlate, e la varianza è catturata lungo queste componenti principali. In altre parole, la matrice di covarianza dei dati proiettati è data da $\hat{\Sigma} = V^T \bar{\Sigma} V$, dove $V$ è la matrice degli autovettori della matrice di covarianza originale. Poiché $V$ diagonalizza $\bar{\Sigma}$, otteniamo una matrice diagonale $\hat{\Sigma}$ con gli autovalori sulla diagonale, rappresentando la varianza catturata da ciascuna componente principale.





\subsection{What is Grubbs' test algorithm?}
% - Grubs test algorithm and which problems has (Assumption of 1 distrubution, bad with high dimensional data)
% - Grubbs test: algoritmo e problemi/assunzioni (es. assunzione di una sola distribuzione; difficoltà in alta dimensionalità). (x2)


% Grubbs' test è un test statistico utilizzato per identificare outlier in un dataset. L'algoritmo di Grubbs' test si basa sulla statistica $G$, che misura quanto un punto sia distante dalla media del dataset in termini di deviazione standard. La formula per calcolare la statistica $G$ è:
% \[G = \frac{|x_i - \bar{x}|}{s}\]
% dove $x_i$ è il punto che si sta testando, $\bar{x}$ è la media del dataset, e $s$ è la deviazione standard del dataset.
% Grubbs' test confronta la statistica $G$ con un valore critico $G_{critico}$, che dipende dalla dimensione del dataset e dal livello di significatività scelto. Se $G > G_{critico}$, allora il punto $x_i$ è considerato un outlier e può essere rimosso dal dataset.

\begin{algorithm}
   \caption{Grubbs' test for outliers}
   \begin{algorithmic}[1]
      \State Find current outlier set $\hat{X}$
      \State If $\hat{X} = \emptyset$, stop
      \State $X = X \setminus \hat{X}$ - remove outliers from dataset
      \State Go to step 1
   \end{algorithmic}
\end{algorithm}

Il test di Grubbs è un test utilizzato per identificare outlier in un dataset rimuovendo iterativamente i punti che sono considerati outlier fino a quando non ne rimangono più. 
Il test di Grubbs assume una distribuzione Normale dei dati, e identifica come outlier i punti che sono più distanti dalla media in termini di deviazione standard. Il punto chiave è che dopo la rimozione di un set di outlier, i parametri della distribuzione (media e deviazione standard) vengono ricalcolati, e il test viene applicato nuovamente sui dati rimanenti, generando dunque risultati diversi, fino a quando non vengono identificati più outlier.

L'obiettivo del test è di risolvere il ``+1 problem'', ovvero il fatto che rimuovendo un outlier, i parametri della distribuzione cambiano, e quindi è possibile che ci siano altri outlier che non vengono identificati se si applica il test una sola volta. Il test di Grubbs risolve questo problema ricalcolando i parametri dopo ogni rimozione di outlier, e continuando a testare fino a quando non vengono identificati più outlier.


\subsubsection{What's the key problem with the algorithm?}
Il primo problema è che assume che i dati seguano una distribuzione Normale (o comunque una singola distribuzione), il che potrebbe non essere sempre vero. Se i dati non seguono una distribuzione Normale, il test di Grubbs potrebbe non essere efficace nell'identificare gli outlier.

Inoltre, in caso di dati ad alta dimensionalità, il test di Grubbs potrebbe non essere efficace, perché la distanza tra i punti può diventare meno significativa, e quindi potrebbe essere difficile identificare gli outlier in modo affidabile. In alta dimensionalità, la distanza tra i punti tende a diventare più simile, e quindi potrebbe essere difficile distinguere tra punti normali e outlier.


\subsection{Outliers with manifold}
% - Outlier/anomaly detection: manifold ("manyfold"/UMAP), density (mixture models) e neighborhood-based methods (LOF). (x4)

\begin{paracol}{2}
   
   Mentre gli approcchi basati su distribuzione definiscono gli outlier come punti che si discostano significativamente dalla distribuzione dei dati, gli approcci basati su manifold definiscono gli outlier come punti che si discostano significativamente dalla struttura del manifold su cui i dati sono distribuiti. In altre parole, un punto è considerato un outlier se non appartiene al manifold che rappresenta la struttura dei dati, o se è lontano da questo manifold.

   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/20/manifold.png}
      \caption{Esempio di manifold. I segmenti in rosso, corrispondono all'errore di approssimazione del Least Squares $e_{x}$ (discusso sotto).}
      \label{fig:20/manifold}
   \end{figure}
\end{paracol}

Possiamo quindi definire il grado di anomalia $\tilde{o}(x)$ come la distanza di $x$ dal manifold, ovvero $\tilde{o}(x) = d(x, M)$, dove $M$ è il manifold che rappresenta la struttura dei dati. In questo modo, i punti che sono lontani dal manifold avranno un grado di anomalia più alto, e saranno considerati outlier.

Per determinare il manifold possiamo usare il metodo dei \textbf{Least Squares} (assumendo un manifold lineare), che vuole di risolvere l'equazione $Ax = b$ minimizzando la norma del residuo $||Ax - b||_2^2$\footnote{il ``2'' a pedice e apice indica la norma euclidea}. In questo modo, possiamo trovare il manifold che meglio rappresenta la struttura dei dati, e poi calcolare la distanza di ogni punto da questo manifold per identificare gli outlier.

Questa proiezione dei punti lungo il manifold ci fornisce anche l'errore nell'approssimazione (la distanza dei punti dalla retta manifold), che è una misura del grado di anomalia dei punti. I punti che sono lontani dal manifold avranno un errore di proiezione più alto, e saranno considerati outlier.

\[\tilde{o}(x_i) = e_{x_i}\]

\subsubsection{Collinearità}

La collinearità si verifica quando due o più feature sono altamente correlate tra loro.
Il metodo Least Squares non ammette soluzioni quando i dati sono perfettamente collineari, ovvero quando esiste una combinazione lineare esatta tra le feature. In questo caso, la matrice $A$ non è invertibile, e quindi non è possibile trovare una soluzione unica per $x$.
Quando i dati sono molto collineari, ma non del tutto, la soluzione esiste, ma è fortemente instabile, e piccoli cambiamenti nei dati possono portare a grandi cambiamenti nella soluzione. 
\note{In questo caso, è possibile utilizzare tecniche di regolarizzazione, come Ridge Regression o Lasso Regression, per stabilizzare la soluzione e gestire la collinearità. (Copilot guess)
}

\[
\tilde{x}= \underbrace{\Big(\big(\underbrace{A^T A}_{\text{sample covariance matrix}}\big)^{-1} A^T\Big)}_{\text{projection matrix }P}\; b
\]
Il metodo dei Least Squares proietta i dati su un sottospazio definito dalle feature, e la matrice di proiezione $P$ è data da $\big(A^T A\big)^{-1} A^T$. Quando le feature sono collineari, la matrice $A^T A$ diventa singolare \footnote{ovvero non ha inversa, perché ha autovalori nulli}, e quindi non è invertibile, rendendo impossibile trovare una soluzione unica $\tilde{x}$, ma fornendone, invece, infinite.
Se invece, sono fortemente collineari, ma non del tutto, la matrice $A^T A$ è invertibile, ma è mal condizionata, il che significa che piccoli cambiamenti nei dati possono portare a grandi cambiamenti nei coefficienti della soluzione $\tilde{x}$, rendendo la soluzione instabile. Il risultato potrebbe comunque essere circa una retta, ma i coefficienti della soluzione potrebbero essere molto grandi (che si compensano a vicenda), e quindi la soluzione potrebbe essere molto sensibile a piccoli cambiamenti nei dati, portando a risultati non affidabili.


Una possibile soluzione a questo problema è di utilizzare la PCA per ridurre la dimensionalità dei dati e rimuovere la collinearità, prima di applicare il metodo dei Least Squares. In questo modo, si ottiene una matrice di proiezione che è invertibile, e si può trovare una soluzione stabile per $\tilde{x}$.

Consideriamo un dataset a due feature (quindi visualizzabile nel piano) che presentano forte collinearità, ad esempio $y = 2x+1  + \epsilon$ (collinearità perfetta al netto di $\epsilon$). Applicando PCA al dataset, otteniamo due componenti principali, di cui la prima cattura la maggior parte della varianza dei dati, e la seconda cattura la varianza residua. La prima componente principale rappresenta il manifold su cui i dati sono distribuiti, e la seconda componente principale rappresenta la direzione ortogonale al manifold. In questo modo, possiamo identificare gli outlier come i punti che sono lontani dalla prima componente principale (il manifold), e quindi hanno un alto grado di anomalia.



% - outlier detection

\subsection{Outlier Detection}

Ci sono vari approcci per l'identificazione di outlier, ma principalmente si basano su tre principi:
\begin{itemize}
   \item Distribuzione 
   \item Manifold (shape dei dati)
   \item Neighborhood (connettività e densità con vicini)
\end{itemize}

Basati su distribuzione abbiamo il test di Grubbs, che identifica outlier come punti che si discostano significativamente dalla distribuzione dei dati, e assume che i dati seguano una distribuzione Normale. Basati su manifold abbiamo il metodo dei Least Squares, che identifica outlier come punti che si discostano significativamente dalla struttura del manifold su cui i dati sono distribuiti. Basati su neighborhood abbiamo il Local Outlier Factor (LOF), che identifica outlier come punti che hanno una densità locale significativamente inferiore rispetto ai loro vicini.
Abbiamo anche gli Isolation Tree, discussi in un'altra domanda, che identificano outlier come punti che possono essere isolati facilmente in un albero di decisione, e quindi hanno un alto grado di anomalia.

\subsubsection{Connectivity and Concentration Neighborhood-based methods}

Nei metodi basati su \textbf{connettività}, si cerca di misurare quanto un punto è connesso ai suoi vicini.
Più è connesso, più è probabile che sia un punto normale, mentre se è poco connesso, è più probabile che sia un outlier. Un esempio di metodo basato su connettività è il Local Outlier Factor (LOF).

Si costruisce una \textit{posting matrix}, che descrive la connettività, non la \textit{densità}.
L'elemento $A_{ij}$ rappresenta l'indice del $j^{th}$ vicino di $i$. La matrice di connettività $A$ è una matrice di dimensione $n \times k$, dove $n$ è il numero di campioni e $k$ è il numero di vicini considerati. Ogni riga della matrice rappresenta un campione, e ogni colonna rappresenta l'indice del vicino corrispondente. La matrice di connettività viene utilizzata per calcolare la densità locale dei punti, che è una misura della connettività dei punti con i loro vicini, e quindi per identificare gli outlier.
\note{La matrice non è simmetrica, perché $A_{ij}$ rappresenta l'indice del $j^{th}$ vicino di $i$, e non necessariamente $A_{A_{ij}j} = i$. ``Io potrei essere il tuo primo vicino, ma tu potresti non essere il mio primo vicino''}

La posting matrix ci permette di avere tre possibili meccanismi di identificazione degli outlier:
\begin{itemize}
   \item \textbf{Hub} - un punto è un hub (inlier) se è almeno il $t^{th}$ vicino di almeno $k$ istanze
   \item \textbf{Popularity} - un punto è popolare (inlier) se è in media almeno il $t^{th}$ 
   \item \textbf{Ostracism} - un punto è un ostracismo (outlier) se è al massimo il $t^{th}$ vicino di al massimo $k$ istanze
\end{itemize}

Per usare invece \textbf{concentrazione} possiamo transire da una matrice di connettività a una matrice di distanza. È importante ordinare la matrice di distanza in modo che $D_{ij}$ rappresenti la distanza tra il punto $i$ e il suo $j^{th}$ vicino, e non la distanza tra il punto $i$ e il punto $j$. 
Dunque, ciascuna riga della matrice di distanza rappresenta le distanze del punto $i$ dai suoi vicini, ordinate in modo crescente. Trattata in questo modo, la matrice prende il nome di matrice di \textbf{gittata} $A_\gamma$, ed è costruita come segue.
\[
A_{\gamma} = \begin{bmatrix}
\gamma^1(x_1) & \gamma^2(x_1) & \gamma^3(x_1) \\
\gamma^1(x_2) & \gamma^2(x_2) & \gamma^3(x_2) \\
\gamma^1(x_3) & \gamma^2(x_3) & \gamma^3(x_3)
\end{bmatrix}
\]
Dove $\gamma^j(x_i)$ rappresenta la distanza tra il punto $x_i$ e il suo $j^{th}$ vicino. 

A questo punto, possiamo anche ottenere la gittata media definisce una concentrazione approssimativa.
Possiamo ottenere la gittata media al primo vicino, secondo vicino, terzo vicino, ecc. fino al $k^{th}$ vicino.
$\bar{\gamma}^k(x)$ è la media di $\gamma^1(x), \gamma^2(x), \ldots, \gamma^k(x)$, ovvero la distanza media del punto $x$ dai suoi primi $k$ vicini.
Costruiamo una matrice di concentrazione $A_{\bar{\gamma}}$, che rappresenta la distanza media dei punti dai loro vicini. La matrice di concentrazione $A_{\bar{\gamma}}$ è data da:

\[
A_{\gamma} \cdot \begin{bmatrix}
1 & \frac{1}{2} & \frac{1}{3} \\
0 & \frac{1}{2} & \frac{1}{3} \\
0 & 0 & \frac{1}{3}
\end{bmatrix} = \begin{bmatrix}
\bar{\gamma}^1(x_1) & \bar{\gamma}^2(x_1) & \bar{\gamma}^3(x_1) \\
\bar{\gamma}^1(x_2) & \bar{\gamma}^2(x_2) & \bar{\gamma}^3(x_2) \\
\bar{\gamma}^1(x_3) & \bar{\gamma}^2(x_3) & \bar{\gamma}^3(x_3)
\end{bmatrix}
= \begin{bmatrix}
\gamma^1(x_1) & \frac{1}{2}(\gamma^1(x_1) + \gamma^2(x_1)) & \frac{1}{3}(\gamma^1(x_1) + \gamma^2(x_1) + \gamma^3(x_1)) \\
\gamma^1(x_2) & ... & ... \\
... & ... & ... 
\end{bmatrix}
\]

Definiamo il \textbf{Reach Ratio Factor} come il rapporto fra la gittata media al $k^{th}$ vicino di due istanze $x$ e $y$:
\[
   \tilde{o}^k_{i,j} = \frac{\bar{\gamma}^k(x_i)}{\bar{\gamma}^k(x_j)}
\]

Da qui otteniamo il \textbf{Local Outlier Factor} (LOF), mediando il Reach Ratio Factor sui $k$ vicini di un'istanza.
	\[
		\tilde{o}(x_i) = \sum_{x_j \in neigh(x_i)} \tilde{o}^k_{i,j}
   \]

Il \textbf{Connectivity Outlier Factor} (COF) è una variante del LOF che costruisce ``catene'' di vicini, invece di considerare solo i vicini diretti. In questo modo, il COF tiene anche conto della connettività dei punti con i loro vicini. Dunque se un punto ha un vicino molto vicino, e gli altri relativamente lontani, il COF potrebbe essere più tollerante, se c'è una catena di vicini che lo collega a zone più dense, mentre il LOF potrebbe considerarlo un outlier a causa dei numerosi vicini lontani.
   \[
		\tilde{o}(x_i) = \sum_{x_j \in \textit{connect\_neigh}(x_i)} \tilde{o}^k_{i,j}
      \]

Il \textbf{k-NN Outlier Factor} (kNNOF) è un'altra variante del LOF che usa come misura la gittata al $k^{th}$ vicino, invece della gittata media. In questo modo, il kNNOF è più sensibile alla presenza di vicini molto lontani, e potrebbe identificare come outlier i punti che hanno un vicino molto lontano, anche se la maggior parte dei loro vicini è relativamente vicina.
   \[
   \tilde{o}(x_i) = \gamma^k(x_i)
   \]
\nl
\nl

Abbiamo anche approcci basati sulla concentrazione esplicita di vicini in un'area circostante un punto.
Possiamo considerare un raggio $\delta$ intorno a un punto $x$, e contare quanti vicini si trovano all'interno di questo raggio. Scegliamo anche un secondo raggio $\epsilon$, più grande o più piccolo di $\delta$, e contiamo quanti vicini si trovano all'interno di questo secondo raggio. 
Contare semplicimenente il numero di vicini è insufficiente, perché non possiamo stabilire arbitrariamente quale sia una concentrazione sufficiente di vicini all'interno di $\delta$ (o $\epsilon$) per considerare un punto come normale, e quale sia una concentrazione insufficiente per considerarlo un outlier.\\
Per questo motivo si fa una comparazione fra la concentrazione di un punto e la concentrazione dei suoi vicini.
\[
\tilde{o}(x_i) = \bar{c}^\varepsilon(B_i) - c^\varepsilon(x_i),
\quad \text{with} \quad
\bar{c}^\varepsilon(B_i) = \frac{\sum_{x_j \in B_i} c^\varepsilon(x_j)}{|B_i|}
\]

% Typically we define concentration as a ratio with $n$ the number of instances in the dataset but I guess we could also use $\max_i(|B_i|)$
Tipicamente si definisce la concentrazione come il rapporto fra il numero di vicini all'interno del raggio $\varepsilon$ e il numero totale di istanze nel dataset, ma si potrebbe anche usare il numero massimo di vicini all'interno di un raggio $\varepsilon$ (o $\delta$) per normalizzare la concentrazione.

\[
c^\varepsilon(x_i) = \frac{{x_j \in \mathcal{X} : d(x_i,x_j) \leq \varepsilon} - 1}{n-1}
\]





\subsection{Isolation Trees e Forests}
% Isolation tree e forests
% - Isolation Tree per outlier detection (chiederà anche gli altri immagino)
% - Isolation Tree / Isolation Forest: idea generale; come segmenta lo spazio d’ingresso; come definisce l’outlier score. (x5)
% - isolation forest
% - How do isolation forests segment the input space? How do they define the outlier degree of instances?
% - outlier detection using manifold, density (mixture models), neighborhood and isolation forest


L'Isolation Tree è un algoritmo di apprendimento automatico utilizzato per l'identificazione di outlier. L'idea alla base dell'Isolation Tree è che gli outlier sono più facili da isolare rispetto ai punti normali, poiché tendono ad essere più distanti dagli altri punti nel dataset.\\
L'Isolation Tree segmenta lo spazio d'ingresso in modo ricorsivo, scegliendo casualmente una feature e un valore di split per dividere i dati in due parti. Questo processo continua fino a quando ogni punto è isolato in una foglia dell'albero o finché non si raggiunge una profondità massima.\\
Se avessimo i dati in un piano, potremmo immaginare di tracciare linee verticali e orizzontali (o diagonali, se si fa una combinazione lineare delle feature $x$ e $y$, gli assi del piano, questo avviene nelle EIF (\textit{Extended Isolation Forests})) per segmentare lo spazio, e ogni volta che tracciamo una linea, stiamo isolando i punti. Gli outlier, essendo più distanti dagli altri punti, tendono ad essere isolati più rapidamente, ovvero con meno linee, rispetto ai punti normali.\\ 
Gli outlier tendono ad essere isolati più rapidamente rispetto ai punti normali, poiché sono più distanti dagli altri punti.

L'outlier score è definito in funzione della lunghezza del percorso da un nodo foglia alla root e della lunghezza media di un path nell'albero, o forse dalla soglia di massima profondità. Dato un nodo foglia $x_i$ (che rappresenta un punto del dataset) dell'albero con radice $t$ e $c = avg(path(j,t))$ abbiamo:
\[\tilde{o}^t = \frac{path(x_i,t)}{c}\]
Diversamente da altri casi, minore è il valore di $\tilde{o}^t$, più è probabile che $x_i$ sia un outlier, perché significa che è stato isolato più rapidamente rispetto alla media.

L'Isolation Forest è un ensemble di Isolation Tree, e l'outlier score di un punto $x_i$ è dato dalla media degli outlier score calcolati su ciascun albero dell'ensemble. L'idea è che, combinando più alberi, si ottiene una stima più robusta dell'outlier score, e si riduce la varianza del modello.
Formalmente,
\[
\tilde{o}(x_i) = 2 \frac{\sum_{t\in T}path(x_i,t)}{|T|c}
\]


\section{Clustering}

\subsection{K-Means Convergence}
% Convergenza K-Means
% - convergence proof of k-means
% - proof of convergence of kmeans
% - K-means: proof of convergence (dimostrazione di convergenza). (x4)

K-Means è un algoritmo di clustering che mira a partizionare un dataset in $k$ cluster, minimizzando la somma dei quadrati delle distanze tra i punti e i centroidi dei cluster. L'algoritmo iterativamente assegna i punti ai cluster più vicini e aggiorna i centroidi fino a quando non converge, ovvero quando le assegnazioni dei punti ai cluster non cambiano più o quando i centroidi non si spostano più.

Consideriamo la funzione di costo (o goodness, vista al contrario) di K-Means, che è data da:
\begin{align*}
   SSE(s,s_c) = \sum_{i=1}^n (d_i, s_c)^2\\
   G(C,s) = \sum_{c \in C} SSE(c,s_c)
\end{align*}
Con $s$ che rappresenta i centroidi dei cluster, $s_c$ che rappresenta il centroide del cluster $c$, e $d_i$ che rappresenta un punto del dataset. La funzione di costo $G(C,s)$ misura la somma dei quadrati delle distanze tra i punti e i centroidi dei cluster, e l'obiettivo di K-Means è minimizzare questa funzione di costo.

Più nel dettaglio, dato in input il numero di Cluster, l'algoritmo assegna casualmente $k$ centroidi scegliendo $k$ istanze casuali, e poi iterativamente esegue due passaggi:
\begin{enumerate}
   \item Fissati i centroidi $s$, ottimizziamo $C$ - assegnamento dei punti $d_i$ al cluster più vicino, ovvero al centroide più vicino $s_c$. Questo passaggio riduce la funzione di costo $G(C,s)$ rispetto a $C$, mantenendo fissi i centroidi $s$.
   \item Fissati i cluster $C$, ottimizziamo $s$ - aggiornamento dei centroidi $s_c$ come media dei punti assegnati al cluster $c$. Questo passaggio riduce la funzione di costo $G(C,s)$ rispetto a $s$, mantenendo fissi i cluster $C$.
\end{enumerate}
Entrambi i passaggi portano a una riduzione del costo $G(C,s)$, e poiché la funzione di costo è limitata inferiormente (non può essere negativa), l'algoritmo converge a un punto in cui non è più possibile ridurre ulteriormente il costo, ovvero quando le assegnazioni dei punti ai cluster non cambiano più o quando i centroidi non si spostano più (o si muovono di una quantità trascurabile).\\
È importante notare che K-Means può convergere a un minimo locale, e non necessariamente al minimo globale della funzione di costo. Per questo motivo, è comune eseguire K-Means più volte con inizializzazioni casuali diverse dei centroidi, e scegliere la soluzione con il costo più basso.

\subsubsection{What does it change if we use categorical attributes?}

% - K-means proof of convergence; what would change if using categorical attributes, and does it still converge if we use the version with categorical attributes?
% - Clustering/K-means con attributi categorici: come gestirli; impatto su SSE; convergenza nella versione con categoriche. (x2)
% - how are handled categorical attribute for clustering and how the SSE is affected
Se usiamo attributi categorici, dobbiamo definire una distanza appropriata per misurare la distanza tra i punti e i centroidi dei cluster. Una possibile scelta è la distanza di Jaccard, che misura la dissimilarità tra due insiemi di attributi categorici. 

La distanza di Jaccard è definita così:
\[d_j(A,B) = 1 - J(A,B) = 1 - \frac{|A \cap B|}{|A \cup B|}\]
\begin{align*}
   x_i = \{sport, musica, viaggi\} \\
   x_j = \{arte, musica\}\\
   d_j(x_i, x_j) = 1 - \frac{|\{musica\}|}{|\{sport, musica, viaggi, arte\}|} = 1 - \frac{1}{4} = \frac{3}{4}
\end{align*}
\begin{align*}
   A = \{\textit{colore = rosso, forma = rotonda, materiale = legno}\} \\
   B = \{\textit{colore = rosso, forma = quadrata, materiale = metallo}\} \\
   d_j(A,B) = 1 - \frac{|\{\textit{colore = rosso}\}|}{|\{\textit{colore = rosso, forma = rotonda, materiale = legno, forma = quadrata, materiale = metallo}\}|} = 1 - \frac{1}{5} = \frac{4}{5}
\end{align*}
\begin{align*}
   A = [1,0,1,0,0] \\
   B = [1,0,0,1,0] \\
   d_j(A,B) = 1 - \frac{|\{1\textit{ in comune}\}|}{|\{1\textit{ in totale}\}|} = 1 - \frac{1}{3} = \frac{2}{3}
\end{align*}

Jaccard tuttavia non ci permette di definire un centroide come media dei punti assegnati al cluster, perché non possiamo calcolare la media di attributi categorici. In questo caso, potremmo definire il centroide come l'istanza più rappresentativa del cluster, ovvero l'istanza che minimizza la distanza media dagli altri punti del cluster.
Questo algoritmo si chiama K-medoids, e converge anche lui, perché la funzione di costo è limitata inferiormente (non può essere negativa), e ogni iterazione riduce la funzione di costo, quindi l'algoritmo converge a un punto in cui non è più possibile ridurre ulteriormente il costo, ovvero quando le assegnazioni dei punti ai cluster non cambiano più o quando i centroidi non si spostano più (o si muovono di una quantità trascurabile).

N.B. La moda non funziona come minimizzazione della distanza, perché non è detto che la moda sia l'istanza più vicina agli altri punti del cluster, e quindi potrebbe non minimizzare la funzione di costo.
Considera questo esempio:
\begin{align*}
   x_i = \{sport, musica, viaggi, matematica\} \\
   x_k = \{arte, filosofia\}\\
   s_c = \{arte\}\\
   x_f = \{judo, musica, libri, storia,\} \\
   s_c^{new} = \{musica\}
\end{align*}
Supponendo che inizialmente la moda sia \textit{arte} e poi diventi \textit{musica}, è possibile che la distanza media di \textit{musica} dagli altri punti del cluster sia maggiore della distanza media di \textit{arte} dagli altri punti del cluster, e quindi la moda non minimizza la funzione di costo.
In particolare, avremmo 
\begin{align*}
   s_c = \{arte\} \Rightarrow \frac{1 + \frac{1}{2}}{2} = \frac{3}{4} \\
   s_c^{new} = \{musica\} \Rightarrow \frac{\frac{3}{4} + 1 + \frac{3}{4}}{3} = \frac{5}{6}
\end{align*}
Vediamo come la moda non abbia minimizzato la funzione di costo, perché la distanza media di \textit{musica} dagli altri punti del cluster è maggiore della distanza media di \textit{arte} dagli altri punti del cluster, e quindi la funzione di costo è aumentata invece di diminuire.

\subsection{Hierarchical Clustering}
% - hierarchical

Il clustering gerarchico è un metodo di clustering che costruisce una gerarchia di cluster, rappresentata da un albero chiamato dendrogramma. Esistono due approcci principali al clustering gerarchico: agglomerativo (bottom-up) e divisivo (top-down).\\
Nel clustering agglomerativo, si parte con ogni punto come un cluster separato, e si uniscono iterativamente i cluster più vicini fino a quando non rimane un solo cluster che contiene tutti i punti. Nel clustering divisivo, si parte con tutti i punti in un unico cluster, e si dividono iterativamente i cluster in sottogruppi più piccoli fino a quando ogni punto è in un cluster separato.

Per stabilire la vicinanza fra cluster ci sono vari metodi di linkage:
\begin{itemize}
   \item \textbf{Single linkage} - la distanza tra due cluster è definita come la distanza \textbf{minima} tra i punti dei due cluster. Questo metodo tende a creare cluster allungati e può essere sensibile ai rumori e agli outlier.
   \item \textbf{Complete linkage} - la distanza tra due cluster è definita come la distanza \textbf{massima} tra i punti dei due cluster. Questo metodo tende a creare cluster più compatti e può essere meno sensibile ai rumori e agli outlier rispetto al single linkage.
   \item \textbf{Average linkage} - la distanza tra due cluster è definita come la distanza \textbf{media} tra i punti dei due cluster. Questo metodo tende a creare cluster più equilibrati e può essere meno sensibile ai rumori e agli outlier rispetto al single linkage.
   \item \textbf{Ward's linkage} - la distanza tra due cluster è definita come l'aumento della somma dei quadrati delle distanze all'interno dei cluster che si verifica quando i due cluster vengono uniti. Questo metodo tende a creare cluster più compatti e può essere meno sensibile ai rumori e agli outlier rispetto al single linkage.
   \[
      d(c_i,c_j) = |c_i| \cdot |c_j| \cdot \frac{||\mu_i - \mu_j||^2}{|c_i| + |c_j|}
   \]
   Dove $|c_i|$ e $|c_j|$ sono le dimensioni dei cluster $c_i$ e $c_j$, e $\mu_i$ e $\mu_j$ sono i centroidi dei cluster $c_i$ e $c_j$. La distanza di Ward's linkage misura l'aumento della somma dei quadrati delle distanze all'interno dei cluster che si verifica quando i due cluster vengono uniti, e quindi tende a creare cluster più compatti.
   \item \textbf{Centroid linkage} - la distanza tra due cluster è definita come la distanza tra i centroidi dei due cluster. Questo metodo può essere sensibile ai rumori e agli outlier, poiché i centroidi possono essere influenzati da punti anomali.
\end{itemize}

Questo viene fatto usando una matrice di prossimità, che contiene le distanze tra i cluster nel dataset. La matrice di prossimità viene aggiornata ad ogni iterazione del processo di clustering, e viene utilizzata per determinare quali cluster unire o dividere in base al metodo di linkage scelto.
\nl

Per quanto riguarda il clustering \textbf{divisivo}, si inizia costruendo un MST (Minimum Spanning Tree) dei dati, e poi si rimuovono iterativamente gli archi più lunghi del MST per dividere i cluster in sottogruppi più piccoli, finché ogni punto è in un cluster separato. 
% Questo metodo è chiamato divisive hierarchical clustering based on MST, e può essere meno sensibile ai rumori e agli outlier rispetto al clustering agglomerativo, poiché i bordi più lunghi del MST tendono a collegare punti anomali o rumore.


\subsubsection{Categorical Features}
% - Hierarchical clustering: spiegare tutti i tipi di linkage visti; gestione di feature categoriche (es. distanza di Jaccard). (x3)
% - Hierarchical con feature categoriche (come fare con Jaccard distance)
In caso di feature categoriche non abbiamo numeri su cui calcolare le distanze, ma possiamo usare la distanza di Jaccard, che misura la dissimilarità tra due insiemi di attributi categorici. La distanza di Jaccard è definita come:
\[d_j(A,B) = 1 - J(A,B) = 1 - \frac{|A \cap B|}{|A \cup B|}\]
Dove $A$ e $B$ sono due insiemi di attributi categorici, $|A \cap B|$ è il numero di attributi in comune tra $A$ e $B$,e $|A \cup B|$ è il numero totale di attributi presenti in $A$ o in $B$. La distanza di Jaccard varia tra 0 e 1,dove 0 indica che i due insiemi sono identici, e 1 indica che i due insiemi non hanno attributi in comune. \\
Utilizzando la distanza di Jaccard come misura di distanza, non possiamo applicare tutti i metodi di linkage, ma solo \textit{single}, \textit{complete} e \textit{average}.\\
Infatti, per il \textbf{centroid} linkage abbiamo il problema di definire un centroide come media dei punti assegnati al cluster, perché non possiamo calcolare la media di attributi categorici. In questo caso, potremmo definire il centroide come l'istanza più rappresentativa del cluster, ovvero l'istanza che minimizza la distanza media dagli altri punti del cluster, ma questo forse non è sempre possibile, e quindi il centroid linkage potrebbe non essere applicabile in questo caso.\\
Per il \textbf{Ward's} linkage, invece, abbiamo il problema di definire l'aumento della somma dei quadrati delle distanze all'interno dei cluster che si verifica quando i due cluster vengono uniti, perché non possiamo calcolare la media dei punti assegnati al cluster, e quindi non possiamo calcolare la distanza tra i centroidi dei cluster. 



\section{Pattern Mining}


\subsection{Apriori principle and algorithm}
% - pattern mining: apriori algorithm
% - A Priori principle
% Apriori

Innanzitutto, un \textbf{itemset} è una collezione di uno o più item, e un itemset è \textbf{frequente} se la sua frequenza (o supporto) nel dataset è maggiore o uguale a una soglia minima di supporto.

Se il support count è denotato da $\sigma(X)$ (ovvero l'effettivo numero di occurrenze di $X$ nel dataset), allora il supporto di una regola di associazione $X \rightarrow Y$ è il seguente, con $T$ che rappresenta le transazioni nel dataset:
\[s = \frac{\sigma(X\cup Y)}{|T|}\]
La sua confidence invece è la proporzione delle transizioni che contengono $X$ che contengono anche $Y$ rispetto a tutte le transazioni che contengono $X$:
\[c = \frac{\sigma(X\cup Y)}{\sigma(X)}\]

L'obiettivo del association rule mining è di trovare tutte le regole tali che il loro supporto è maggiore o uguale a una soglia minima di supporto, e la loro confidence è maggiore o uguale a una soglia minima di confidence.

Enumerare tutte le regole di associazione è computazionalmente proibitivo, perché il numero di possibili regole cresce esponenzialmente con il numero di item nel dataset. 
L'approccio segue due step:
\begin{enumerate}
   \item Generazione degli itemset frequenti - si generano tutti gli itemset che hanno un supporto maggiore o uguale alla soglia minima di supporto.
   \item Generazione delle regole di associazione - si generano tutte le regole di associazione che hanno un supporto maggiore o uguale alla soglia minima di supporto e una confidence maggiore o uguale alla soglia minima di confidence, a partire dagli itemset frequenti
\end{enumerate}


\subsubsection{Antimonotonicity of support and confidence}
% - antimonotonia sia nel support che nella confidence
% - Apriori (frequent itemset mining / pattern mining): come funziona; Apriori principle; pruning; antimonotonicità (supporto e confidence). (x5)
% - Antimonotonicity of confidence

Per trovare efficientemente tutti gli itemset frequenti, si usa l'Apriori principle, che afferma che se un itemset è frequente, allora tutti i suoi sottoinsiemi sono frequenti. Questo ci permette di ridurre lo spazio di ricerca, perché possiamo escludere tutti gli itemset che contengono un itemset non frequente.
\[
\forall X,Y : (X\subseteq Y)
\]
Il supporto di un itemset $Y$ è sempre minore o uguale al supporto di un suo sottoinsieme $X$, perché ogni transazione che contiene $Y$ contiene anche $X$. Quindi, se $Y$ è frequente, allora $X$ deve essere frequente, e se $X$ non è frequente, allora $Y$ non può essere frequente.

Per la confidence è leggermente diverso, non c'è una relazione di antimonotonicità diretta: $c(ABC \rightarrow D)$ non è necessariamente maggiore o uguale a $c(AB \rightarrow D)$, perché la confidence dipende dal supporto di $X$ e $X \cup Y$, e non c'è una relazione di inclusione diretta tra questi due itemset. Tuttavia, se consideriamo regole generate dallo stesso itemset, allora possiamo osservare un'anti-monotonicità della confidence rispetto al numero di item sul lato destro della regola. Ad esempio, se ${A,B,C,D}$ è un frequent 4-itemset, allora possiamo generare le seguenti regole:
\[c(ABC \rightarrow D) \geq c(AB \rightarrow CD) \geq c(A \rightarrow BCD)\]
Infatti ricordando la formula della confidence
\[c(X \Rightarrow Y) = \frac{\sigma(X \cup Y)}{\sigma(X)}\]
Il numeratore $\sigma(X \cup Y)$ rimane lo stesso mentre il denominatore $\sigma(X)$ diminuisce man mano che spostiamo gli item dal lato destro al lato sinistro della regola, perché $\sigma(A) \geq \sigma(AB) \geq \sigma(ABC)$ secondo il principio di Apriori, portando a un aumento della confidence.


% In general, confidence does not have an antimonotone
% property, so $c(ABC \rightarrow D)$ can be larger or smaller than $c(AB \rightarrow D)$.
% However, confidence of rules generated from the same itemset has an anti-monotone property. 
% E.g., Suppose ${A,B,C,D}$ is a frequent 4-itemset:
% \[c(ABC \rightarrow D) \geq c(AB \rightarrow CD) \geq c(A \rightarrow BCD)\]
% Confidence is anti-monotone w.r.t. number of items on the right-hand-side (RHS) of the rule. This happens because, recalling the confidence formula 
% \[c(X \Rightarrow Y) = \frac{\sigma(X \cup Y)}{\sigma(X)}\]
% the numerator $\sigma(X \cup Y)$ remains the same while the denominator $\sigma(X)$ decreases as we move items from the RHS to the left-hand-side (LHS) of the rule, because $\sigma(A) \geq \sigma(AB) \geq \sigma(ABC)$ according to the Apriori principle, yielding an increase in confidence.



\subsubsection{Rule generation}
% - Rule generation: come si generano le regole da itemset frequenti. (x1)
% - rule generation
% - Merge/join step nel’algoritmo: come si fa il merge e tutti i particolari. (x1)

\begin{algorithm}[H]
\caption{Apriori Algorithm}
\begin{algorithmic}[1]
\State $k \gets 1$
\State $F_1 \gets \{\text{frequent 1-itemsets}\}$ \Comment{Scan database and count support of each item}
\Repeat
\State $L_{k+1} \gets$ \textbf{Candidate Generation} from $F_k$ \Comment{Generate candidate $(k+1)$-itemsets}
\State $L_{k+1} \gets$ \textbf{Candidate Pruning} of $L_{k+1}$ \Comment{Prune candidates with infrequent $k$-subsets}
\State Scan transaction database to count support of each candidate in $L_{k+1}$
\State $F_{k+1} \gets$ candidates in $L_{k+1}$ with support $\geq$ minsup
\State $k \gets k + 1$
\Until{$F_k = \emptyset$ or $|F_k| = 1$}
\State \Return $\bigcup_k F_k$ \Comment{Return all frequent itemsets}
\end{algorithmic}
\end{algorithm}

Si parte dall'insieme di frequent 1-itemsets, che si può trovare banalmente.
Al passo induttivo, si generano dei candidati $L_{k+1}$ con il metodo del prefisso $F_{k-1} \times F_{k-1}$ oppure un alternativo $F_{k-1} \times F_{k-1}$ (stessa sigla), poi si esegue un pruning step, in cui si eliminano tutti i candidati che contengono un sottoinsieme non frequente, e infine si conta il supporto dei candidati rimanenti per determinare quali sono effettivamente frequenti.
Se l'insieme di itemset frequenti $F_k$ è vuoto o contiene un singolo elemento, allora l'algoritmo termina, altrimenti si continua con il passo induttivo successivo.

Il metodo del prefisso $F_{k-1} \times F_{k-1}$ consiste nel generare candidati di lunghezza $k$ unendo coppie di itemset frequenti di lunghezza $k-1$ che condividono un prefisso comune di lunghezza $k-2$. Ad esempio, se $F_3$ contiene gli itemset $\{A,B,C\}$ e $\{A,B,D\}$, allora possiamo generare il candidato $\{A,B,C,D\}$ unendo questi due itemset, perché condividono il prefisso comune $\{A,B\}$.

Il metodo alternativo $F_{k-1} \times F_{k-1}$ consiste nel generare candidati di lunghezza $k$ unendo due itemset frequenti di lunghezza $k-1$ se gli ultimi $k-2$ item di uno sono uguali ai primi $k-2$ item dell'altro. Ad esempio, se $F_3$ contiene gli itemset $\{A,B,C\}$ e $\{B,C,D\}$, allora possiamo generare il candidato $\{A,B,C,D\}$ unendo questi due itemset, perché gli ultimi $k-2=1$ item di $\{A,B,C\}$ (ovvero $C$) sono uguali ai primi $k-2=1$ item di $\{B,C,D\}$ (ovvero $B$).


Per quanto riguarda il \textbf{pruning step}, si eliminano tutti i candidati che contengono un sottoinsieme non frequente, perché secondo il principio di Apriori, se un itemset è frequente, allora tutti i suoi sottoinsiemi devono essere frequenti. Quindi, se un candidato contiene un sottoinsieme che non è frequente, allora quel candidato non può essere frequente, e quindi viene eliminato dallo spazio di ricerca.

\begin{align*}
   F_3 = {ABC, ABD, ABE,ACD,BCD,BDE,CDE} \\
   L_4 = {ABCD, ACDE, ABDE, BCDE}
\end{align*}
{Per fare il pruning dei candidati in $L_4$ dobbiamo verificare se tutti i loro possibili sottoinsiemi di 3-item sono in $F_3$:\ns
\begin{itemize}
   \item $ABCD$ - tutti i sottoinsiemi sono frequenti - mantieni
   \item $ACDE$ - $ADE$ e $ACE$ non sono frequenti - elimina
   \item $ABDE$ - $ADE$ non è frequente - elimina
   \item $BCDE$ - $BCE$ non è frequente - elimina
\end{itemize}}


\subsubsection{Rule Generation}

Avendo ottenuto gli itemset frequenti, dobbiamo generare regole di associazione che soddisfino le soglie di supporto e confidence. 
Si costruisce un \textit{lattice} di regole a partire dagli itemset frequenti più lunghi, e da ciascuno di questi si generano regole spostando un item alla volta dal lato sinistro al lato destro della regola, e calcolando la confidence di ciascuna regola generata. Se la confidence di una regola è maggiore o uguale alla soglia minima di confidence, allora quella regola viene mantenuta, altrimenti viene eliminata dallo spazio di ricerca, insieme a tutte le regole ottenute spostando altri elementi da sinistra a destra.
Guardando il lattice in \autoref{fig:08/confidenceLattice} è più semplice da capire.
Partendo da $ABCD \Rightarrow {}$, possiamo generare le seguenti regole:
\begin{align*}
   ABC \Rightarrow D \\
   ABD \Rightarrow C \\
   ACD \Rightarrow B \\
   BCD \Rightarrow A
\end{align*}
Se ad esempio $c(ABC \Rightarrow D) < minconf$, allora eliminiamo questa regola, e tutte le regole che si ottengono spostando altri elementi da sinistra a destra, ovvero $AB \Rightarrow CD$, $AC \Rightarrow BD$, $BC \Rightarrow AD$, $A \Rightarrow BCD$, $B \Rightarrow ACD$, $C \Rightarrow ABD$ e $D \Rightarrow ABC$.
E così via.


\subsection{GSP algorithm}
% - GSP: come funziona
% - GSP (sequential pattern mining): come funziona; differenze rispetto ad Apriori; vincoli temporali (min-gap, max-gap, max-span) e impatto sul pruning. (x4)
% - GSP, differences compared to Apriori (e.g. in generating new sequences); time constraints (how GSP changes when they are applied and the reduction in pruning power)
% GSP

GSP (Generalized Sequential Pattern) è un algoritmo di mining di pattern sequenziali che mira a trovare tutte le sequenze frequenti in un dataset di sequenze, dove una sequenza è un insieme ordinato di itemset. GSP è un'estensione dell'Apriori algorithm, e condivide con esso il principio di Apriori, che afferma che se una sequenza è frequente, allora tutte le sue sottosequenze sono frequenti. Tuttavia, GSP introduce alcune differenze rispetto ad Apriori, principalmente nella generazione delle nuove sequenze e nella gestione dei vincoli temporali.

Intanto, una \ul{sequenza è una lista ordinata di elementi (transazioni/itemsets)}. Un elemento è un insieme di item (o ``eventi'') che occorrono simultaneamente a un timestamp $t$.\\
Una k-sequence contiene $k$ \textit{eventi} ($\neq$ elementi!).


% - Definizione formale di sottosequenza (subsequence). (x1)
% - "dimmi la definizione formale di sottosequenze"
È importante definire anche le sottosequenze.
Una sequenza $s_a = \langle a_1,a_2,...,a_n \rangle$ è una sottosequenza di $s_b = \langle b_1,b_2,...,b_m \rangle$ se esiste una sequenza di indici $1 \leq i_1 < i_2 < ... < i_n \leq m$ tale che $a_1 \subseteq b_{i_1}$, $a_2 \subseteq b_{i_2}$, ..., $a_n \subseteq b_{i_n}$. In altre parole, $s_a$ è una sottosequenza di $s_b$ se possiamo trovare gli elementi di $s_a$ in $s_b$ nell'ordine corretto, anche se non necessariamente consecutivi.


L'Apriori principle, adattato per le sequenze, si formula così:
\[
\forall S, S' : (S \subseteq S') \Rightarrow (support(S) \geq support(S'))
\]
Ovvero, se una sequenza $S'$ è frequente, allora tutte le sue sottosequenze $S$ devono essere frequenti, perché ogni sequenza che contiene $S'$ contiene anche $S$, e quindi il supporto di $S$ deve essere maggiore o uguale al supporto di $S'$.

L'algoritmo è strutturato così:
\begin{enumerate}
   \item Generazione delle sequenze frequenti di lunghezza 1 - si generano tutte le sequenze di lunghezza 1 che hanno un supporto maggiore o uguale alla soglia minima di supporto.
   \item Passo induttivo
   \begin{enumerate}
      \item Generazione Candidati - unisci coppie di sottosequenze frequenti di lunghezza $k-1$ che condividono un prefisso comune di lunghezza $k-2$ per generare candidati di lunghezza $k$.
      \item Pruning - elimina tutti i candidati che contengono una sottosequenza non frequente, perché secondo il principio di Apriori, se una sequenza è frequente, allora tutte le sue sottosequenze devono essere frequenti.
      \item Conta il supporto dei candidati rimanenti per determinare quali sono effettivamente frequenti.
      \item Elimina i candidati che non soddisfano la soglia minima di supporto.
   \end{enumerate}
\end{enumerate}

% - come si fa il merge e tutti i particolari dell'algoritmo
L'operazione di \textbf{merge} dei candidati è la più critica. 
\begin{itemize}
   \item Caso base ($k=2$): unisci coppie di sequenze frequenti di lunghezza 1 per generare candidati di lunghezza 2. Ad esempio, se $F_1$ contiene le sequenze $\{A\}$ e $\{B\}$, allora possiamo generare i candidati $\{A\}\{B\}$ e $\{B\}\{A\}$ unendo queste due sequenze, perché condividono il prefisso comune $\{\}$.\\
   Si può anche fare il merge di una sequenza con sé stessa, ad esempio $\{A\}\{A\}$.
   \item Passo induttivo ($k>2$): unisci coppie di sequenze $w_1$ e $w_2$ frequenti di lunghezza $k-1$ se la sottosequenza $w_1[2:k-1]$ è uguale alla sottosequenza $w_2[1:k-2]$, ovvero se rimuovendo il primo evento da $w_1$ otteniamo la stessa sequenza che si ottiene rimuovendo l'ultimo evento da $w_2$.\\ 
   Ad esempio, se $F_3$ contiene le sequenze $\{A\}\{B\}\{C\}$ e $\{B\}\{C\}\{D\}$, allora possiamo generare il candidato $\{A\}\{B\}\{C\}\{D\}$ unendo queste due sequenze, perché la sottosequenza $\{B\}\{C\}$ di $w_1$ è uguale alla sottosequenza $\{B\}\{C\}$ di $w_2$.

   Bisogna porre attenzione agli elementi.

   \begin{itemize}
      \item Se gli ultimi due eventi in $w_2$ appartengono allo stesso elemento, allora l'ultimo evento in $w_2$ diventa parte dell'ultimo elemento in $w_1$. Ad esempio, se $w_1 = \langle\{d\}\{a\}\{b\}\rangle$ e $w_2 = \langle\{a\}\{b,c\}\rangle$, allora possiamo generare il candidato $\langle\{d\}\{a\}\{b,c\}\rangle$ unendo queste due sequenze, perché se rimuoviamo il primo evento da $w_1$ otteniamo $\langle\{a\}\{b\}\rangle$, e se rimuoviamo l'ultimo evento da $w_2$ otteniamo $\langle\{a\}\{b\}\rangle$, che sono la stessa sequenza.\\
      \item Altrimenti, l'ultimo evento in $w_2$ diventa un elemento separato aggiunto alla fine di $w_1$. Ad esempio, se $w_1 = \langle\{a,d\}\{b\}\rangle$ e $w_2 = \langle\{d\}\{b\}\{c\}\rangle$, allora possiamo generare il candidato $\langle\{a,d\}\{b\}\{c\}\rangle$ unendo queste due sequenze, perché se rimuoviamo il primo evento da $w_1$ otteniamo $\langle\{d\}\{b\}\rangle$, e se rimuoviamo l'ultimo evento da $w_2$ otteniamo $\langle\{d\}\{b\}\rangle$, che sono la stessa sequenza.
      \item Quello che non possiamo fare è: se $w_1 = \langle\{d\}\{a,b\}\rangle$ e $w_2 = \langle\{a\}\{b,d\}\rangle$, non possiamo generare il candidato $\langle\{d\}\{a,b,d\}\rangle$ unendo queste due sequenze, perché se rimuoviamo il primo evento da $w_1$ otteniamo $\langle\{a,b\}\rangle$, e se rimuoviamo l'ultimo evento da $w_2$ otteniamo $\langle\{a\}\{b\}\rangle$, che non sono la stessa sequenza, seppur abbiano gli stessi eventi, perché la sequenza è definita in termini di elementi, non solo di eventi.
   \end{itemize}
\end{itemize}

\subsubsection{Time constraints}
% - GSP con vincoli temporali: min-gap, max-gap e max span. Cosa cambia con i vincoli temporali in GSP? (ultime slides lo dice)

Sulle transazioni è possibile porre dei vincoli temporali, come il min-gap, il max-gap e il max-span, che limitano le sequenze che possono essere considerate frequenti.
\begin{itemize}
   \item Il min-gap è il tempo minimo che deve intercorrere tra due eventi consecutivi in una sequenza. Ad esempio, se il min-gap è di 5 minuti, allora una sequenza come $\langle\{A\}\{B\}\rangle$ non sarebbe considerata frequente se gli eventi $A$ e $B$ si verificano a meno di 5 minuti di distanza l'uno dall'altro.
   \item Il max-gap è il tempo massimo che può intercorrere tra due eventi consecutivi in una sequenza. Ad esempio, se il max-gap è di 30 minuti, allora una sequenza come $\langle\{A\}\{B\}\rangle$ non sarebbe considerata frequente se gli eventi $A$ e $B$ si verificano a più di 30 minuti di distanza l'uno dall'altro.
   \item Il max-span è il tempo massimo che può intercorrere tra il primo e l'ultimo evento in una sequenza. Ad esempio, se il max-span è di 1 ora, allora una sequenza come $\langle\{A\}\{B\}\{C\}\rangle$ non sarebbe considerata frequente se gli eventi $A$ e $C$ si verificano a più di 1 ora di distanza l'uno dall'altro, anche se $A$ e $B$ e $B$ e $C$ si verificano a meno di 1 ora di distanza l'uno dall'altro.
\end{itemize}
Si pone un problema fondamentale. Nella fase di pruning consideriamo tutte le possibili sottosequenze dei candidati, ma se ci sono dei vincoli temporali, non tutte le sottosequenze di un candidato sono valide, perché potrebbero non soddisfare i vincoli temporali.\\
Per min-gap e max-span non c'è problema, perché rimuovere un evento da una sequenza non può che aumentare il gap fra gli eventi, o ridurre lo span della sequenza. Per \textit{max-gap} invece questo non è vero, perché rimuovere un evento da una sequenza può ridurre il gap fra gli eventi, e quindi potrebbe rendere una sottosequenza valida che altrimenti non sarebbe valida.\\
Per porre rimedio, si utilizza la nozione di \textbf{sottosequenze contigue}.
$s$ è una sottosequenza contigua di $w = \langle e_1 \rangle \langle e_2 \rangle \dots \langle e_k \rangle$ se una delle seguenti condizioni è soddisfatta:
\begin{itemize}
   \item $s$ è ottenuta da $w$ cancellando un evento da $e_1$ o da $e_k$. Ad esempio, se $w = \langle\{A\},\{B\}\{C\}\angle$, allora $s = \langle\{B\}\{C\}\rangle$ è una sottosequenza contigua di $w$, perché si ottiene cancellando l'evento $A$ da $e_1$.
   \item $s$ è ottenuta da $w$ cancellando un evento da un elemento $e_i$ che contiene almeno 2 eventi. Ad esempio, se $w = \langle\{A,B\}\{C\}\angle$, allora $s = \langle\{A\}\{C\}\rangle$ è una sottosequenza contigua di $w$, perché si ottiene cancellando l'evento $B$ da $e_1$.
   \item $s$ è una sottosequenza contigua di $s'$ e $s'$ è una sottosequenza contigua di $w$. 
\end{itemize}

\subsection{FP-Growth}
% FP-Growth
Possiamo costruire un trie per rappresentare tutte le transazioni del dataset, in cui ogni nodo rappresenta un item, e ogni percorso dalla radice a una foglia rappresenta una transazione. 

L'algoritmo FP-Growth utilizza una struttura dati chiamata FP-Tree (Frequent Pattern Tree) per rappresentare il dataset in modo compatto, e poi esegue una ricerca ricorsiva per trovare tutti gli itemset frequenti.
L'FP-Tree è costruito in due passaggi:
\begin{enumerate}
   \item Scansione del dataset per contare il supporto di ogni item, e per ordinare gli item in ordine decrescente di supporto (oppure lessicografico). Gli item che non soddisfano la soglia minima di supporto vengono eliminati.
   \note{Gli itemset, di base, non hanno un ordinamento fra gli item, dobbiamo imporlo noi.}
   \item Scansione del dataset per costruire l'FP-Tree, inserendo ogni transazione ordinata secondo l'ordinamento degli item stabilito nel passo precedente. Ogni nodo dell'FP-Tree rappresenta un item, e contiene un contatore che indica quante transazioni contengono quel nodo. Inoltre, ogni nodo contiene un puntatore al nodo successivo che rappresenta lo stesso item, in modo da poter facilmente accedere a tutte le transazioni che contengono un certo item.
\end{enumerate}

\framedt{Miglior ordinamento nell'itemset}{
   L'ordinamento degli item all'interno di un itemset è cruciale per l'efficienza dell'algoritmo FP-Growth. Gli itemset, di base, non hanno un ordinamento fra gli item, quindi dobbiamo imporlo noi. Un ordinamento comune (e il migliore) è quello decrescente in base al supporto degli item, in modo da massimizzare la condivisione dei prefissi nell'FP-Tree.\\
   Fare diversamente può portare ad avere più foglie.\\
   Consideriamo il caso estremo in cui abbiamo itemset della forma $\{A,Z\},\{B,Z\},\{C,Z\},\{D,Z\},\{E,Z\}\dots\{Y,Z\}$, con $Z$ l'item più frequente. 
   Se ordinassimo con ordine lessicografico (crescente) avremmo 25 rami, uno per lettera, ciascuno con un figlio $Z$. Invece, se ordiniamo gli item in ordine decrescente di supporto, otteniamo un FP-Tree con un solo ramo, in cui $Z$ è il nodo radice, e $A$, $B$, $C$, $D$ ,$E$, ecc. sono i suoi figli.
}


L'idea è di considerare tutti gli itemsets che che finiscono con un certo suffisso $X$, ad esempio $EDCBA$.
\begin{itemize}
   \item Per ogni possibile evento finale, consideriamo gli itemset che finiscono uno degli eventi che lo precedono nell'ordinamento
   \item e.g. per E, consideriamo tutti gli itemset che abbiano come item finale $D, C, B$ o $A$. In questo modo otteniamo tutti gli itemset che finiscono con $DE$, $CE$, $BE$ e $AE$. Per ognuno di questi itemset, otteniamo un FP-Tree condizionale, che rappresenta tutte le transazioni che contengono quell'itemset come suffisso. 
   \item Procediamo ricorsivamente su ciascuno di questi FP-Tree condizionali, considerando come suffisso l'itemset che stiamo analizzando, e così via fino a quando non otteniamo itemset frequenti di lunghezza 1.
\end{itemize}

\begin{algorithm}
\caption{FP-Growth Algorithm}
\begin{algorithmic}[1]
\State $\bar{X}$ = set di possibili suffissi nel FP-tree \footnote{Li otteniamo seguendo i puntatori della header table}
\For{each suffix $X$}
    \State \textbf{Fase 1:} Costruisci il trie per $X$ e calcola il supporto di $X$ usando i puntatori nell'header table
    \If{$X$ is frequent}
        \State \textbf{Fase 2:} Costruisci l'FP-tree \textit{condizionale} per $X$:
        \State \quad 1. Ricalcola il supporto
        \State \quad 2. Elimina gli item infrequenti
        \State \quad 3. Elimina le foglie e ricorsione
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Facendo un esempio discorsivo riprendendo il trie in \autoref{fig:08/trie2}.
Consideriamo il suffisso $E$. I percorsi, seguendo una visita bottom-up, che terminano in $E$ sono $ACDE$, $ADE$ e $BCE$.
A questo punto dobbiamo ricalcolare il supporto dei nodi che precedono $E$, contando solo le tre transazioni $ACDE$, $ADE$ e $BCE$. Possiamo eliminare i nodi foglia $E$ dall'albero.\\
Tutti gli item nei prefissi di $E$ hanno supporto pari a 2, ad eccezione di $B$ che ha supporto 1, quindi eliminiamo $B$.
A questo punto rimaniamo con i suffissi $DE$, $CE$, e $AE$.
\note{Si considera anche $AE$ ($BE$ no perché $B$ è infrequente) nonostantenon appaia come suffisso nelle tre transazioni che terminano in $E$, perché stiamo esplorando l'albero degli itemset possibili.}
Si considera prima $DE$, e ricordando che i nodi $E$ li avevamo già rimossi, possiamo ripetere il procedimento identico a prima ma usando $D$ invece di $E$. 
Innanzitutto rimuoviamo i nodi che non hanno $D$ nel proprio subtree.
Quindi, aggiorniamo il supporto dei nodi intermedi e rimuoviamo i nodi foglia $D$.
Rimane solo il nodo, quindi sappiamo che $ADE$ è un itemset frequente.\\
Si procede ugualmente per $CE$. 
Ripartiamo dal conditional subtree di $E$, e rimuoviamo i nodi che non hanno $C$ nel proprio subtree, ricalcoliamo supporto e rimuoviamo i nodi foglia $C$. Rimane solo il nodo radice \verb|null| quindi nessun suffisso $CE$ è frequente.\\
Infine, si considera $AE$.
Ripartiamo dal conditional subtree di $E$, e rimuoviamo i nodi che non hanno $A$ nel proprio subtree, ricalcoliamo supporto e rimuoviamo i nodi foglia $A$. Rimane solo il nodo radice \verb|null| quindi nessun suffisso $AE$ è frequente.

Notare che i sottoproblemi risolti a ogni iterazione sono disgiunti, perché ogni suffisso $X$ è considerato solo una volta, e quindi ogni itemset che termina con $X$ è considerato solo una volta, e quindi non c'è sovrapposizione tra i sottoproblemi risolti a ogni iterazione.

\subsubsection{Is there a better items ordering? Why?}
Come menzionato prima, il miglior ordinamento è quello su support decrescente, permette di avere meno nodi nell'albero, e quindi meno iterazioni.

\subsection{CORELS}
% - CORELS: nel search tree, relazione tra un nodo e i figli; riflesso sull’obiettivo di ottimizzazione. (x1)
% - In the CORELS search tree, what relationships exist between a node and its children? How does this reflect on the optimization objective defined by CORELS?
In CORELS (Certifiably Optimal RulE ListS) si modella una rule list come una tupla $R : (P_R,Y_R,y_R^0)$, rispettivamente premesse, output e default output.
Il search tree di CORELS è un prefix tree (trie) in cui ogni nodo rappresenta una regola da aggiungere alla rule list, e ogni percorso dalla radice a una foglia rappresenta una rule list completa.
Se una rule list $R$ è prefisso di una $R^+$, allora  $R \preceq R^+$, e quindi $R$ è più generale di $R^+$, perché $R$ ha meno regole di $R^+$, e quindi è più semplice, e quindi ha un costo minore secondo la funzione di costo definita da CORELS, che penalizza la complessità della rule list.

Il punto chiave è che l'errore ---rispetto a un dataset etichettato $(X,Y)$--- è composta da due termini:
\[
l(R,X,Y) = \underbrace{supported}{l_R(R,X,Y)} + \underbrace{default}{l_0(R,X,Y)}
\]
Aggiungere regole può solo aumentare $l_R$, non può diminuirlo. Possono invece ridurre $l_0$, perché aggiungere regole può ridurre il numero di istanze classificate con l'etichetta di default, e quindi ridurre il costo associato a queste istanze.\\
Quindi, se $R \preceq R^+$, allora $l_R(R,X,Y) \leq l_R(R^+,X,Y)$, perché $R$ è più generale di $R^+$.

La funzione di Loss $L$ definita da CORELS vuole considerare sia l'errore che la complessità della rule list, e quindi è definita come:
\[
L(R,X,Y) = \underbrace{errore}_{l(R,X,Y)} + \underbrace{regolarizzazione\\complessità}_{\lambda \cdot k^R}
\]
\[
   b(R,X,Y) = l_R(R,X,Y) + \lambda k^R \leq L(R,X,Y)\quad \text{Lower bound sulla loss per } R
\]

Per navigare e ridurre lo spazio di ricerca, CORELS considera il miglior modello attuale $R^C$ e la sua loss $L^C$, e anche una lista ottimale $R^*$ e la sua loss $L^*$.\\
Vengono eliminati nodi che hanno un bound $b$ superiore a $b^C$, perché non possono portare a una rule list migliore di $R^C$.\\
Inoltre, voglio tagliare anche i nodi con liste troppo lunghe: per un massimo di $\bar{r}$ regole, abbiamo la seguende relazione fra la lunghezza di $R^*$ e $R^C$ (l'attuale ottimo):
\[k^{R^*} \leq min(\frac{L^C}{\lambda}, \bar{r})\]
Questo ci permette di eliminare tutti i nodi a profondità maggiore di $max(\frac{L^C}{\lambda}, \bar{r})$, perché non possono portare a una rule list migliore di $R^C$.\\
Si può anche eseguire pruning in base a un'accuracy minima che deve avere una nuova regola.



\section{Classification}

\subsection{Bagging e Boosting}
% Bagging boosting 
% - Boosting e bagging. (x2)
\subsubsection{Bagging}
Il bagging è un metodo di ensemble learning che consiste nel creare più modelli di apprendimento automatico indipendenti, addestrati su campioni casuali del dataset originale (con sostituzione), e poi combinare le loro previsioni per ottenere una previsione finale più robusta. Il bagging è particolarmente efficace nel ridurre la varianza del modello, e può migliorare le prestazioni di modelli instabili come gli alberi decisionali.\\
L'idea di fondo è che la varianza di più modelli indipendenti sia minore della varianza di un singolo modello.
Il bagging funziona bene con modelli ad alta varianza e basso bias, come i Decision Tree. ``Bias'' si riferisce a quanto il modello è ``rigido'', se è ad alto bias è un modello semplice, se è a basso bias, ha una maggiore capacità  di catturare la complessità dei dati. ``Varianza'', invece si riferisce alla sensibilità del modello ai cambiamenti nei dati di addestramento. I modelli ad alta varianza tendono a sovradattarsi ai dati di addestramento, mentre i modelli ad alto bias tendono a sottodattarsi. Il bagging aiuta a ridurre la varianza combinando più modelli, e quindi può migliorare le prestazioni di modelli instabili come i Decision Tree.

Il campionamento con rimpiazzo (bootstrap) è importante perché consente di creare modelli indipendenti, addestrati su campioni diversi del dataset originale. Se si utilizzasse un campionamento senza rimpiazzo, i modelli sarebbero addestrati su campioni simili del dataset, e quindi  meno indipendenti, riducendo l'efficacia del bagging nel ridurre la varianza.

Nelle isolation forest si applica bagging sia sulle feature che sulle istanze. Ogni albero dell'ensemble è addestrato su un campione casuale del dataset originale, e su un sottoinsieme casuale delle feature. Ogni albero sarà capace di catturare la complessità dei dati, e sarà anche troppo ``rumoroso'', eccessivamente adattato ai dati di addestramento, ma combinando più alberi, si ottiene una stima più robusta, riducendo la rumorosità ma mantenendo la specificità di ciascun albero, e quindi migliorando le prestazioni complessive dell'ensemble.

\subsubsection{Boosting}
Boosting crea delle ``soft/fuzzy'' bag. Invece di creare modelli indipendenti, il boosting crea una sequenza di modelli dipendenti, in cui ogni modello successivo cerca di correggere gli errori del modello precedente. In questo senso, i dati vengono sempre usati tutti, ma a ciascuna iterazione con pesi diversi.
Il boosting è particolarmente efficace nel ridurre il bias del modello, e può migliorare le prestazioni di modelli stabili come i Decision Tree.

Il boosting può essere interpretato nello spazio delle funzioni come un gradient descent, in cui ogni modello successivo cerca di minimizzare la funzione di perdita del modello precedente, seguendo il gradiente negativo della funzione di perdita. 
Essendo lo spazio delle funzioni non necessariamente continuo, e non necessariamente differenziabile, il boosting utilizza una stima del gradiente, basata sui residui del modello precedente, per aggiornare i pesi dei dati e addestrare il modello successivo, in modo che sia il più vicino possibile alla direzione ``pura'' del gradiente negativo della funzione di perdita.

È importante il learning step $\eta_t$ che controlla quanto ci spostiamo nella direzione del gradiente negativo, e quindi quanto il modello successivo cerca di correggere gli errori del modello precedente. Se $\eta_t$ è troppo grande, potremmo sovradattare i dati di addestramento all'iterazione $t$, mentre se è troppo piccolo, potremmo non correggere sufficientemente gli errori del modello precedente, e quindi non migliorare le prestazioni complessive dell'ensemble. 

Lo spazio delle funzioni può essere qualunque modello, lo spazio dei Decision Trees, della Regressione Lineare, ecc\dots

\subsection{Decision Trees}

% - Decision Tree: come scegliere lo split (threshold o probability). (x1)
% - how to split for DT (threshold or probability)

\subsubsection{Threshold or Probability?}



\section{Time Series}

\subsection{Shapelet extraction and classification}
% - classificazione con shapelet: come si fa
% - estrazione degli shapelets
% Shapelet
% - Shapelets: classificazione; estrazione degli shapelets; (se richiesto) uso di istogrammi. (x4)

L'idea degli shapelet è di rappresentare una Time Series come un vettore di distanze da un insieme di sottosequenze rappresentative, chiamate shapelets.
Gli shapelet hanno un potere descrittivo e disciminativo, ovvero essere descrivere bene una classe ed eventualmente distinguerla dalle altre. Sono particolarmente interessanti e robusti perché catturano feature locali.
L'estrazione degli shapelet è un processo computazionalmente costoso, perché richiede di considerare tutte le possibili sottosequenze di tutte le Time Series del dataset, e di calcolare la distanza di ciascuna sottosequenza da tutte le Time Series del dataset, per poi selezionare quelle che hanno il potere descrittivo e discriminativo più elevato.\\
Per ridurre la complessità computazionale, si possono utilizzare degli istogrammi per rappresentare la distribuzione delle distanze di ciascuna sottosequenza dalle Time Series del dataset, e poi selezionare solo le sottosequenze che hanno una distribuzione di distanze che è significativamente diversa tra le classi. In questo modo, si riduce il numero di sottosequenze da considerare, e si aumenta la probabilità di selezionare shapelet che sono realmente descrittivi e discriminativi.

L'algoritmo, a macrostep è così composto:
\begin{enumerate}
   \item Generazione candidati di dimensioni \verb|(MINLEN,MAXLEN)| a partire dal dataset D
   \item Per ogni candidato, si ottiene il \textit{gain} associato a quel candidato, che misura quanto quel candidato è descrittivo e discriminativo rispetto alle classi del dataset. Il gain è calcolato come la differenza tra l'entropia della distribuzione delle classi prima di considerare il candidato, e l'entropia della distribuzione delle classi dopo aver considerato il candidato, pesata per la proporzione di Time Series che sono vicine al candidato (ovvero, che hanno una distanza inferiore a una certa soglia).\\
   \item Si seleziona il candidato con gain più elevato e si aggiunge alla lista degli shapelet selezionati.
\end{enumerate}

\subsubsection{How are Histogram used in Shapelet classification?}
La generazione di candidati si fa facendo scorrere una finestra di size $l$ (inizialmente pari a \verb|MAXLEN| e poi fatta decrescere a ogni iterazione fino a \verb|MINLEN|) su tutte le Time Series del dataset, e considerando tutte le sottosequenze di size $l$ come candidati shapelet, aggiungendoli a un pool di candidati.\\
A questo punto entrano in gioco gli istogrammi.
Per ogni candidato $S$, viene eseguita la funzione \verb|CheckCandidate(D,S)|, la quale per ogni time series $T$ in $D$ ottiene la distanza $dist$ fra $T$ e $S$, e inserisce $T$ (non $dist$!!!)in un istogramma usando come chiave $dist$.
Fatto questo, \verb|CheckCandidate| calcola l'information gain usando l'istogramma così costruito.\\
\verb|CalculateInformationGain(hist)| innanzitutto trova fra le chiavi $dist$ dell'istogramma un punto di split ottimale $split_dist$ che massimizzi l'information gain (come lo faccia è a noi oscuro, userà la sfera di cristallo, o calcola il gain per tutti i possibili split (tutte le chiavi dell'istogramma)\dots).
Dopodiché itera sui bin dell'istogramma, e tutte le TS a sinistra di $split_dist$ finiscono in un pool $D_1$ e tutte le TS a destra di $split_dist$ finiscono in un pool $D_2$. Si calcola l'entropia di $D_1$ e $D_2$, e si restituisce la differenza tra l'entropia di $D$ e la media pesata delle entropie di $D_1$ e $D_2$.

\subsubsection{Possibili ottimizzazioni}
Ci sono due possibili ottimizzazioni.
\begin{itemize}
   \item \textbf{Distance early abandoning}: ridurre la computazione della distanza fra una TS e un candidato shapelet. Si cerca di sovrapporre uno shapelet a una time series in cerca della migliore matching location per avere la distanza minima. Teniamo traccia della distanza minima trovata finora, e se durante il processo di sovrapposizione la distanza corrente supera la distanza minima, possiamo abbandonare il calcolo della distanza per quella TS, perché sappiamo che non sarà mai migliore della distanza minima trovata finora.
   
   Per capirlo meglio, dobbiamo ricordarci che $SubsequenceDist(T,S)$
   

   \item \textbf{Admissible Entropy Pruning}:
   Non è necessario calcolare interamente l'information gain per tutti i candidati shapelet. Si può fare un calcolo parziale dell'entropia mantentendo un upper bound (i.e. l'entropia avendo assegnato a SX e DX 26 TS su 72 vale X, se tutte le rimanenti si distribuissero nel miglior modo possibile avrei entropia Y), e se questo upper bound è inferiore al gain del miglior shapelet trovato finora, allora possiamo abbandonare il calcolo dell'information gain per quel candidato shapelet.
\end{itemize}


% - classification with shapelets (say that histogram are used, they want it to be said)
\subsection{Motif extraction}
% - come si trovano i motif
% - Motif discovery (time series): come si trovano; algoritmo di estrazione; differenza con shapelet. (x3)
% - differenza tra shaplet e motif e algoritmo per estrarre i motif

I motif sono sottosequenze che appaiono frequentemente in un dataset di Time Series, e che rappresentano pattern ricorrenti nei dati. A differenza degli shapelet, i motif non sono necessariamente descrittivi o discriminativi rispetto alle classi del dataset, ma sono semplicemente pattern ricorrenti che possono essere utili per l'analisi dei dati, ad esempio per identificare eventi comuni o per segmentare le Time Series in parti significative.

I Motif li troviamo con il Matrix Profile, che è una vettore che contiene la distanza di ciascuna possibile sottosequenza di una Time Series dalla sottosequenza più vicina (della stessa Time Series). I motif sono identificati come coppie di sottosequenze che hanno una distanza minima nella Matrix Profile, ovvero sono le coppie di sottosequenze più simili tra loro.

Per trovarli serve in input una lunghezza $m$ desiderata per i motif.
Si estraggono tutte le sottosequenze di lunghezza $m$ di una time series con una sliding window, e poi si calcolano le distanze a coppie fra tali sottosequenze, e per ciascuna sottosequenza si mantiene solo la distanza minima dalla sua vicina più prossima.
Il valore $i-esimo$ è la distanza fra la sottosequenza $T_i$ di $T$ e la sua vicina più prossima, ovvero la sottosequenza di $T$ che è più simile a $T_i$ (escludendo se stessa).\\
Notare che questa relazione non è simmetrica, ovvero se $T_j$ è la vicina più prossima di $T_i$, non è detto che $T_i$ sia la vicina più prossima di $T_j$.
Il MP ci permette di trovare il vicino più prossimo di ciascuna sottosequenza in tempo costante.

\subsubsection{Come si calcola il Matrix Profile?}

\subsubsection{Motif vs Shapelets}
% TimeSeries Motif discovery
\subsection{Fourier vs Wavelet representation}
% Fourier e wavelets
% - Fourier e wavelet representation: Fourier (phasor → ampiezza) e wavelet (rappresentazione multi-risoluzione). (x3)
\subsubsection{How does a phasor define amplitude?}
% - In Fourier analysis, how does a phasor define amplitude?
\subsection{Dynamic Time Warping}
% - Dynamic Time Warping (DTW): uso per shapelet/classification; invarianza a time shifts e come limitarla a local shifts; definizione di “warp” (casi di allineamento). (x3)
% - DTW come utilizzarlo per shapelet e classification
\subsubsection{Invariance of DTW to time shifts}
% - Dynamic Time Warping is invariant to time shifts. In order to allow invariances to local shifts only, which solution can we apply?


\section{XAI}
% - SHAP, LIME, LORE. (x1)
% - SHAP, LIME, LORE

\subsection{SHAP}

\subsection{LIME}

\subsection{LORE}



% Francesco, [28/01/2026 11:49]
% domande DM (tra stamani e leggendo quello di altre persone quest'anno e anni passati, ordine sparso):
% - Esercizi non ne chiede quest'anno sembra, ma saper fare quelli sulle slides aiuta, consigliato

% Giovanni
% Riguardando nel gruppo a me chiese:
% Ricordo che anche se dovevo avere l’orale semplificato perché avevo fatto il paper la Monreale non aveva avvertito il Setzu della cosa e quindi ci fece comunque l’orale completo





% Gruppo DM
% Convergenza K-Means
% Leonardo Cozzolino, [28/01/2026 12:01]
% - Esercizi delle slide: saperli fare (anche se “quest’anno sembra non li chieda”). (x1)

% MONREALE:

% SETZU:
% SCRITTO:
