\chapter{Time Series}


\section{Classification}

Given a set $X$ of $n$ time series, $X = {x_1, x_2, \dots, x_n}$, each time series has m ordered values $x_i = \langle x_{t1}, x_{t2}, \dots, x_tm \rangle$ and a class value $c_i$.

The objective is to find a function $f$ that maps from the space of possible time series to the space of possible class values.

Generally, it is assumed that all the TS have the same length m.

\subsection{Shapelet-based classification}
\begin{paracol}{2}
   
   \begin{definition}
      [Shapelet-based classification]
   
      First represent a TS as a vector of distances with representative subsequences, namely \textbf{shapelets}.
      Then, use it as input for machine
      learning classifiers.
   
   \end{definition}

   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/17/shapelet0.png}
      % \caption{}
      \label{fig:17/shapelet0}
   \end{figure}
\end{paracol}


\begin{figure}[htbp]
   \centering
   \includegraphics{images/17/shapelet1.png}
   \caption{Shapelet application example}
   \label{fig:17/shapelet1}
\end{figure}


When it comes to time series, \ul{\textbf{shapelets} are \textit{TS subsequences} which are
maximally representative} of a class.\\
Shapelets can provide interpretable results,which may help domain practitioners betterunderstand their data.\\
Furthermore, shapelets can be significantly more accurate/robust because they are local features, whereas most other state-of-the-art TS classifiers consider global features.

% Finding shapelets


% finding candidates


\begin{algorithm}
\caption{GenerateCandidates(\textbf{dataset} $D$, $MAXLEN$, $MINLEN$)}
\begin{algorithmic}[1]
   \State $pool \gets \varnothing$
   \State $l \gets MAXLEN$
   \While{$l \ge MINLEN$}
      \For{each time series $T$ in $D$}
         \State $pool \gets pool \cup S_T^{l}$ \Comment{add all subsequences of length $l$ from $T$}
      \EndFor
      \State $l \gets l - 1$
   \EndWhile
   \State \Return $pool$
\end{algorithmic}
\label{alg:generatecandidates}
\end{algorithm}

\begin{algorithm}
\caption{FindingShapeletBF(\textbf{dataset} $D$, $MAXLEN$, $MINLEN$)}
\begin{algorithmic}[1]
   \State $candidates \gets \text{GenerateCandidates}(D, MAXLEN, MINLEN)$
   \State $bsf\_gain \gets 0$
   \State $bsf\_shapelet \gets \text{null}$
   \For{each shapelet $S$ in $candidates$}
      \State $gain \gets \text{CheckCandidate}(D, S)$
      \If{$gain > bsf\_gain$}
         \State $bsf\_gain \gets gain$
         \State $bsf\_shapelet \gets S$
      \EndIf
   \EndFor
   \State \Return $bsf\_shapelet$
\end{algorithmic}
\label{alg:findingshapeletbf}
\end{algorithm}


\subsection{Information Gain issues}
\begin{algorithm}
\caption{CalculateInformationGain(\textbf{distance histogram} $obj\_hist$)}
\begin{algorithmic}[1]
   \State $split\_dist \gets \text{OptimalSplitPoint}(obj\_hist)$
   \State $D_1 \gets \varnothing,\quad D_2 \gets \varnothing$
   \For{each entry $d$ in $obj\_hist$}
      \If{$d.dist \le split\_dist$}
         \State $D_1 \gets D_1 \cup d.objects$
      \Else
         \State $D_2 \gets D_2 \cup d.objects$
      \EndIf
   \EndFor
   \State \Return $I(D) - \hat{I}(D)$ \Comment{return information gain after split}
\end{algorithmic}
\label{alg:calculateinformationgain}
\end{algorithm}

% check candidates


Distance from the TS to the subsequence SubsequenceDist(T, S) is a distance
function that takes time series T and subsequence S as inputs and returns a non-
negative value d, which is the distance from T to S.

% Algorithms from slides: CheckCandidate, GenerateCandidates


% \begin{algorithm}
% \caption{CheckCandidateDetailed(\textbf{dataset} $D$, \textbf{shapelet candidate} $S$)}
% \begin{algorithmic}[1]
%    \State $objects\_histogram \gets \varnothing$
%    \For{each $T$ in $D$}
%       \State $dist \gets \text{SubsequenceDist}(T, S)$
%       \State // group objects by the distance value
%       \State insert $T$ into $objects\_histogram$ by the key $dist$
%    \EndFor
%    \State \Return $\text{CalculateInformationGain}(objects\_histogram)$
% \end{algorithmic}
% \label{alg:checkcandidate-detailed}
% \end{algorithm}

The idea behind information gain is: given a splitting rule $sp$ dividing the dataset in two parts $D_1$ and $D_2$, we want to measure how much uncertainty in the class labels is reduced by the split, i.e., how much more ``pure'' the two resulting datasets are compared to the original one.
We use entropy $I(D)$ to measure uncertainty in the class labels of dataset $D$.
The information gain is then computed as the difference between the entropy of the original dataset and the weighted average entropy of the two resulting datasets:
\[\text{InformationGain}(D, D_1, D_2) = I(D) - \hat{I}(D)\]
where
\[\hat{I}(D) = \frac{|D_1|}{|D|} I(D_1) + \frac{|D_2|}{|D|} I(D_2)\]
We may use the distance from T to S as a splitting rule: all time series with distance less than or equal to a threshold $d_{th}$ go to $D_1$, and the rest go to $D_2$.

The problem is that we have many candidates, and for each candidate we need to compute the distance from each time series sample to the candidate, which is expensive.
\[
\#{\textit{candidates}} = \sum_{l=MINLEN}^{MAXLEN}\sum_{T_i \in D} (|T_i| - l + 1) 
\]
That is, having $200$ instances of length $275$ each, we have about $7.480.200$ candidates for some $MINLEN = \dots$ and $MAXLEN = 200$.
This results in a very high computational cost.

{There are two main optimizations:\ns
\begin{enumerate}
   \begin{paracol}{2}
      
      \begin{figure}[htbp]
         \centering
         \includegraphics[width=0.98\columnwidth]{images/17/earlyabandon.png}
         \caption{Early abandoning}
         \label{fig:17/earlyabandon}
      \end{figure}
      \switchcolumn
      \colfill
      \item Early abandoning distance calculations\\
      If we know that the current best minimum distance found so far is $d_{best}$, and while computing the distance between a time series $T$ and a candidate shapelet $S$ we find that the partial distance already exceeds $d_{best}$, we can abandon the computation early.
      This can save a lot of computation time, especially when the shapelet is not similar to
      the time series.
      \colfill
   \end{paracol}
   % \newpage
   \begin{paracol}{2}
      \begin{figure}[htbp]
         \centering
         \includegraphics[width=0.98\columnwidth]{images/17/entropybound.png}
         \caption{Admissible Entropy pruning}
         \label{fig:17/entropybound}
      \end{figure}
      \switchcolumn
      \colfill
      \item Pruning candidates based on an upper bound on information gain\\
      Given a candidate shapelet $S$, we can compute an upper bound on the information gain that can be achieved by splitting the dataset based on distances to $S$.\\
      If this upper bound is less than the best information gain found so far, we can prune $S$ without computing the actual information gain.
      
      So, in other words, for a candidate shapelet $S$, we do not necessarily need to calculate the distance for each training sample.
      After calculating some training samples, if the upper bound of information gain (corresponding to the optimistic scenario) \textit{is less} than the best candidate shapelet, we stop calculation for that candidate and try the next one, since there is no way it can be a better fit than the current best.
      \colfill
   \end{paracol}
      
   \end{enumerate}
}



\section{Motif}

\textbf{Motif} discovery essentially is finding repeated patterns, i.e., pattern mining. That is
\coolquote{
   Are there any repeated patterns, of length m in the TS?
}{}
\begin{figure}[htbp]
   \centering
   \includegraphics{images/17/motif0.png}
   \caption{Repeated patterns example}
   \label{fig:17/motif0}
\end{figure}


There are several reasons why motif discovery is useful:
\begin{itemize}
	\item Mining \textbf{association rules} in TS requires the discovery of motifs. These are referred to as \textit{primitive shapes} and \textit{frequent patterns}.
	\item Several \textbf{TS classifiers} work by constructing typical \textit{prototypes} of each class. These prototypes may be considered motifs.
	\item Many \textbf{TS anomaly detection} algorithms consist of modeling \textit{normal behavior} with a set of typical shapes (which we see as motifs), and detecting future patterns that are dissimilar to all typical shapes.
\end{itemize}

\subsection{Algorithm}

The Matrix Profile (MP) is a data structure that annotates a TS and
can be exploited for many purposes, e.g. efficient Motif Discovery.

Given a time series, T and a desired subsequence length, $m$.

Note that $m$ is \ul{fixed and decided apriori}. 
The algorithm may be applied multiple times with multiple $m$ values to find the best motifs.


\begin{figure}[htbp]
   \centering
   \includegraphics{images/17/matrixProfile.png}
   \caption{Finding subsequences of length $m$, along with distance matrix}
   \label{fig:17/matrixProfile}
\end{figure}

\begin{paracol}{2}
   
   For each subsequence we keep only the distance with the closest \textbf{nearest neighbor}.
   The \textbf{distance} to the corresponding nearest neighbor of each subsequence can be stored in a vector called \textbf{matrix profile} $P$.\\
   The matrix profile value at location $i$ is the distance between $T_i$ and its nearest neighbor

   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/17/matrixNeighbors.png}
      \caption{Finding the closest neighbor for each pattern}
      \label{fig:17/matrixNeighbors}
   \end{figure}

\end{paracol}


\begin{figure}[htbp]
   \centering
   \includegraphics{images/17/matrixProfileIndex.png}
   \caption{Matrix profile \textbf{index}}
   \label{fig:17/matrixProfileIndex}
   The index of corresponding nearest neighbor of each
   subsequence is also stored in a vector called matrix profile
   index.
\end{figure}


The MP index allows to find the nearest neighbor to any subsequence in constant
time.\\
Note that the pointers in the matrix profile index are not necessarily symmetric.
If A points to B, then B \textit{may or may not} point to A.

The classic TS motif: the two smallest values in the MP must have the same value,
and their pointers must be mutual.

\begin{figure}[htbp]
   \centering
   \includegraphics{images/17/matrixProfile1.png}
   \caption{Matrix Profile and what its index represents}
   \label{fig:17/matrixProfile1}
\end{figure}

\subsection{Reading the Matrix Profile}
\begin{itemize}
	\item For relatively low values, you know that the subsequence in the original TS must have (at least one) relatively similar subsequence elsewhere in the data (such regions are “motifs”)
	\item For relatively high values, you know that the subsequence in the original TS must be unique in its shape (such areas are anomalies).
\end{itemize}

\begin{figure}[htbp]
   \centering
   \includegraphics{images/17/readingMP.png}
   \caption{Reading the matrix profile}
   \label{fig:17/readingMP}
\end{figure}


\subsection{Computing the Matrix Profile}

% // TODO computing the matrix profile





