\chapter{Data Representation}

\begin{itemize}
	\item By \textbf{correlation}\\ 
   I want to represent data according to the correlation of the dataset\\
Algorithm: \texttt{PCA}
	\item By \textbf{neighborhood}\\ I want to represent the data so that similar instances are similar\\
Algorithm: \texttt{t-SNE}
	\item By \textbf{manifold}\\ 
   I want to represent the data so that its manifold is preserved\\
Algorithm: \texttt{UMAP}
\end{itemize}


\section{Principal Component Analysis (PCA)}

PCA is a linear dimensionality reduction technique
that transforms the data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

Essentially, PCA exploits spectral decomposition of the whole dataset to find a new basis for the data.

Data can often be correlated, and linear dependencies can exist among variables,
e.g.,
\begin{itemize}
	\item Rent is linearly dependent on salary and food expenses
	\item Bank deposit is linearly dependent on salary and work
	\item Cardio is linearly dependent on $VO_2 max$
\end{itemize}

Vectors are $m$-dimensional elements in a field, and enjoy both addition and multiplication by scalar.\\ Composing these two, we can generate an infinite number of vectors: this is a \textbf{vector space}, and is defined by the basis vectors involved in the composition.

\begin{figure}[htbp]
   \centering
   \includegraphics{images/02/span.png}
   \caption{Vector space spanned by two vectors}
   \label{fig:02/span}
\end{figure}

A matrix $A$ defines a space\dots and thus a linear
transformation! $Av$ linearly combines the columns of
with coefficients given by $v$.

\begin{paracol}{2}
   
   Eigenpairs $(\lambda, v)$ of a square matrix $A$ are defined by the equation $Av = \lambda v$.
   
   The eigenvectors $v_1,\dots,v_m$ of a matrix $A$ define the stretching of the space, and
   their eigenvalues $\lambda_1 > \dots > \lambda_m$ define the stretching factor.

   \switchcolumn

   \begin{figure}[htbp]
      \centering
      \includegraphics{images/02/eigenvector.png}
      \caption{Eigenvectors of a matrix}
      \label{fig:eigenvector}
   \end{figure}
\end{paracol}

% Bla about matrices skipped

PCA projects some data $X$ to $\hat{X}$ through a linear transformation $A$: $AX = \hat{X}$.

Fun fact \#1: for a mean-centered $\bar{X}$, the slope is directly proportional to the covariance!

\[
\bar{\Sigma} = \begin{bmatrix}
\sigma_{\bar{X}^1}^2 & \cdots & \text{cov}(\bar{X}^1, \bar{X}^n) \\
\cdots & \cdots & \cdots \\
& & \sigma_{\bar{X}^n}^2
\end{bmatrix}
\]

%PCA notes

% TODO various notes fino a slide 25

\framedt{PCA in a nutshell}{
   There is some pretty complicated linear algebra behind PCA, but the main steps are the following:
   \begin{enumerate}
      \item \ul{Mean-center your data} $X$ to get $\bar{X}$
      
      This means subtracting the mean of each column from the respective column entries, obtaining relative values with respect to the mean.
      \item \ul{Compute its eigenvectors matrix} $\bar{V}$
      
      We build the covariance matrix $\bar{\Sigma}$ of $\bar{X}$, and compute its eigenvectors and eigenvalues.
      The eigenvectors represent the directions of maximum variance (the principal components), and the eigenvalues indicate the magnitude of variance in those directions.
      Eigenvectors are sorted in decreasing order of their eigenvalues and stored in the columns of matrix $V$.
      \note{
         A \textit{correlation matrix} is the same as a covariance matrix, but with normalized values between -1 and 1, obtained by dividing each covariance by the product of the standard deviations of the respective variables.
      }
   
      \item \ul{Transpose} $V$ \ul{to obtain the transformation} $V^T$
      
      We transpose $V$ to prepare for the projection step, as we want to project our data onto the new basis defined by the principal components, allowing for easy computation of dot products.
      \item \ul{Project the data}: $\bar{X}$ through $V^T\bar{X}$, obtaining the PCA-transformed data $\hat{X}$
      
      Here we perform the actual projection of our mean-centered data onto the new basis defined by the principal components, essentially rotating our data to align with the directions of maximum variance.
      Each axis in $\hat{X}$ corresponds to a principal component (PC1,PC2,\dots), ordered by the amount of variance they capture from the original data.
      \vspace*{1em}
   \end{enumerate}
}

\textbf{In simple terms:} PCA looks at your data and finds the ``most important directions" - imagine you have a cloud of points and you want to find the best line that captures the main trend. PCA finds not just one line, but multiple directions ordered by importance. It then rotates your data so that the first dimension captures the most variation, the second dimension captures the second most variation. This allows you to keep only the first two dimensions while retaining most of the information, effectively reducing the complexity of your data while preserving its essential structure.

\subsection{Observations}
\begin{itemize}
	\item PCA redefines data by removing
collinearity: if your data has low
covariance, the transformation will
have minimal effect.
	\item PCA performs a linear
transformation to tackle linear
relationships between variables.
Nonlinear relationships are not
influenced.
   \item PCA does not work very well for high complexity data.
\end{itemize}

\labelitemize{\textit{Uses}}{
  \begin{itemize}
  	\item \textbf{Feature selection}: high covariance
  	      of a feature may indicate
  	      disposability.
  	\item \textbf{Dimensionality reduction}:
  	      trimming columns of lets us
  	      reduce the dimension of the
  	      resulting data.
  	\item \textbf{Clustering preprocessing}:
  	      correlated features inflate object
  	      similarity
  \end{itemize}
}

\section{t-SNE}
t-SNE (t-distributed Stochastic Neighbor Embedding) is a nonlinear dimensionality reduction technique particularly well suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. It works by modeling the data as a distribution of points in a high-dimensional space and then finding a lower-dimensional representation that preserves the pairwise similarities between points.



\texttt{t-SNE} focuses on data clusters rather than subspace representation, and again maps the original data $X$ to a representation $\hat{X}$.

{\texttt{t-SNE} tackles this problem in two phases:\ns
\begin{enumerate}
   \item \textbf{Similarity phase} In the original space $\mathcal{X}$, how similar is $x_i$ to $x_j$?
   \note{How similar is $x_i$ to $x_j$? Even better, what is the probability that $x_i$ is a neighbor of $x_j$?}
   
   \item \textbf{Embedding phase} In the mapped space $\hat{\mathcal{X}}$, how similar is $\hat{x}_i$ to $\hat{x}_j$?
\end{enumerate}
}

\subsection{Similarity phase}
\begin{figure}[htbp]
   \centering
   \includegraphics{images/02/tsne.png}
   \caption{t-SNE plotting distance on an X axis and then projecting it on a normal distribution curve, to get the probability of being a neighbor}
   \label{fig:02/tsne}
\end{figure}
The similarity phase computes the similarity between points in the original high-dimensional space. This is typically done by converting the Euclidean distances between points into conditional probabilities that represent similarities. The probability that point $x_j$ is a neighbor of point $x_i$ is given by a Gaussian distribution centered at $x_i$. The variance of this Gaussian is controlled by a parameter called \textbf{perplexity}, which can be thought of as a smooth measure of the effective number of neighbors.

This yields a neighboring matrix $P$ where each entry $p_{ij}$ represents the probability that point $x_j$ is a neighbor of point $x_i$ in the original high-dimensional space.

The conditional probability is computed as:
\[
p_{j|i} = \frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}
\]

where $\sigma_i$ is the variance of the Gaussian centered at point $x_i$. The perplexity parameter determines $\sigma_i$ through a binary search to match the desired effective number of neighbors.

\subsection{Embedding phase}

In the embedding phase, t-SNE defines a similar probability distribution over the points in the low-dimensional map. However, instead of using a Gaussian distribution, it uses a Student's t-distribution with one degree of freedom (heavy-tailed distribution) to avoid the ``crowding problem" where moderate distant points are forced to be too far apart in the low-dimensional representation.

The probability in the low-dimensional space is:
\[
q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}
\]

where $y_i$ and $y_j$ are the low-dimensional counterparts of $x_i$ and $x_j$.

\subsection{Optimization}

t-SNE minimizes the Kullback-Leibler divergence between the probability distributions $P$ (high-dimensional) and $Q$ (low-dimensional):

\[
C = \sum_i KL(P_i||Q_i) = \sum_i \sum_j p_{j|i} \log \frac{p_{j|i}}{q_{j|i}}
\]

The algorithm uses gradient descent to find the low-dimensional representation $Y$ that minimizes this cost function, effectively preserving the neighborhood structure of the original high-dimensional data.

\textbf{Key advantages:}
\begin{itemize}
	\item Excellent for visualization of high-dimensional data
	\item Preserves local neighborhood structure
	\item Can reveal clusters and patterns not visible in linear methods
\end{itemize}

\textbf{Key limitations:}
\begin{itemize}
	\item Computationally expensive (quadratic in the number of points)
	\item Non-deterministic (different runs can give different results)
	\item Sensitive to hyperparameters, especially perplexity
	\item Not suitable for embedding new data points (no explicit mapping function)
\end{itemize}

\subsection{t-SNE — Practical summary}

t-SNE (t-distributed Stochastic Neighbor Embedding) is a nonlinear method to embed high-dimensional data into 2–3 dimensions for visualization. It prioritizes preserving local neighborhoods rather than global linear structure.

\subsubsection{Workflow (practical)}
\begin{itemize}
  \item \textbf{Similarity in high-dim:} for each point $x_i$ compute conditional probabilities
  \[
    p_{j|i}=\frac{\exp(-\|x_i-x_j\|^2 / 2\sigma_i^2)}{\sum_{k\neq i}\exp(-\|x_i-x_k\|^2 / 2\sigma_i^2)}
  \]
  where $\sigma_i$ is chosen (by binary search) to match a user-specified \emph{perplexity} (effective neighbor count).
  \item \textbf{Symmetrize:} form
  \[
    p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n},
  \]
  producing a joint probability matrix $P$.
  \item \textbf{Low-d similarity:} place points $y_i$ in low-d and define heavy-tailed affinities
  \[
    q_{ij}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k\neq l}(1+\|y_k-y_l\|^2)^{-1}}.
  \]
  \item \textbf{Cost and optimization:} minimize KL divergence
  \[
    C=\sum_{i}\sum_{j} p_{ij}\log\frac{p_{ij}}{q_{ij}}
  \]
  via gradient descent with momentum; common practical boosts include \emph{early exaggeration} (multiply $P$ by a factor early on), learning-rate scheduling, and momentum.
  \item \textbf{Scaling tricks:} for large $n$ use approximations (Barnes–Hut or FFT-based) to reduce complexity from $O(n^2)$.
\end{itemize}

\subsubsection{Practical notes}
\begin{itemize}
  \item Good for revealing clusters and local structure, but distances between far clusters are not reliable.
  \item Sensitive to hyperparameters (perplexity, learning rate, iterations, early-exaggeration).
  \item No explicit mapping function: embedding new points requires recomputing or using approximations.
\end{itemize}

\subsubsection{Why t-SNE is non-deterministic}
Non-determinism arises from several sources commonly used in implementations:
\begin{itemize}
  \item random initialization of low-dimensional points $Y$;
  \item stochasticity in the optimization (random tie-breaks, minibatching or noisy gradients in some variants);
  \item randomized approximations (e.g. tree construction in Barnes–Hut, random projection steps);
  \item sensitivity to optimizer hyperparameters and early-exaggeration phase which amplify small random differences.
\end{itemize}
These factors make different runs (or different implementations/settings) produce different embeddings even on the same data.


\section{UMAP}

UMAP (Uniform Manifold Approximation and Projection) is a nonlinear dimensionality reduction technique that is particularly effective for visualizing high-dimensional data in a low-dimensional space. It is based on manifold learning and topological data analysis, aiming to preserve both local and global structure of the data.

The computed distances induce a connectivity graph, and thus an adjacency matrix $A$,
its edges measuring distances among instances. After turning distances into
probabilities, UMAP optimizes a distance on $A$, to make it so that all and only the edges
on the original manifold also appear in the transformed manifold with the same
magnitude.

For the set of edges $E$, UMAP minimizes
\[
-\sum_{e \in E} \left( \underbrace{\Pr(e; X) \log(\Pr(e; Z))}_{\text{existing edges}} + \underbrace{(1 - \Pr(e; Z)) \log(1 - \Pr(e; X))}_{\text{non-existing edges}} \right),
\]

where $\Pr(e; X), \Pr(e; Z)$ indicate the probability of edge $e$ in the original and transformed representation, respectively.

Note that since the probability is $\in [0,1]$, its logarithm is negative and spans $(-\infty, 0]$, that's the reason behind the minus ``$-$'' before the $\sum$ and the overall functioning of the formula.\\
In this way, when two points in the original space are close ($\Pr(e; X) \approx 1$), the first term dominates and the cost is minimized by making them close in the transformed space as well ($\Pr(e; Z) \approx 1 \Rightarrow \log(\Pr(e; Z)) \approx 0$), because if instead they are far apart in the transformed space $Z$ ($\Pr(e; Z) \approx 0$), then $\log(\Pr(e; Z)) \approx -\infty$ and the cost becomes very large.\\
Conversely, when two points in the original space are far apart ($\Pr(e; X) \approx 0$), the second term dominates and the cost is minimized by making them far apart in the transformed space as well ($\Pr(e; Z) \approx 0 \Rightarrow \log(1 - \Pr(e; Z)) \approx 0$), because if instead they are close in the transformed space ($\Pr(e; Z) \approx 1$), then $\log(1 - \Pr(e; Z)) \approx -\infty$ and the cost becomes very large.


\section{Brief conclusions}
In PCA the process is deterministic and the transformations are all linear, making the results interpretable.\\
UMAP and t-SNE instead are non interpretable methods: the transformations are nonlinear and non-deterministic, since they rely on random initialization and stochastic (probability-related) optimization. Thus, the results may vary between different runs even on the same data, and the axes in the transformed space do not have a clear meaning.


% \subsection{UMAP — Practical summary}

% UMAP (Uniform Manifold Approximation and Projection) is a manifold-based nonlinear method to embed high-dimensional data into 2–3 dimensions for visualization. It builds a fuzzy topological representation of the data and optimizes a low-dimensional layout that preserves local and some global structure.

% \subsubsection{Workflow (practical)}
% \begin{itemize}
%   \item \textbf{Local fuzzy simplicial set:} compute nearest neighbors for each point (parameter \texttt{n\_neighbors}) and convert distances to local connectivity probabilities using a smooth, adaptive kernel; this yields a weighted graph representing the fuzzy topological structure of the data.
%   \item \textbf{Global graph:} combine local fuzzy simplicial sets into a single symmetric fuzzy graph (adjacency matrix) that encodes pairwise connection strengths.
%   \item \textbf{Low-d representation:} initialize low-dimensional embeddings (spectral initialization or random) and define a differentiable similarity function (typically a normalized Student-like kernel).
%   \item \textbf{Optimization:} minimize a cross-entropy–like loss between the high-d fuzzy graph and the low-d graph via stochastic gradient descent; key hyperparameters include \texttt{min\_dist} (controls tightness of clusters) and the chosen \texttt{metric}.
%   \item \textbf{Scaling:} use approximate nearest-neighbor search (ANN) and sparse graph representations for scalability; optimizations are minibatch-based and can be parallelized.
% \end{itemize}

% \subsubsection{Practical notes}
% \begin{itemize}
%   \item Main hyperparameters: \texttt{n\_neighbors} (local vs global balance), \texttt{min\_dist} (cluster compactness), and \texttt{metric} (distance measure in input space).
%   \item Compared to t-SNE, UMAP often preserves more global structure and is faster on large datasets thanks to sparse graphs and ANN.
%   \item UMAP supports a transform method to embed new points using the learned graph (approximate, not exact inverse mapping).
%   \item Good for visualization and downstream tasks (clustering, initialization), but absolute inter-cluster distances remain partially unreliable.
% \end{itemize}

% \subsubsection{Why UMAP can be non-deterministic}
% Non-determinism arises from:
% \begin{itemize}
%   \item approximate nearest-neighbor algorithms (randomized tree/hash structures) used to build the graph;
%   \item random seeds in spectral initialization or random initialization of embeddings;
%   \item stochastic optimization (random sampling/minibatching) during layout optimization;
%   \item implementation-dependent parallelism and float-level nondeterminism.
% \end{itemize}
% Setting explicit random seeds and using deterministic ANN/backends reduces variability but may not eliminate all sources of nondeterminism.
  